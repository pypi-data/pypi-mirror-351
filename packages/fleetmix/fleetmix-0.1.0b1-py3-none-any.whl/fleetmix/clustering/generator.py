"""
generator.py

Main module for generating capacity- and time-feasible customer clusters for the fleet design
optimization process. This is the main entry point for the cluster-first phase of the
cluster-first, fleet-design second heuristic.
"""

from typing import Dict, List, Tuple
import pandas as pd
from joblib import Parallel, delayed
from multiprocessing import Manager
import itertools
from dataclasses import replace

from fleetmix.config.parameters import Parameters
from .common import Cluster, ClusteringSettings, Symbols
from .heuristics import (
    get_feasible_customers_subset,
    create_initial_clusters,
    process_clusters_recursively,
    compute_composite_distance, # TODO: remove
    estimate_num_initial_clusters, # TODO: remove
    get_cached_demand, # TODO: remove
    get_cached_route_time # TODO: remove
)

from fleetmix.utils.logging import FleetmixLogger
logger = FleetmixLogger.get_logger(__name__)

def generate_clusters_for_configurations(
    customers: pd.DataFrame,
    configurations_df: pd.DataFrame,
    params: Parameters,
) -> pd.DataFrame:
    """
    Generate clusters for each vehicle configuration in parallel.
    
    Args:
        customers: DataFrame containing customer data
        configurations_df: DataFrame containing vehicle configurations
        params: Parameters object containing vehicle configuration parameters
    
    Returns:
        DataFrame containing all generated clusters
    """
    logger.info("--- Starting Cluster Generation Process ---")
    if customers.empty or configurations_df.empty:
        logger.warning("Input customers or configurations are empty. Returning empty DataFrame.")
        return pd.DataFrame()

    with Manager() as manager:
        shared_demand_cache = manager.dict()
        shared_route_time_cache = manager.dict()
        
        logger.info("Initializing shared caches for demand and route time calculations")

        # 1. Generate feasibility mapping
        logger.info("Generating feasibility mapping...")
        feasible_customers = _generate_feasibility_mapping(
            customers, 
            configurations_df,
            params.goods
        )
        if not feasible_customers:
            logger.warning("No customers are feasible for any configuration. Returning empty DataFrame.")
            return pd.DataFrame()
        logger.info(f"Feasibility mapping generated for {len(feasible_customers)} customers.")

        # 2. Generate list of ClusteringSettings objects for all runs
        list_of_settings = _get_clustering_settings_list(params)

        # 3. Precompute distance/duration matrices if TSP route estimation is used
        tsp_needed = any(s.route_time_estimation == 'TSP' for s in list_of_settings)
        if tsp_needed:
            logger.info("TSP route estimation detected. Precomputing global distance/duration matrices...")
            # Call the function from route_time module to build and cache matrices
            from fleetmix.utils.route_time import build_distance_duration_matrices
            build_distance_duration_matrices(customers, params.depot, params.avg_speed)
        else:
            logger.info("TSP route estimation not used. Skipping global matrix precomputation.")

        cluster_id_generator = itertools.count()

        # 4. Process configurations in parallel for each settings configuration
        all_clusters = []
        for settings_for_run in list_of_settings:
            logger.info(f"--- Running Configuration: {settings_for_run.method} (GeoW: {settings_for_run.geo_weight:.2f}, DemW: {settings_for_run.demand_weight:.2f}) ---")

            # Run clustering for all configurations using these settings in parallel, process-based
            clusters_by_config = Parallel(n_jobs=-1, backend='loky')(
                delayed(process_configuration)(
                    config, 
                    customers, 
                    feasible_customers, 
                    settings_for_run,
                    shared_demand_cache,
                    shared_route_time_cache,
                    params
                )
                for _, config in configurations_df.iterrows()
            )
            
            # Flatten the list of lists returned by Parallel and assign IDs
            for config_clusters in clusters_by_config:
                for cluster in config_clusters:
                    # Assign unique Cluster_ID
                    cluster.cluster_id = next(cluster_id_generator)
                    all_clusters.append(cluster)
            logger.info(f"--- Configuration {settings_for_run.method} completed, generated {len([c for config_clusters in clusters_by_config for c in config_clusters])} raw clusters ---")

        logger.info(f"Cache statistics: {len(shared_demand_cache)} demand entries, {len(shared_route_time_cache)} route time entries")

    if not all_clusters:
        logger.warning("No clusters were generated by any configuration.")
        return pd.DataFrame()

    # Convert to dictionary for DataFrame
    all_clusters_dicts = [cluster.to_dict() for cluster in all_clusters]
    
    # Remove duplicate clusters based on customer sets
    logger.info(f"Combining and deduplicating {len(all_clusters_dicts)} raw clusters from all configurations...")
    combined_clusters_df = pd.DataFrame(all_clusters_dicts)
    unique_clusters_df = _deduplicate_clusters(combined_clusters_df)
    
    # Check if no clusters were generated
    if unique_clusters_df.empty:
        error_msg = "No feasible clusters could be generated!\n"
        error_msg += "Possible causes:\n"
        error_msg += "- Vehicle capacities are too small for customer demands\n"
        error_msg += "- Time windows are too restrictive\n"
        error_msg += "- Service times + travel times exceed time limits\n"
        error_msg += "- No vehicles have the right compartment configuration for customer demands\n"
        error_msg += "\nPlease review your configuration and customer data."
        raise ValueError(error_msg)

    # Validate cluster coverage
    validate_cluster_coverage(unique_clusters_df, customers)

    logger.info("--- Cluster Generation Complete ---")
    logger.info(f"{Symbols.CHECKMARK} Generated a total of {len(unique_clusters_df)} unique clusters across all configurations.")

    return unique_clusters_df

def process_configuration(
    config: pd.Series,
    customers: pd.DataFrame,
    feasible_customers: Dict,
    settings: ClusteringSettings,
    demand_cache: Dict = None,
    route_time_cache: Dict = None,
    params: Parameters = None
) -> List[Cluster]:
    """Process a single vehicle configuration to generate feasible clusters."""
    # 1. Get customers that can be served by the configuration
    customers_subset = get_feasible_customers_subset(customers, feasible_customers, config['Config_ID'])
    if customers_subset.empty:
        return []
    
    # 2. Create initial clusters (one large cluster for the subset)
    initial_clusters_df = create_initial_clusters(customers_subset, config, settings, params)
    
    # 3. Process clusters recursively until constraints are satisfied
    return process_clusters_recursively(initial_clusters_df, config, settings, demand_cache, route_time_cache, params)

def validate_cluster_coverage(clusters_df, customers_df):
    """Validate that all customers are covered by at least one cluster."""
    customer_coverage = {cid: False for cid in customers_df['Customer_ID']}
    for customers in clusters_df['Customers']:
        for cid in customers:
            customer_coverage[cid] = True
    uncovered = [cid for cid, covered in customer_coverage.items() if not covered]
    
    if uncovered:
        logger.warning(f"Found {len(uncovered)} customers not covered by any cluster: {uncovered[:5]}...")
    else:
        logger.info(f"{Symbols.CHECKMARK} All {len(customer_coverage)} customers are covered by at least one cluster.")

def _generate_feasibility_mapping(
    customers: pd.DataFrame,
    configurations_df: pd.DataFrame,
    goods: List[str]
) -> Dict:
    """Generate mapping of feasible configurations for each customer."""
    feasible_customers = {}
    
    for _, customer in customers.iterrows():
        customer_id = customer['Customer_ID']
        feasible_configs = []
        
        for _, config in configurations_df.iterrows():
            if _is_customer_feasible(customer, config, goods):
                feasible_configs.append(config['Config_ID'])
        
        if feasible_configs:
            feasible_customers[customer_id] = feasible_configs
    
    return feasible_customers

def _is_customer_feasible(
    customer: pd.Series,
    config: pd.Series,
    goods: List[str]
) -> bool:
    """Check if a customer's demands can be served by a configuration."""
    for good in goods:
        if customer[f'{good}_Demand'] > 0 and not config[good]:
            return False
        if customer[f'{good}_Demand'] > config['Capacity']:
            return False
    return True

def _deduplicate_clusters(clusters_df: pd.DataFrame) -> pd.DataFrame:
    """Removes duplicate clusters based on the set of customers."""
    if clusters_df.empty:
        return clusters_df
    logger.debug(f"Starting deduplication with {len(clusters_df)} clusters.")
    # Create a column of frozensets for efficient duplicate checking
    # The lambda handles cases where 'Customers' might not be a list/tuple, creating an empty set
    clusters_df['Customer_Set'] = clusters_df['Customers'].apply(
        lambda x: frozenset(x) if isinstance(x, (list, tuple)) else frozenset()
    )
    # Drop duplicates based on the frozenset column
    deduplicated_df = clusters_df.drop_duplicates(subset=['Customer_Set'], keep='first').drop(columns=['Customer_Set'])
    
    if len(deduplicated_df) < len(clusters_df):
        logger.debug(f"Finished deduplication: Removed {len(clusters_df) - len(deduplicated_df)} duplicate clusters, {len(deduplicated_df)} unique clusters remain.")
    else:
        logger.debug(f"Finished deduplication: No duplicate clusters found ({len(deduplicated_df)} clusters).")
    return deduplicated_df

def _get_clustering_settings_list(params: Parameters) -> List[ClusteringSettings]:
    """Generates a list of ClusteringSettings objects for all runs."""
    settings_list = []

    # Create a base settings object with common parameters
    # The weights here will be used ONLY if a single method is specified
    base_settings = ClusteringSettings(
        method=params.clustering['method'],
        goods=params.goods,
        depot=params.depot,
        avg_speed=params.avg_speed,
        service_time=params.service_time,
        max_route_time=params.max_route_time,
        max_depth=params.clustering['max_depth'],
        route_time_estimation=params.clustering['route_time_estimation'],
        geo_weight=params.clustering['geo_weight'],
        demand_weight=params.clustering['demand_weight']
    )

    if base_settings.method == 'combine':
        logger.info("🔄 Generating settings variations for 'combine' method")
        # 1. Base methods (kmeans, kmedoids, gmm) - Assume primarily geographical (Geo=1, Dem=0)
        base_method_names = ['minibatch_kmeans', 'kmedoids', 'gaussian_mixture']
        for name in base_method_names:
            settings_list.append(replace(
                base_settings, # Start from base
                method=name # Set correct method name
            ))

        # 2. Agglomerative with different explicit weights
        weight_combinations = [
            (1.0, 0.0), (0.8, 0.2), (0.6, 0.4), (0.4, 0.6), (0.2, 0.8), (0.0, 1.0)
        ]
        for geo_w, demand_w in weight_combinations:
            settings_list.append(replace(
                base_settings,
                method='agglomerative',
                geo_weight=geo_w,
                demand_weight=demand_w
            ))

    else:
        # Single method specified: Use the base_settings as configured initially
        # (which already includes the method name and default weights from params)
        logger.info(f"📍 Using single method configuration: {base_settings.method}")
        settings_list.append(base_settings)

    logger.info(f"Generated {len(settings_list)} distinct clustering settings configurations.")
    return settings_list 