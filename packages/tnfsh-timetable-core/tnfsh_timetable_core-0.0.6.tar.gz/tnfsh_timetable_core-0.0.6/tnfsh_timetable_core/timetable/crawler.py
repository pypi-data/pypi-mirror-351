from typing import List, Set, Dict, Optional, Literal, Tuple, TypedDict, TypeAlias
import logging
import aiohttp
from aiohttp import client_exceptions
import asyncio
from bs4 import BeautifulSoup
import json

from tnfsh_timetable_core.index.index import Index
from tnfsh_timetable_core.utils.logger import get_logger

class FetchError(Exception):
    """çˆ¬å–èª²è¡¨æ™‚å¯èƒ½ç™¼ç”Ÿçš„éŒ¯èª¤"""
    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

# è¨­å®šæ—¥èªŒ
logger = get_logger(logger_level="INFO")

# åˆ¥ååˆ—è¡¨
aliases: List[Set[str]] = [
    {"æœ±è’™", "å³éŠ˜"}
]


TimeInfo: TypeAlias = Tuple[str, str]
PeriodName: TypeAlias = str
Subject: TypeAlias = str
CounterPart: TypeAlias = str
Url: TypeAlias = str
Course: TypeAlias = Dict[Subject, Dict[CounterPart, Url]]

class RawParsedResult(TypedDict):
    last_update: str
    periods: Dict[PeriodName, TimeInfo]
    table: List[List[Course]]


from tnfsh_timetable_core.index.models import ReverseIndexResult

def resolve_target(
    target: str,
    reverse_index: ReverseIndexResult,
    aliases: List[Set[str]]
) -> Optional[str]:
    """
    æ ¹æ“šç›®æ¨™åç¨±è§£æåˆ¥åï¼Œå›å‚³å¯ç”¨æ–¼ reverse_index çš„åˆæ³• keyã€‚
    """
    
    if target in reverse_index:
        logger.debug(f"ğŸ¯ æ‰¾åˆ° {target} çš„èª²è¡¨ç¶²å€")
        return target

    for alias_set in aliases:
        if target in alias_set:
            candidates = alias_set - {target}
            for alias in candidates:
                if alias in reverse_index:
                    logger.info(f"ğŸ”„ å°‡ {target} è§£æç‚ºåˆ¥å {alias}")
                    return alias

    return None

async def fetch_raw_html(target: str, refresh: bool = False, max_retries: int = 3, retry_delay: float = 1.0) -> BeautifulSoup:
    """
    éåŒæ­¥æŠ“å–åŸå§‹èª²è¡¨ HTML

    Args:
        target (str): ç›®æ¨™åç¨±ï¼ˆç­ç´šæˆ–æ•™å¸«ï¼‰
        refresh (bool, optional): æ˜¯å¦åˆ·æ–°ç´¢å¼•å¿«å–. é è¨­ç‚º False
        max_retries (int, optional): æœ€å¤§é‡è©¦æ¬¡æ•¸. é è¨­ç‚º 3
        retry_delay (float, optional): é‡è©¦é–“éš”ç§’æ•¸. é è¨­ç‚º 1.0

    Returns:
        BeautifulSoup: è§£æå¾Œçš„ HTML å†…å®¹

    Raises:
        FetchError: ç•¶ç™¼ç”Ÿç¶²è·¯è«‹æ±‚éŒ¯èª¤æˆ–ç„¡æ³•è§£æç›®æ¨™æ™‚æ‹‹å‡º
    """
    from tnfsh_timetable_core.index.index import Index
    from tnfsh_timetable_core.index.models import ReverseIndexResult
    index: Index = Index()
    await index.fetch(refresh=refresh)
    if index.reverse_index is None:
        logger.error("âŒ ç„¡æ³•ç²å–ç´¢å¼•è³‡æ–™")
        raise FetchError("ç„¡æ³•ç²å–ç´¢å¼•è³‡æ–™")
    reverse_index: ReverseIndexResult = index.reverse_index
    base_url: str = index.base_url

    global aliases
    logger.debug(f"ğŸ” è§£æç›®æ¨™ï¼š{target}")
    real_target = resolve_target(target, reverse_index, aliases)
    if real_target is None:
        logger.error(f"âŒ æ‰¾ä¸åˆ° {target} çš„èª²è¡¨ç¶²å€")
        raise FetchError(f"æ‰¾ä¸åˆ° {target} çš„èª²è¡¨ç¶²å€")

    if target == "307":
        relative_url = "C101307.html"
    else:
        relative_url = reverse_index[real_target]["url"]

    full_url = base_url + relative_url
    logger.debug(f"ğŸŒ æº–å‚™è«‹æ±‚ç¶²å€ï¼š{full_url}")

    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                logger.debug(f"ğŸ“¡ ç™¼é€è«‹æ±‚ï¼š{target} (å˜—è©¦ {attempt + 1}/{max_retries})")
                async with session.get(full_url, headers=headers) as response:
                    response.raise_for_status()
                    content = await response.read()
                    logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰ï¼š{target}")
                    soup = BeautifulSoup(content, 'html.parser')
                    logger.debug(f"âœ… HTML è§£æå®Œæˆï¼š{target}")
                    return soup

        except client_exceptions.ClientResponseError as e:
            error_msg = f"HTTP ç‹€æ…‹ç¢¼éŒ¯èª¤ {e.status}: {e.message}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
            else:
                raise FetchError(error_msg)

        except client_exceptions.ClientConnectorError as e:
            error_msg = f"é€£ç·šéŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
            else:
                raise FetchError(error_msg)

        except (client_exceptions.ServerTimeoutError, asyncio.TimeoutError):
            error_msg = "è«‹æ±‚è¶…æ™‚"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
            else:
                raise FetchError(error_msg)

        except client_exceptions.ClientError as e:
            error_msg = f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
            else:
                raise FetchError(error_msg)

        except Exception as e:
            error_msg = f"æœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

def parse_html(soup: BeautifulSoup) -> RawParsedResult:
    """
    è§£æåŸå§‹ HTMLï¼Œæ“·å– last_updateã€periodsã€table
    """
    logger.debug("ğŸ”„ é–‹å§‹è§£æ HTML")
    
    # æ“·å–æ›´æ–°æ—¥æœŸ
    update_element = soup.find('p', class_='MsoNormal', align='center')
    if update_element:
        spans = update_element.find_all('span')
        last_update = spans[1].text if len(spans) > 1 else "No update date found."
        logger.debug(f"ğŸ“… æ›´æ–°æ—¥æœŸï¼š{last_update}")
    else:
        last_update = "No update date found."
        logger.warning("âš ï¸ æ‰¾ä¸åˆ°æ›´æ–°æ—¥æœŸ")

    # æ“·å–èª²è¡¨ table ä¸¦ç§»é™¤ border
    logger.debug("ğŸ” æœå°‹ä¸»è¦èª²è¡¨")
    main_table = None
    for table in soup.find_all("table"):
        new_table = BeautifulSoup('<table></table>', 'html.parser').table
        for row in table.find_all("tr"):
            for td in row.find_all('td'):
                if td.get('style') and 'border' in td['style']:
                    td.decompose()
            if len(row.find_all('td')) == 7:
                new_table.append(row)
                
        if len(new_table.find_all('tr')) > 0:
            main_table = new_table
            break

    if main_table is None:
        logger.error("âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„èª²è¡¨")
        raise FetchError("æ‰¾ä¸åˆ°ç¬¦åˆæ ¼å¼çš„èª²è¡¨ table")

    logger.debug("ğŸ“Š è§£æèª²è¡¨æ™‚é–“")
    # æ“·å– periods
    import re
    re_pattern = r'(\d{2})(\d{2})'
    re_sub = r'\1:\2'
    periods: Dict[str, Tuple[str, str]] = {}
    for row in main_table.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) < 2:
            continue
        lesson_name = cells[0].text.replace("\n", "").replace("\r", "")
        time_text = cells[1].text.replace("\n", "").replace("\r", "")
        times = [re.sub(re_pattern, re_sub, t.replace(" ", "")) for t in time_text.split("ï½œ")]
        if len(times) == 2:
            periods[lesson_name] = (times[0], times[1])

    # æ“·å–èª²ç¨‹åç¨±å’Œæ•™å¸«åç¨±
    # é€™è£¡çš„ class_td æ˜¯ä¸€å€‹ td æ¨™ç±¤ï¼ŒåŒ…å«äº†èª²ç¨‹åç¨±å’Œæ•™å¸«åç¨±
    def class_name_split(class_td) -> Dict[str, Dict[str, str]]:
        """åˆ†æèª²ç¨‹å­—ä¸²ç‚ºèª²ç¨‹åç¨±å’Œæ•™å¸«åç¨±ï¼Œçµ±ä¸€å›å‚³å­—å…¸æ ¼å¼"""
        def clean_text(text: str) -> str:
            """æ¸…ç†æ–‡å­—å…§å®¹ï¼Œç§»é™¤å¤šé¤˜ç©ºæ ¼èˆ‡æ›è¡Œ"""
            return text.strip("\n").strip("\r").strip(" ").replace(" ", ", ")

        def is_teacher_p(p_tag) -> bool:
            """æª¢æŸ¥æ˜¯å¦ç‚ºåŒ…å«æ•™å¸«è³‡è¨Šçš„pæ¨™ç±¤"""
            return bool(p_tag.find_all('a'))
        
        def parse_teachers(teacher_ps) -> Dict[str, str]:
            """è§£ææ‰€æœ‰æ•™å¸«pæ¨™ç±¤çš„è³‡è¨Š"""
            teachers_dict = {}
            for p in teacher_ps:
                for link in p.find_all('a'):
                    name = clean_text(link.text)
                    href = link.get('href', '')
                    teachers_dict[name] = href
            return teachers_dict
        
        def combine_class_name(class_ps) -> str:
            """çµ„åˆèª²ç¨‹åç¨±"""
            texts = [clean_text(p.text) for p in class_ps]
            combine = ''.join(filter(None, texts)).replace("\n", ", ").replace("\u00a0", "")
            return combine
        
        ps = class_td.find_all('p')
        if not ps:
            return {"": {"": ""}}
        
        teacher_ps = []
        class_ps = []
        for p in ps:
            if is_teacher_p(p):
                teacher_ps.append(p)
            else:
                class_ps.append(p)
        
        teachers_dict = parse_teachers(teacher_ps) if teacher_ps else {"": ""}
        
        if class_ps:
            class_name = combine_class_name(class_ps)
        elif teacher_ps == {'':''}:
            class_name = "æ‰¾ä¸åˆ°èª²ç¨‹"
        else:
            class_name = ""
        
        if (class_name and class_name != " ") or teachers_dict != {"": ""}:
            return {class_name: teachers_dict}
        return {"": {"": ""}}

    # æ“·å– table raw æ ¼å¼
    table: List[List[Dict[str, Dict[str, str]]]] = []
    for row in main_table.find_all("tr"):
        cells = row.find_all("td")[2:]  # è·³éå‰å…©åˆ—ï¼ˆç¯€æ¬¡å’Œæ™‚é–“ï¼‰
        row_data = []
        for cell in cells:
            row_data.append(class_name_split(cell))
        if row_data:
            table.append(row_data)

    logger.info("âœ… HTML è§£æå®Œæˆ")
    result = RawParsedResult(
        last_update=last_update,
        periods=periods,
        table=table
    )
    return result


if __name__ == "__main__":
    # For test cases, see: tests/test_timetable/test_crawler.py
    pass