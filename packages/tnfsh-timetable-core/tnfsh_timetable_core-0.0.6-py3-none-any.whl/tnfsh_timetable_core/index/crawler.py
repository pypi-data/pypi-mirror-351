from typing import Optional, TypeAlias, Dict, Union
import aiohttp
from aiohttp import client_exceptions
import asyncio
from bs4 import BeautifulSoup
import re
from tnfsh_timetable_core.index.models import IndexResult, ReverseIndexResult, GroupIndex, ReverseMap, AllTypeIndexResult

from tnfsh_timetable_core import TNFSHTimetableCore
core = TNFSHTimetableCore()
logger = core.get_logger()

class FetchError(Exception):
    """çˆ¬å–èª²è¡¨æ™‚å¯èƒ½ç™¼ç”Ÿçš„éŒ¯èª¤"""
    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

async def request_html(base_url: str, url: str, timeout: int = 15, from_file_path: Optional[str] = None, max_retries: int = 3, retry_delay: float = 1.0) -> BeautifulSoup:
    """éåŒæ­¥å–å¾—ç¶²é å…§å®¹ä¸¦è§£æ
    
    Args:
        base_url (str): åŸºç¤ URL
        url (str): ç›¸å°è·¯å¾‘ URL
        timeout (int): è«‹æ±‚è¶…æ™‚æ™‚é–“
        from_file_path (Optional[str]): å¯é¸çš„æª”æ¡ˆè·¯å¾‘ï¼Œè‹¥æä¾›å‰‡å¾è©²æª”æ¡ˆè®€å–
        max_retries (int, optional): æœ€å¤§é‡è©¦æ¬¡æ•¸. é è¨­ç‚º 3
        retry_delay (float, optional): é‡è©¦é–“éš”ç§’æ•¸. é è¨­ç‚º 1.0
        
    Returns:
        BeautifulSoup: è§£æå¾Œçš„ BeautifulSoup ç‰©ä»¶
        
    Raises:
        aiohttp.ClientError: ç•¶ç¶²é è«‹æ±‚å¤±æ•—æ™‚
        Exception: ç•¶è§£æ HTML å¤±æ•—æ™‚
    """
    if from_file_path:
        logger.debug(f"ğŸ“‚ å¾æª”æ¡ˆè®€å–ï¼š{from_file_path}")
        with open(from_file_path, 'r', encoding='utf-8') as f:
            return BeautifulSoup(f.read(), 'html.parser')
    
    full_url = base_url + url
    logger.debug(f"ğŸŒ æº–å‚™è«‹æ±‚ç¶²å€ï¼š{full_url}")
    
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    for attempt in range(max_retries):
        try:
            logger.debug(f"ğŸ“¡ ç™¼é€è«‹æ±‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
            async with aiohttp.ClientSession() as session:
                async with session.get(full_url, headers=headers, timeout=aiohttp.ClientTimeout(total=timeout)) as response:
                    response.raise_for_status()
                    content = await response.read()
                    logger.debug(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰")
                    soup = BeautifulSoup(content, 'html.parser')
                    logger.debug(f"âœ… HTML è§£æå®Œæˆ")
                    return soup

        except client_exceptions.ClientResponseError as e:
            error_msg = f"HTTP ç‹€æ…‹ç¢¼éŒ¯èª¤ {e.status}: {e.message}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
                continue
            raise aiohttp.ClientError(error_msg)

        except client_exceptions.ClientConnectorError as e:
            error_msg = f"é€£ç·šéŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
                continue
            raise aiohttp.ClientError(error_msg)

        except (client_exceptions.ServerTimeoutError, asyncio.TimeoutError):
            error_msg = "è«‹æ±‚è¶…æ™‚"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
                continue
            raise aiohttp.ClientError(error_msg)

        except client_exceptions.ClientError as e:
            error_msg = f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤ï¼š{str(e)}"
            logger.warning(f"âš ï¸ {error_msg}")
            if attempt + 1 < max_retries:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(retry_delay)
                continue
            raise aiohttp.ClientError(error_msg)

        except Exception as e:
            error_msg = f"æœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise FetchError(error_msg)

def parse_html(soup: BeautifulSoup, url: str) -> GroupIndex:
    """è§£æç¶²é å…§å®¹
    
    Args:
        soup (BeautifulSoup): è¦è§£æçš„ BeautifulSoup ç‰©ä»¶
        url (str): è©²ç´¢å¼•çš„ URL

    Returns:
        GroupIndex: è§£æå¾Œçš„ç´¢å¼•è³‡æ–™çµæ§‹
    """
    parsed_data = {}
    current_category = None
    
    for tr in soup.find_all("tr"):
        category_tag = tr.find("span")
        if category_tag and not tr.find("a"):
            current_category = category_tag.text.strip()
            parsed_data[current_category] = {}
        for a in tr.find_all("a"):
            link = a.get("href")
            text = a.text.strip()
            if text.isdigit() and link:
                parsed_data[current_category][text] = link
            else:
                match = re.search(r'([\u4e00-\u9fa5]+)', text)
                if match:
                    text = match.group(1)
                    parsed_data[current_category][text] = link
                else:
                    text = text.replace("\r", "").replace("\n", "").replace(" ", "").strip()
                    if len(text) > 3:
                        text = text[3:].strip()
                        parsed_data[current_category][text] = link
    
    return GroupIndex(url=url, data=parsed_data)


def reverse_index(index: IndexResult) -> ReverseIndexResult:
    """å°‡ç´¢å¼•è³‡æ–™è½‰æ›ç‚ºåæŸ¥è¡¨æ ¼å¼
    
    å°‡ IndexResult ä¸­çš„ç­ç´šå’Œè€å¸«è³‡æ–™è½‰æ›ç‚º ReverseIndexResult æ ¼å¼ï¼Œ
    æ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾ç‰¹å®šç­ç´šæˆ–è€å¸«çš„è³‡è¨Šã€‚
    
    Args:
        index (IndexResult): åŸå§‹ç´¢å¼•è³‡æ–™
        
    Returns:
        ReverseIndexResult: åæŸ¥è¡¨æ ¼å¼çš„è³‡æ–™
    """
    result: ReverseIndexResult = {}
    
    # è™•ç†è€å¸«è³‡æ–™
    for category, teachers in index.teacher.data.items():
        for teacher_name, url in teachers.items():
            result[teacher_name] = ReverseMap(url=url, category=category)
    
    # è™•ç†ç­ç´šè³‡æ–™
    for category, classes in index.class_.data.items():
        for class_name, url in classes.items():
            result[class_name] = ReverseMap(url=url, category=category)
    
    return result

async def request_all_index(base_url: str) -> IndexResult:
    """éåŒæ­¥ç²å–å®Œæ•´çš„èª²è¡¨ç´¢å¼•
    
    Args:
        base_url (str): åŸºç¤ URL
        
    Returns:
        IndexResult: å®Œæ•´çš„èª²è¡¨ç´¢å¼•è³‡æ–™
    """
    # ä¸¦è¡Œç²å–æ•™å¸«å’Œç­ç´šç´¢å¼•
    tasks = [
        request_html(base_url, "_TeachIndex.html"),
        request_html(base_url, "_ClassIndex.html")
    ]
    teacher_soup, class_soup = await asyncio.gather(*tasks)
    
    # è§£æè³‡æ–™
    teacher_result = parse_html(teacher_soup, "_TeachIndex.html")
    class_result = parse_html(class_soup, "_ClassIndex.html")
    
    # å»ºç«‹å®Œæ•´ç´¢å¼•
    return IndexResult(
        base_url=base_url,
        root="index.html",
        class_=class_result,
        teacher=teacher_result
    )

def merge_results(index: IndexResult, reverse_index: ReverseIndexResult) -> AllTypeIndexResult:
    """åˆä½µç´¢å¼•å’ŒåæŸ¥è¡¨çµæœ
    
    Args:
        index (IndexResult): å®Œæ•´çš„èª²è¡¨ç´¢å¼•è³‡æ–™
        reverse_index (ReverseIndexResult): åæŸ¥è¡¨è³‡æ–™
        
    Returns:
        AllTypeIndexResult: åˆä½µå¾Œçš„çµæœ
    """
    return AllTypeIndexResult(
        index=index,
        reverse_index=reverse_index
    )

async def fetch_all_index(base_url: str) -> AllTypeIndexResult:
    """ç²å–æ‰€æœ‰é¡å‹çš„ç´¢å¼•è³‡æ–™
    
    Args:
        base_url (str): åŸºç¤ URL
        
    Returns:
        AllTypeIndexResult: æ‰€æœ‰é¡å‹çš„ç´¢å¼•è³‡æ–™
    """
    index_result = await request_all_index(base_url)
    reverse_index_result = reverse_index(index_result)
    return merge_results(index_result, reverse_index_result)

if __name__ == "__main__":
    # For test cases, see: tests/test_index/test_crawler.py
    pass
