const __vite__mapDeps=(i,m=__vite__mapDeps,d=(m.f||(m.f=["./MjKsG873.js","./D6-XlEtG.js","./6md2sFro.js"])))=>i.map(i=>d[i]);
import{D as w,k as re,bB as ie,ar as V,aa as oe,bC as q,al as se,bD as ae,bE as ce,bF as ge,bG as ue,bH as $,bI as de,bJ as he,ac as le,b9 as L,bK as me,bL as H,U as pe,G as fe,_ as G,d as l,e as J,p as Y,a1 as K,bM as Q,E as x,l as u,bN as _e,bw as ke,aj as D,j as X,y as j,bO as Te,bP as ye,bQ as Se,bR as be,bS as Le,x as W,bT as ve,bU as we,C as Ce,bV as ze,f as Ee,bW as Me,bX as Ie,s as Z,t as xe,W as Pe,S as De,bY as je,bZ as Oe,aw as Ae}from"./DRtkoAoK.js";import{o as Ne,I as ee}from"./DrACU5nM.js";import{_ as A}from"./ZE6ibgzo.js";import{a as te,g as Re,r as Fe}from"./DMTBZmPI.js";import{l as Be}from"./C63fz6O_.js";import{I as Ge,t as v,g as Ve}from"./B8XR8j5c.js";class $e extends w{constructor(e,t,n,r,o,s,i){super(),this._grammar=e,this._initialState=t,this._containsEmbeddedLanguages=n,this._createBackgroundTokenizer=r,this._backgroundTokenizerShouldOnlyVerifyTokens=o,this._reportTokenizationTime=s,this._reportSlowTokenization=i,this._seenLanguages=[],this._onDidEncounterLanguage=this._register(new re),this.onDidEncounterLanguage=this._onDidEncounterLanguage.event}get backgroundTokenizerShouldOnlyVerifyTokens(){return this._backgroundTokenizerShouldOnlyVerifyTokens()}getInitialState(){return this._initialState}tokenize(e,t,n){throw new Error("Not supported!")}createBackgroundTokenizer(e,t){if(this._createBackgroundTokenizer)return this._createBackgroundTokenizer(e,t)}tokenizeEncoded(e,t,n){const r=Math.random()*1e4<1,o=this._reportSlowTokenization||r,s=o?new ie(!0):void 0,i=this._grammar.tokenizeLine2(e,n,500);if(o){const c=s.elapsed();(r||c>32)&&this._reportTokenizationTime(c,e.length,r)}if(i.stoppedEarly)return console.warn(`Time limit reached when tokenizing line: ${e.substring(0,100)}`),new V(i.tokens,n);if(this._containsEmbeddedLanguages){const c=this._seenLanguages,g=i.tokens;for(let h=0,m=g.length>>>1;h<m;h++){const P=g[(h<<1)+1],S=oe.getLanguageId(P);c[S]||(c[S]=!0,this._onDidEncounterLanguage.fire(S))}}let a;return n.equals(i.ruleStack)?a=n:a=i.ruleStack,new V(i.tokens,a)}}class We extends w{get backgroundTokenizerShouldOnlyVerifyTokens(){return this._actual.backgroundTokenizerShouldOnlyVerifyTokens}constructor(e,t,n,r){super(),this._encodedLanguageId=e,this._actual=t,this._maxTokenizationLineLength=r,this._register(q(this._maxTokenizationLineLength)),this._register(n)}getInitialState(){return this._actual.getInitialState()}tokenize(e,t,n){throw new Error("Not supported!")}tokenizeEncoded(e,t,n){return e.length>=this._maxTokenizationLineLength.get()?se(this._encodedLanguageId,n):this._actual.tokenizeEncoded(e,t,n)}createBackgroundTokenizer(e,t){if(this._actual.createBackgroundTokenizer)return this._actual.createBackgroundTokenizer(e,t)}}const b=class b{static getChannel(e){return e.getChannel(b.CHANNEL_NAME)}static setChannel(e,t){e.setChannel(b.CHANNEL_NAME,t)}};b.CHANNEL_NAME="textMateWorkerHost";let N=b;class Ue{constructor(e){this.edits=e.slice().sort(ae(t=>t.offset,ce))}applyToArray(e){for(let t=this.edits.length-1;t>=0;t--){const n=this.edits[t];e.splice(n.offset,n.length,...new Array(n.newLength))}}}class qe{constructor(e,t,n){this.offset=e,this.length=t,this.newLength=n}toString(){return`[${this.offset}, +${this.length}) -> +${this.newLength}}`}}class E{static fromMany(e){const t=e.map(n=>new E(n));return new He(t)}constructor(e){this.transformation=e,this.idx=0,this.offset=0}transform(e){let t=this.transformation.edits[this.idx];for(;t&&t.offset+t.length<=e;)this.offset+=t.newLength-t.length,this.idx++,t=this.transformation.edits[this.idx];if(!(t&&t.offset<=e))return e+this.offset}}class He{constructor(e){this.transformers=e}transform(e){for(const t of this.transformers){const n=t.transform(e);if(n===void 0)return;e=n}return e}}const I=class I extends w{constructor(e,t,n,r,o,s){super(),this._model=e,this._worker=t,this._languageIdCodec=n,this._backgroundTokenizationStore=r,this._configurationService=o,this._maxTokenizationLineLength=s,this.controllerId=I._id++,this._pendingChanges=[],this._states=new ge,this._loggingEnabled=Ne("editor.experimental.asyncTokenizationLogging",!1,this._configurationService),this._register(q(this._loggingEnabled)),this._register(this._model.onDidChangeContent(c=>{this._shouldLog&&console.log("model change",{fileName:this._model.uri.fsPath.split("\\").pop(),changes:O(c.changes)}),this._worker.$acceptModelChanged(this.controllerId,c),this._pendingChanges.push(c)})),this._register(this._model.onDidChangeLanguage(c=>{const g=this._model.getLanguageId(),h=this._languageIdCodec.encodeLanguageId(g);this._worker.$acceptModelLanguageChanged(this.controllerId,g,h)}));const i=this._model.getLanguageId(),a=this._languageIdCodec.encodeLanguageId(i);this._worker.$acceptNewModel({uri:this._model.uri,versionId:this._model.getVersionId(),lines:this._model.getLinesContent(),EOL:this._model.getEOL(),languageId:i,encodedLanguageId:a,maxTokenizationLineLength:this._maxTokenizationLineLength.get(),controllerId:this.controllerId}),this._register(ue(c=>{const g=this._maxTokenizationLineLength.read(c);this._worker.$acceptMaxTokenizationLineLength(this.controllerId,g)}))}dispose(){super.dispose(),this._worker.$acceptRemovedModel(this.controllerId)}requestTokens(e,t){this._worker.$retokenize(this.controllerId,e,t)}async setTokensAndStates(e,t,n,r){if(this.controllerId!==e)return;let o=$.deserialize(new Uint8Array(n));if(this._shouldLog&&console.log("received background tokenization result",{fileName:this._model.uri.fsPath.split("\\").pop(),updatedTokenLines:o.map(i=>i.getLineRange()).join(" & "),updatedStateLines:r.map(i=>new de(i.startLineNumber,i.startLineNumber+i.stateDeltas.length).toString()).join(" & ")}),this._shouldLog){const i=this._pendingChanges.filter(a=>a.versionId<=t).map(a=>a.changes).map(a=>O(a)).join(" then ");console.log("Applying changes to local states",i)}for(;this._pendingChanges.length>0&&this._pendingChanges[0].versionId<=t;){const i=this._pendingChanges.shift();this._states.acceptChanges(i.changes)}if(this._pendingChanges.length>0){if(this._shouldLog){const c=this._pendingChanges.map(g=>g.changes).map(g=>O(g)).join(" then ");console.log("Considering non-processed changes",c)}const i=E.fromMany(this._pendingChanges.map(c=>U(c.changes))),a=new $;for(const c of o)for(let g=c.startLineNumber;g<=c.endLineNumber;g++)i.transform(g-1)!==void 0&&a.add(g,c.getLineTokens(g));o=a.finalize();for(const c of this._pendingChanges)for(const g of c.changes)for(let h=0;h<o.length;h++)o[h].applyEdit(g.range,g.text)}const s=E.fromMany(this._pendingChanges.map(i=>U(i.changes)));if(!this._applyStateStackDiffFn||!this._initialState){const{applyStateStackDiff:i,INITIAL:a}=await A(async()=>{const{applyStateStackDiff:c,INITIAL:g}=await import("./MjKsG873.js").then(h=>h.m);return{applyStateStackDiff:c,INITIAL:g}},__vite__mapDeps([0,1]),import.meta.url).then(c=>c.default??c);this._applyStateStackDiffFn=i,this._initialState=a}for(const i of r){let a=i.startLineNumber<=1?this._initialState:this._states.getEndState(i.startLineNumber-1);for(let c=0;c<i.stateDeltas.length;c++){const g=i.stateDeltas[c];let h;g?(h=this._applyStateStackDiffFn(a,g),this._states.setEndState(i.startLineNumber+c,h)):h=this._states.getEndState(i.startLineNumber+c);const m=s.transform(i.startLineNumber+c-1);m!==void 0&&this._backgroundTokenizationStore.setEndState(m+1,h),i.startLineNumber+c>=this._model.getLineCount()-1&&this._backgroundTokenizationStore.backgroundTokenizationFinished(),a=h}}this._backgroundTokenizationStore.setTokens(o)}get _shouldLog(){return this._loggingEnabled.get()}};I._id=0;let R=I;function U(d){return new Ue(d.map(e=>new qe(e.range.startLineNumber-1,e.range.endLineNumber-e.range.startLineNumber+1,he(e.text)[0]+1)))}function O(d){return d.map(e=>le.lift(e.range).toString()+" => "+e.text).join(" & ")}var C,_;let F=(_=class{constructor(e,t,n,r,o,s,i){this._reportTokenizationTime=e,this._shouldTokenizeAsync=t,this._extensionResourceLoaderService=n,this._configurationService=r,this._languageService=o,this._notificationService=s,this._telemetryService=i,this._workerProxyPromise=null,this._worker=null,this._workerProxy=null,this._workerTokenizerControllers=new Map,this._currentTheme=null,this._currentTokenColorMap=null,this._grammarDefinitions=[]}dispose(){this._disposeWorker()}createBackgroundTokenizer(e,t,n){if(!this._shouldTokenizeAsync()||e.isTooLargeForSyncing())return;const r=new L,o=this._getWorkerProxy().then(s=>{if(r.isDisposed||!s)return;const i={controller:void 0,worker:this._worker};return r.add(Je(e,()=>{const a=new R(e,s,this._languageService.languageIdCodec,t,this._configurationService,n);return i.controller=a,this._workerTokenizerControllers.set(a.controllerId,a),fe(()=>{i.controller=void 0,this._workerTokenizerControllers.delete(a.controllerId),a.dispose()})})),i});return{dispose(){r.dispose()},requestTokens:async(s,i)=>{const a=await o;a!=null&&a.controller&&a.worker===this._worker&&a.controller.requestTokens(s,i)},reportMismatchingTokens:s=>{C._reportedMismatchingTokens||(C._reportedMismatchingTokens=!0,this._notificationService.error({message:"Async Tokenization Token Mismatch in line "+s,name:"Async Tokenization Token Mismatch"}),this._telemetryService.publicLog2("asyncTokenizationMismatchingTokens",{}))}}}setGrammarDefinitions(e){this._grammarDefinitions=e,this._disposeWorker()}acceptTheme(e,t){this._currentTheme=e,this._currentTokenColorMap=t,this._currentTheme&&this._currentTokenColorMap&&this._workerProxy&&this._workerProxy.$acceptTheme(this._currentTheme,this._currentTokenColorMap)}_getWorkerProxy(){return this._workerProxyPromise||(this._workerProxyPromise=this._createWorkerProxy()),this._workerProxyPromise}async _createWorkerProxy(){const e={grammarDefinitions:this._grammarDefinitions,onigurumaWASMUri:new URL(""+new URL("../assets/onig.Du5pRr7Y.wasm",import.meta.url).href,import.meta.url).href},t=this._worker=me(H.asBrowserUri("vs/workbench/services/textMate/browser/backgroundTokenization/worker/textMateTokenizationWorker.workerMain.js"),"TextMateWorker");return N.setChannel(t,{$readFile:async n=>{const r=pe.revive(n);return this._extensionResourceLoaderService.readExtensionResource(r)},$setTokensAndStates:async(n,r,o,s)=>{const i=this._workerTokenizerControllers.get(n);i&&i.setTokensAndStates(n,r,o,s)},$reportTokenizationTime:(n,r,o,s,i)=>{this._reportTokenizationTime(n,r,o,s,i)}}),await t.proxy.$init(e),this._worker!==t?null:(this._workerProxy=t.proxy,this._currentTheme&&this._currentTokenColorMap&&this._workerProxy.$acceptTheme(this._currentTheme,this._currentTokenColorMap),t.proxy)}_disposeWorker(){for(const e of this._workerTokenizerControllers.values())e.dispose();this._workerTokenizerControllers.clear(),this._worker&&(this._worker.dispose(),this._worker=null),this._workerProxy=null,this._workerProxyPromise=null}},C=_,_._reportedMismatchingTokens=!1,_);F=C=G([l(2,te),l(3,J),l(4,Y),l(5,K),l(6,Q)],F);function Je(d,e){const t=new L,n=t.add(new L);function r(){d.isAttachedToEditor()?n.add(e()):n.clear()}return r(),t.add(d.onDidChangeAttached(()=>{r()})),t}class Ye{constructor(){this._scopeNameToLanguageRegistration=Object.create(null)}reset(){this._scopeNameToLanguageRegistration=Object.create(null)}register(e){this._scopeNameToLanguageRegistration[e.scopeName]=e}getGrammarDefinition(e){return this._scopeNameToLanguageRegistration[e]||null}}const z="No TM Grammar registered for this language.";class Ke extends w{constructor(e,t,n,r){super(),this._host=e,this._initialState=n.INITIAL,this._scopeRegistry=new Ye,this._injections={},this._injectedEmbeddedLanguages={},this._languageToScope=new Map,this._grammarRegistry=this._register(new n.Registry({onigLib:r,loadGrammar:async o=>{const s=this._scopeRegistry.getGrammarDefinition(o);if(!s)return this._host.logTrace(`No grammar found for scope ${o}`),null;const i=s.location;try{const a=await this._host.readFile(i);return n.parseRawGrammar(a,i.path)}catch(a){return this._host.logError(`Unable to load and parse grammar for scope ${o} from ${i}`,a),null}},getInjections:o=>{const s=o.split(".");let i=[];for(let a=1;a<=s.length;a++){const c=s.slice(0,a).join(".");i=[...i,...this._injections[c]||[]]}return i}}));for(const o of t){if(this._scopeRegistry.register(o),o.injectTo){for(const s of o.injectTo){let i=this._injections[s];i||(this._injections[s]=i=[]),i.push(o.scopeName)}if(o.embeddedLanguages)for(const s of o.injectTo){let i=this._injectedEmbeddedLanguages[s];i||(this._injectedEmbeddedLanguages[s]=i=[]),i.push(o.embeddedLanguages)}}o.language&&this._languageToScope.set(o.language,o.scopeName)}}has(e){return this._languageToScope.has(e)}setTheme(e,t){this._grammarRegistry.setTheme(e,t)}getColorMap(){return this._grammarRegistry.getColorMap()}async createGrammar(e,t){const n=this._languageToScope.get(e);if(typeof n!="string")throw new Error(z);const r=this._scopeRegistry.getGrammarDefinition(n);if(!r)throw new Error(z);const o=r.embeddedLanguages;if(this._injectedEmbeddedLanguages[n]){const a=this._injectedEmbeddedLanguages[n];for(const c of a)for(const g of Object.keys(c))o[g]=c[g]}const s=Object.keys(o).length>0;let i;try{i=await this._grammarRegistry.loadGrammarWithConfiguration(n,t,{embeddedLanguages:o,tokenTypes:r.tokenTypes,balancedBracketSelectors:r.balancedBracketSelectors,unbalancedBracketSelectors:r.unbalancedBracketSelectors})}catch(a){throw a.message&&a.message.startsWith("No grammar provided for")?new Error(z):a}return{languageId:e,grammar:i,initialState:this._initialState,containsEmbeddedLanguages:s,sourceExtensionId:r.sourceExtensionId}}}const f=x.registerExtensionPoint({extensionPoint:"grammars",deps:[Be],jsonSchema:{description:u(12450,"Contributes textmate tokenizers."),type:"array",defaultSnippets:[{body:[{language:"${1:id}",scopeName:"source.${2:id}",path:"./syntaxes/${3:id}.tmLanguage."}]}],items:{type:"object",defaultSnippets:[{body:{language:"${1:id}",scopeName:"source.${2:id}",path:"./syntaxes/${3:id}.tmLanguage."}}],properties:{language:{description:u(12451,"Language identifier for which this syntax is contributed to."),type:"string"},scopeName:{description:u(12452,"Textmate scope name used by the tmLanguage file."),type:"string"},path:{description:u(12453,"Path of the tmLanguage file. The path is relative to the extension folder and typically starts with './syntaxes/'."),type:"string"},embeddedLanguages:{description:u(12454,"A map of scope name to language id if this grammar contains embedded languages."),type:"object"},tokenTypes:{description:u(12455,"A map of scope name to token types."),type:"object",additionalProperties:{enum:["string","comment","other"]}},injectTo:{description:u(12456,"List of language scope names to which this grammar is injected to."),type:"array",items:{type:"string"}},balancedBracketScopes:{description:u(12457,"Defines which scope names contain balanced brackets."),type:"array",items:{type:"string"},default:["*"]},unbalancedBracketScopes:{description:u(12458,"Defines which scope names do not contain balanced brackets."),type:"array",items:{type:"string"},default:[]}},required:["scopeName","path"]}}});var T,k;let B=(k=class extends w{constructor(e,t,n,r,o,s,i,a,c,g){super(),this._languageService=e,this._themeService=t,this._extensionResourceLoaderService=n,this._notificationService=r,this._logService=o,this._configurationService=s,this._progressService=i,this._environmentService=a,this._instantiationService=c,this._telemetryService=g,this._createdModes=[],this._encounteredLanguages=[],this._debugMode=!1,this._debugModePrintFunc=()=>{},this._grammarDefinitions=null,this._grammarFactory=null,this._tokenizersRegistrations=this._register(new L),this._currentTheme=null,this._currentTokenColorMap=null,this._threadedBackgroundTokenizerFactory=this._instantiationService.createInstance(F,(h,m,P,S,ne)=>this._reportTokenizationTime(h,m,P,S,!0,ne),()=>this.getAsyncTokenizationEnabled()),this._vscodeOniguruma=null,this._styleElement=_e(),this._styleElement.className="vscode-tokens-styles",f.setHandler(h=>this._handleGrammarsExtPoint(h)),this._updateTheme(this._themeService.getColorTheme(),!0),this._register(this._themeService.onDidColorThemeChange(()=>{this._updateTheme(this._themeService.getColorTheme(),!1)})),this._register(this._languageService.onDidRequestRichLanguageFeatures(h=>{this._createdModes.push(h)}))}getAsyncTokenizationEnabled(){return!!this._configurationService.getValue("editor.experimental.asyncTokenization")}getAsyncTokenizationVerification(){return!!this._configurationService.getValue("editor.experimental.asyncTokenizationVerification")}_handleGrammarsExtPoint(e){this._grammarDefinitions=null,this._grammarFactory&&(this._grammarFactory.dispose(),this._grammarFactory=null),this._tokenizersRegistrations.clear(),this._grammarDefinitions=[];for(const t of e){const n=t.value;for(const r of n){const o=this._validateGrammarDefinition(t,r);if(o&&(this._grammarDefinitions.push(o),o.language)){const s=new ke(()=>this._createTokenizationSupport(o.language));this._tokenizersRegistrations.add(s),this._tokenizersRegistrations.add(D.registerFactory(o.language,s))}}}this._threadedBackgroundTokenizerFactory.setGrammarDefinitions(this._grammarDefinitions);for(const t of this._createdModes)D.getOrCreate(t)}_validateGrammarDefinition(e,t){if(!Ze(e.description.extensionLocation,t,e.collector,this._languageService))return null;const n=X(e.description.extensionLocation,t.path),r=Object.create(null);if(t.embeddedLanguages){const a=Object.keys(t.embeddedLanguages);for(let c=0,g=a.length;c<g;c++){const h=a[c],m=t.embeddedLanguages[h];typeof m=="string"&&this._languageService.isRegisteredLanguageId(m)&&(r[h]=this._languageService.languageIdCodec.encodeLanguageId(m))}}const o=Object.create(null);if(t.tokenTypes){const a=Object.keys(t.tokenTypes);for(const c of a)switch(t.tokenTypes[c]){case"string":o[c]=j.String;break;case"other":o[c]=j.Other;break;case"comment":o[c]=j.Comment;break}}const s=t.language&&this._languageService.isRegisteredLanguageId(t.language)?t.language:void 0;function i(a,c){return!Array.isArray(a)||!a.every(g=>typeof g=="string")?c:a}return{location:n,language:s,scopeName:t.scopeName,embeddedLanguages:r,tokenTypes:o,injectTo:t.injectTo,balancedBracketSelectors:i(t.balancedBracketScopes,["*"]),unbalancedBracketSelectors:i(t.unbalancedBracketScopes,[]),sourceExtensionId:e.description.id}}startDebugMode(e,t){if(this._debugMode){this._notificationService.error(u(12439,"Already Logging."));return}this._debugModePrintFunc=e,this._debugMode=!0,this._debugMode&&this._progressService.withProgress({location:Te.Notification,buttons:[u(12440,"Stop")]},n=>(n.report({message:u(12441,"Preparing to log TM Grammar parsing. Press Stop when finished.")}),this._getVSCodeOniguruma().then(r=>(r.setDefaultDebugCall(!0),n.report({message:u(12442,"Now logging TM Grammar parsing. Press Stop when finished.")}),new Promise((o,s)=>{})))),n=>{this._getVSCodeOniguruma().then(r=>{this._debugModePrintFunc=()=>{},this._debugMode=!1,r.setDefaultDebugCall(!1),t()})})}_canCreateGrammarFactory(){return!!this._grammarDefinitions}async _getOrCreateGrammarFactory(){if(this._grammarFactory)return this._grammarFactory;const[e,t]=await Promise.all([A(()=>import("./MjKsG873.js").then(r=>r.m),__vite__mapDeps([0,1]),import.meta.url).then(r=>r.default??r),this._getVSCodeOniguruma()]),n=Promise.resolve({createOnigScanner:r=>t.createOnigScanner(r),createOnigString:r=>t.createOnigString(r)});return this._grammarFactory?this._grammarFactory:(this._grammarFactory=new Ke({logTrace:r=>this._logService.trace(r),logError:(r,o)=>this._logService.error(r,o),readFile:r=>this._extensionResourceLoaderService.readExtensionResource(r)},this._grammarDefinitions||[],e,n),this._updateTheme(this._themeService.getColorTheme(),!0),this._grammarFactory)}async _createTokenizationSupport(e){if(!this._languageService.isRegisteredLanguageId(e)||!this._canCreateGrammarFactory())return null;try{const t=await this._getOrCreateGrammarFactory();if(!t.has(e))return null;const n=this._languageService.languageIdCodec.encodeLanguageId(e),r=await t.createGrammar(e,n);if(!r.grammar)return null;const o=et("editor.maxTokenizationLineLength",e,-1,this._configurationService),s=new L,i=s.add(new $e(r.grammar,r.initialState,r.containsEmbeddedLanguages,(a,c)=>this._threadedBackgroundTokenizerFactory.createBackgroundTokenizer(a,c,o),()=>this.getAsyncTokenizationVerification(),(a,c,g)=>{this._reportTokenizationTime(a,e,r.sourceExtensionId,c,!1,g)},!0));return s.add(i.onDidEncounterLanguage(a=>{if(!this._encounteredLanguages[a]){const c=this._languageService.languageIdCodec.decodeLanguageId(a);this._encounteredLanguages[a]=!0,this._languageService.requestBasicLanguageFeatures(c)}})),new We(n,i,s,o)}catch(t){return t.message&&t.message===z||ye(t),null}}_updateTheme(e,t){var o;if(!t&&this._currentTheme&&this._currentTokenColorMap&&Xe(this._currentTheme.settings,e.tokenColors)&&Se(this._currentTokenColorMap,e.tokenColorMap))return;this._currentTheme={name:e.label,settings:e.tokenColors},this._currentTokenColorMap=e.tokenColorMap,(o=this._grammarFactory)==null||o.setTheme(this._currentTheme,this._currentTokenColorMap);const n=Qe(this._currentTokenColorMap),r=be(n);this._styleElement.textContent=r,D.setColorMap(n),this._currentTheme&&this._currentTokenColorMap&&this._threadedBackgroundTokenizerFactory.acceptTheme(this._currentTheme,this._currentTokenColorMap)}async createTokenizer(e){if(!this._languageService.isRegisteredLanguageId(e))return null;const t=await this._getOrCreateGrammarFactory();if(!t.has(e))return null;const n=this._languageService.languageIdCodec.encodeLanguageId(e),{grammar:r}=await t.createGrammar(e,n);return r}_getVSCodeOniguruma(){return this._vscodeOniguruma||(this._vscodeOniguruma=(async()=>{const[e,t]=await Promise.all([A(()=>import("./6md2sFro.js").then(n=>n.m),__vite__mapDeps([2,1]),import.meta.url).then(n=>n.default??n),this._loadVSCodeOnigurumaWASM()]);return await e.loadWASM({data:t,print:n=>{this._debugModePrintFunc(n)}}),e})()),this._vscodeOniguruma}async _loadVSCodeOnigurumaWASM(){return ze?await(await fetch(new URL(""+new URL("../assets/onig.Du5pRr7Y.wasm",import.meta.url).href,import.meta.url).href)).arrayBuffer():await fetch(H.asBrowserUri(`${Le}/vscode-oniguruma/release/onig.wasm`).toString(!0))}_reportTokenizationTime(e,t,n,r,o,s){const i=o?"async":"sync";T.reportTokenizationTimeCounter[i]>50||(T.reportTokenizationTimeCounter[i]===0&&setTimeout(()=>{T.reportTokenizationTimeCounter[i]=0},1e3*60*60),T.reportTokenizationTimeCounter[i]++,this._telemetryService.publicLog2("editor.tokenizedLine",{timeMs:e,languageId:t,lineLength:r,fromWorker:o,sourceExtensionId:n,isRandomSample:s,tokenizationSetting:this.getAsyncTokenizationEnabled()?this.getAsyncTokenizationVerification()?2:1:0}))}},T=k,k.reportTokenizationTimeCounter={sync:0,async:0},k);B=T=G([l(0,Y),l(1,Ge),l(2,te),l(3,K),l(4,Ee),l(5,J),l(6,Me),l(7,Ie),l(8,Z),l(9,Q)],B);function Qe(d){const e=[null];for(let t=1,n=d.length;t<n;t++)e[t]=Ce.fromHex(d[t]);return e}function Xe(d,e){if(!e||!d||e.length!==d.length)return!1;for(let t=e.length-1;t>=0;t--){const n=e[t],r=d[t];if(n.scope!==r.scope)return!1;const o=n.settings,s=r.settings;if(o&&s){if(o.fontStyle!==s.fontStyle||o.foreground!==s.foreground||o.background!==s.background)return!1}else if(!o||!s)return!1}return!0}function Ze(d,e,t,n){if(e.language&&(typeof e.language!="string"||!n.isRegisteredLanguageId(e.language)))return t.error(u(12443,"Unknown language in `contributes.{0}.language`. Provided value: {1}",f.name,String(e.language))),!1;if(!e.scopeName||typeof e.scopeName!="string")return t.error(u(12444,"Expected string in `contributes.{0}.scopeName`. Provided value: {1}",f.name,String(e.scopeName))),!1;if(!e.path||typeof e.path!="string")return t.error(u(12445,"Expected string in `contributes.{0}.path`. Provided value: {1}",f.name,String(e.path))),!1;if(e.injectTo&&(!Array.isArray(e.injectTo)||e.injectTo.some(o=>typeof o!="string")))return t.error(u(12446,"Invalid value in `contributes.{0}.injectTo`. Must be an array of language scope names. Provided value: {1}",f.name,JSON.stringify(e.injectTo))),!1;if(e.embeddedLanguages&&!W(e.embeddedLanguages))return t.error(u(12447,"Invalid value in `contributes.{0}.embeddedLanguages`. Must be an object map from scope name to language. Provided value: {1}",f.name,JSON.stringify(e.embeddedLanguages))),!1;if(e.tokenTypes&&!W(e.tokenTypes))return t.error(u(12448,"Invalid value in `contributes.{0}.tokenTypes`. Must be an object map from scope name to token type. Provided value: {1}",f.name,JSON.stringify(e.tokenTypes))),!1;const r=X(d,e.path);return ve(r,d)||t.warn(u(12449,"Expected `contributes.{0}.path` ({1}) to be included inside extension's folder ({2}). This might make the extension non-portable.",f.name,r.path,d.path)),!0}function et(d,e,t,n){return we(r=>n.onDidChangeConfiguration(o=>{o.affectsConfiguration(d,{overrideIdentifier:e})&&r(o)}),()=>n.getValue(d,{overrideIdentifier:e})??t)}const p=Ve(),tt=x.registerExtensionPoint({extensionPoint:"semanticTokenTypes",jsonSchema:{description:u(12646,"Contributes semantic token types."),type:"array",items:{type:"object",properties:{id:{type:"string",description:u(12647,"The identifier of the semantic token type"),pattern:v,patternErrorMessage:u(12648,"Identifiers should be in the form letterOrDigit[_-letterOrDigit]*")},superType:{type:"string",description:u(12649,"The super type of the semantic token type"),pattern:v,patternErrorMessage:u(12650,"Super types should be in the form letterOrDigit[_-letterOrDigit]*")},description:{type:"string",description:u(12651,"The description of the semantic token type")}}}}}),nt=x.registerExtensionPoint({extensionPoint:"semanticTokenModifiers",jsonSchema:{description:u(12652,"Contributes semantic token modifiers."),type:"array",items:{type:"object",properties:{id:{type:"string",description:u(12653,"The identifier of the semantic token modifier"),pattern:v,patternErrorMessage:u(12654,"Identifiers should be in the form letterOrDigit[_-letterOrDigit]*")},description:{type:"string",description:u(12655,"The description of the semantic token modifier")}}}}}),rt=x.registerExtensionPoint({extensionPoint:"semanticTokenScopes",jsonSchema:{description:u(12656,"Contributes semantic token scope maps."),type:"array",items:{type:"object",properties:{language:{description:u(12657,"Lists the languge for which the defaults are."),type:"string"},scopes:{description:u(12658,"Maps a semantic token (described by semantic token selector) to one or more textMate scopes used to represent that token."),type:"object",additionalProperties:{type:"array",items:{type:"string"}}}}}}});class it{constructor(){function e(t,n,r){if(typeof t.id!="string"||t.id.length===0)return r.error(u(12659,"'configuration.{0}.id' must be defined and can not be empty",n)),!1;if(!t.id.match(v))return r.error(u(12660,"'configuration.{0}.id' must follow the pattern letterOrDigit[-_letterOrDigit]*",n)),!1;const o=t.superType;return o&&!o.match(v)?(r.error(u(12661,"'configuration.{0}.superType' must follow the pattern letterOrDigit[-_letterOrDigit]*",n)),!1):typeof t.description!="string"||t.id.length===0?(r.error(u(12662,"'configuration.{0}.description' must be defined and can not be empty",n)),!1):!0}tt.setHandler((t,n)=>{for(const r of n.added){const o=r.value,s=r.collector;if(!o||!Array.isArray(o)){s.error(u(12663,"'configuration.semanticTokenType' must be an array"));return}for(const i of o)e(i,"semanticTokenType",s)&&p.registerTokenType(i.id,i.description,i.superType)}for(const r of n.removed){const o=r.value;for(const s of o)p.deregisterTokenType(s.id)}}),nt.setHandler((t,n)=>{for(const r of n.added){const o=r.value,s=r.collector;if(!o||!Array.isArray(o)){s.error(u(12664,"'configuration.semanticTokenModifier' must be an array"));return}for(const i of o)e(i,"semanticTokenModifier",s)&&p.registerTokenModifier(i.id,i.description)}for(const r of n.removed){const o=r.value;for(const s of o)p.deregisterTokenModifier(s.id)}}),rt.setHandler((t,n)=>{for(const r of n.added){const o=r.value,s=r.collector;if(!o||!Array.isArray(o)){s.error(u(12665,"'configuration.semanticTokenScopes' must be an array"));return}for(const i of o){if(i.language&&typeof i.language!="string"){s.error(u(12666,"'configuration.semanticTokenScopes.language' must be a string"));continue}if(!i.scopes||typeof i.scopes!="object"){s.error(u(12667,"'configuration.semanticTokenScopes.scopes' must be defined as an object"));continue}for(const a in i.scopes){const c=i.scopes[a];if(!Array.isArray(c)||c.some(g=>typeof g!="string")){s.error(u(12668,"'configuration.semanticTokenScopes.scopes' values must be an array of strings"));continue}try{const g=p.parseTokenSelector(a,i.language);p.registerTokenStyleDefault(g,{scopesToProbe:c.map(h=>h.split(" "))})}catch{s.error(u(12669,"configuration.semanticTokenScopes.scopes': Problems parsing selector {0}.",a))}}}}for(const r of n.removed){const o=r.value;for(const s of o)for(const i in s.scopes){const a=s.scopes[i];try{const c=p.parseTokenSelector(i,s.language);p.registerTokenStyleDefault(c,{scopesToProbe:a.map(g=>g.split(" "))})}catch{}}}})}}var y;let M=(y=class{constructor(e){this.instantiationService=e,this.instantiationService.createInstance(it)}},y.ID="workbench.contrib.tokenClassificationExtensionPoint",y);M=G([l(0,Z)],M);xe(M.ID,M,Pe.BlockStartup);Fe(async d=>{d.get(je).when(Oe.Ready).then(()=>{Ae.get(ee)})});function dt(){return{...Re(),[ee.toString()]:new De(B,[],!1)}}export{ee as ITextMateTokenizationService,dt as default};
//# sourceMappingURL=6JpkiI_c.js.map
