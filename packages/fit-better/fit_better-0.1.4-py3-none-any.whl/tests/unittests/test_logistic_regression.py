"""
Tests for logistic regression implementation in fit_better.

Author: Generated by GitHub Copilot
Create Time: 2025-05-22
"""

import pytest
import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression

# Import directly from the module to avoid circular imports
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from fit_better.core.models import RegressorType


def test_logistic_regression_creation():
    """Test that logistic regression regressor can be created."""
    # Instead of using create_regressor, we'll import and test the LogisticRegression class directly
    model = LogisticRegression(
        C=1.0,  # Inverse of regularization strength
        solver="lbfgs",  # Algorithm to use in the optimization problem
        max_iter=200,  # Maximum number of iterations
        multi_class="auto",  # Auto-detect whether the problem is binary or multi-class
        penalty="l2",  # L2 regularization
        random_state=42,  # For reproducible results
        n_jobs=-1,  # Use all available cores
        tol=1e-4,  # Tolerance for stopping criteria
    )
    assert hasattr(model, "fit"), "Logistic regression model should have fit method"
    assert hasattr(
        model, "predict"
    ), "Logistic regression model should have predict method"
    assert hasattr(
        model, "predict_proba"
    ), "Logistic regression model should have predict_proba method"


def test_logistic_regression_fit_predict():
    """Test that logistic regression can fit and predict on simple data."""
    # Create a simple binary classification dataset
    X, y = make_classification(
        n_samples=100,
        n_features=5,
        n_informative=3,
        n_redundant=1,
        n_classes=2,
        random_state=42,
    )

    # Create and fit the logistic regression model
    model = LogisticRegression(C=1.0, solver="lbfgs", max_iter=200, random_state=42)
    model.fit(X, y)

    # Make predictions
    y_pred = model.predict(X)

    # Check performance - it should be better than random on this simple dataset
    accuracy = accuracy_score(y, y_pred)
    assert accuracy > 0.7, f"Accuracy should be > 0.7, got {accuracy}"

    # Test probability predictions
    y_proba = model.predict_proba(X)
    assert y_proba.shape == (
        100,
        2,
    ), "Probability shape should be (n_samples, n_classes)"
    assert np.all(
        (y_proba >= 0) & (y_proba <= 1)
    ), "Probabilities should be between 0 and 1"


def test_logistic_regression_multiclass():
    """Test logistic regression with multiclass data."""
    # Create a multiclass dataset
    X, y = make_classification(
        n_samples=150,
        n_features=5,
        n_informative=3,
        n_redundant=1,
        n_classes=3,
        random_state=42,
    )

    # Create and fit the logistic regression model
    model = LogisticRegression(
        C=1.0, solver="lbfgs", max_iter=200, multi_class="auto", random_state=42
    )
    model.fit(X, y)

    # Make predictions
    y_pred = model.predict(X)

    # Validate the output
    assert len(np.unique(y_pred)) == 3, "Should predict all 3 classes"

    # Check performance metrics
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred, average="weighted")
    recall = recall_score(y, y_pred, average="weighted")

    # It should perform reasonably well on this simple dataset
    assert accuracy > 0.6, f"Accuracy should be > 0.6, got {accuracy}"
    assert precision > 0.6, f"Precision should be > 0.6, got {precision}"
    assert recall > 0.6, f"Recall should be > 0.6, got {recall}"


def test_logistic_regression_regularization():
    """Test logistic regression with different regularization strengths."""
    # Create a dataset with some irrelevant features
    X, y = make_classification(
        n_samples=100,
        n_features=20,  # More features, some will be irrelevant
        n_informative=5,
        n_redundant=5,
        n_classes=2,
        random_state=42,
    )

    # Split the dataset
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Test with weak regularization (high C)
    model_weak_reg = LogisticRegression(C=10.0, random_state=42)
    model_weak_reg.fit(X_train, y_train)
    y_pred_weak = model_weak_reg.predict(X_test)

    # Test with strong regularization (low C)
    model_strong_reg = LogisticRegression(C=0.1, random_state=42)
    model_strong_reg.fit(X_train, y_train)
    y_pred_strong = model_strong_reg.predict(X_test)

    # Both models should have decent performance
    acc_weak = accuracy_score(y_test, y_pred_weak)
    acc_strong = accuracy_score(y_test, y_pred_strong)

    assert (
        acc_weak > 0.6
    ), f"Weak regularization accuracy should be > 0.6, got {acc_weak}"
    assert (
        acc_strong > 0.6
    ), f"Strong regularization accuracy should be > 0.6, got {acc_strong}"

    # Get the number of features with non-zero coefficients (feature selection effect)
    n_coefs_weak = np.sum(np.abs(model_weak_reg.coef_) > 1e-10)
    n_coefs_strong = np.sum(np.abs(model_strong_reg.coef_) > 1e-10)

    # Strong regularization should use fewer features
    # Note: This test might occasionally fail due to randomness in the data
    assert (
        n_coefs_strong <= n_coefs_weak
    ), f"Strong regularization should use fewer features: got {n_coefs_strong} vs {n_coefs_weak}"


def test_logistic_in_regressor_types():
    """Test that LOGISTIC is included in the RegressorType enum."""
    assert hasattr(
        RegressorType, "LOGISTIC"
    ), "RegressorType should have LOGISTIC attribute"
    assert (
        RegressorType.LOGISTIC.value == "Logistic Regression"
    ), "LOGISTIC value should be 'Logistic Regression'"

    # Should be included in available types
    available_types = RegressorType.available_types()
    assert (
        RegressorType.LOGISTIC in available_types
    ), "LOGISTIC should be in available_types"
