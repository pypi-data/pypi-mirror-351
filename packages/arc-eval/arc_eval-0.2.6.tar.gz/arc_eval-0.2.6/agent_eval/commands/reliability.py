"""
Reliability command handlers for ARC-Eval CLI.

Handles workflow reliability analysis, agent debugging, and unified debugging commands.
Pure presentation layer - all core logic delegated to ReliabilityAnalyzer.
"""

from typing import Optional, List, Any
from rich.console import Console

from .base import BaseCommandHandler

console = Console()


class ReliabilityCommandHandler(BaseCommandHandler):
    """Handler for reliability-focused commands - pure presentation layer."""
    
    def execute(self, **kwargs) -> int:
        """Execute reliability commands based on parameters."""
        debug_agent = kwargs.get('debug_agent', False)
        unified_debug = kwargs.get('unified_debug', False)
        workflow_reliability = kwargs.get('workflow_reliability', False)
        schema_validation = kwargs.get('schema_validation', False)
        
        try:
            if debug_agent or unified_debug:
                return self._handle_unified_debugging(**kwargs)
            elif workflow_reliability:
                return self._handle_workflow_reliability_analysis(**kwargs)
            elif schema_validation:
                return self._handle_schema_validation(**kwargs)
            else:
                self.logger.error("No reliability command specified")
                return 1
        except ImportError as e:
            console.print(f"[red]Missing dependency:[/red] {e}")
            self.logger.error(f"ImportError in reliability command: {e}")
            return 1
        except (FileNotFoundError, ValueError, KeyError) as e:
            console.print(f"[red]Error in reliability analysis:[/red] {e}")
            self.logger.error(f"Reliability command failed: {e}")
            return 1
    
    def _handle_unified_debugging(self, **kwargs) -> int:
        """Handle unified debugging workflow using comprehensive reliability analysis."""
        
        console.print("üîß [bold cyan]Agentic Workflow Reliability Platform[/bold cyan]")
        console.print("Debug agent failures with unified visibility across the entire stack\\n")
        
        # Load and validate inputs
        agent_outputs, _ = self._load_and_validate_inputs(**kwargs)
        if agent_outputs is None:
            return 1
        
        # Get analysis parameters
        framework = kwargs.get('framework')
        debug_agent = kwargs.get('debug_agent', False)
        unified_debug = kwargs.get('unified_debug', False)
        dev = kwargs.get('dev', False)
        
        console.print(f"üîß Starting unified debugging session...")
        console.print(f"üìä Analyzing {len(agent_outputs)} workflow components...")
        
        # Delegate to core reliability analyzer
        try:
            from agent_eval.evaluation.reliability_validator import ReliabilityAnalyzer
            
            analyzer = ReliabilityAnalyzer()
            analysis = analyzer.generate_comprehensive_analysis(
                agent_outputs=agent_outputs,
                framework=framework
            )
            
            # Display comprehensive analysis
            console.print(analysis.reliability_dashboard)
            
            # Show insights
            if analysis.insights_summary:
                console.print(f"\\nüí° [bold cyan]Key Insights:[/bold cyan]")
                for insight in analysis.insights_summary:
                    console.print(f"  {insight}")
            
            # Show mode-specific guidance
            if debug_agent:
                console.print(f"\\nüí° [bold cyan]Debug Agent Mode Insights:[/bold cyan]")
                console.print("‚Ä¢ Focus on step-by-step failure analysis")
                console.print("‚Ä¢ Identify root causes of agent failures") 
                console.print("‚Ä¢ Get framework-specific optimization suggestions")
            elif unified_debug:
                console.print(f"\\nüí° [bold cyan]Unified Debug Mode Insights:[/bold cyan]")
                console.print("‚Ä¢ Single view of tool calls, prompts, memory, timeouts")
                console.print("‚Ä¢ Cross-stack visibility for production debugging")
                console.print("‚Ä¢ Comprehensive workflow reliability assessment")
            
            # Show next steps
            if analysis.next_steps:
                console.print(f"\\nüìã [bold]Next Steps:[/bold]")
                for i, step in enumerate(analysis.next_steps, 1):
                    if step.startswith(f"{i}."):
                        console.print(step)
                    else:
                        console.print(f"{i}. {step}")
            
            # Show compliance bonus value
            console.print("\\nüìã [bold cyan]Enterprise Compliance Ready[/bold cyan] (Bonus Value)")
            console.print("‚úÖ 355 compliance scenarios available across finance, security, ML")
            
            if dev:
                console.print(f"\\n[dim]Debug: Framework={analysis.detected_framework}, "
                            f"Confidence={analysis.framework_confidence:.2f}, "
                            f"Evidence={analysis.evidence_quality}, "
                            f"Outputs={analysis.sample_size}[/dim]")
            
        except ImportError as e:
            console.print(f"[yellow]‚ö†Ô∏è Advanced reliability analysis unavailable: {e}[/yellow]")
            console.print("üí° Falling back to basic analysis...")
            
            # Basic fallback
            framework_info = self._basic_framework_detection(agent_outputs, framework)
            console.print(f"\\nüéØ [bold]Basic Analysis Results:[/bold]")
            console.print(f"‚úÖ Total Components: {len(agent_outputs)}")
            console.print(f"üîß Framework: {framework_info}")
            console.print(f"üìã Debug Mode: {'Agent Debugging' if debug_agent else 'Unified Debug'}")
        
        return 0
    
    def _handle_workflow_reliability_analysis(self, **kwargs) -> int:
        """Handle workflow reliability-focused analysis."""
        
        console.print("üéØ [bold cyan]Workflow Reliability Analysis[/bold cyan]")
        
        framework = kwargs.get('framework')
        if framework:
            console.print(f"Analyzing workflow reliability for [cyan]{framework.upper()}[/cyan] framework...")
        else:
            console.print("Analyzing workflow reliability with auto-framework detection...")
        
        # Load and validate inputs
        agent_outputs, _ = self._load_and_validate_inputs(**kwargs)
        if agent_outputs is None:
            return 1
        
        dev = kwargs.get('dev', False)
        domain = kwargs.get('domain')
        endpoint = kwargs.get('endpoint')
        
        console.print(f"\\nüîç Analyzing {len(agent_outputs)} workflow components...")
        
        # Delegate to comprehensive reliability analyzer
        try:
            from agent_eval.evaluation.reliability_validator import ReliabilityAnalyzer
            
            analyzer = ReliabilityAnalyzer()
            analysis = analyzer.generate_comprehensive_analysis(
                agent_outputs=agent_outputs,
                framework=framework
            )
            
            # Display framework-specific analysis if detected
            if analysis.framework_performance:
                perf = analysis.framework_performance
                console.print(f"\\nüìã [bold]{perf.framework_name.upper()} Framework Analysis (Data-Driven):[/bold]")
                console.print(f"üìä [bold]Performance Analysis (Sample: {perf.sample_size} outputs):[/bold]")
                console.print(f"  ‚Ä¢ Success Rate: {perf.success_rate:.1%}")
                console.print(f"  ‚Ä¢ Avg Response Time: {perf.avg_response_time:.1f}s")
                console.print(f"  ‚Ä¢ Tool Call Failures: {perf.tool_call_failure_rate:.1%}")
                console.print(f"  ‚Ä¢ Timeout Rate: {perf.timeout_frequency:.1%}")
                
                # Display evidence-based bottlenecks
                if perf.performance_bottlenecks:
                    console.print(f"\\n‚ö†Ô∏è [bold]Performance Bottlenecks Detected:[/bold]")
                    for bottleneck in perf.performance_bottlenecks:
                        severity_color = "red" if bottleneck.get('severity') == 'high' else "yellow"
                        console.print(f"  ‚Ä¢ [{severity_color}]{bottleneck['type'].replace('_', ' ').title()}[/{severity_color}]")
                        console.print(f"    Evidence: {bottleneck['evidence']}")
                        if 'avg_time' in bottleneck:
                            console.print(f"    Average time: {bottleneck['avg_time']:.1f}s")
                
                # Display optimization opportunities
                if perf.optimization_opportunities:
                    console.print(f"\\nüí° [bold]Evidence-Based Optimization Opportunities:[/bold]")
                    for opportunity in perf.optimization_opportunities:
                        priority_color = "red" if opportunity.get('priority') == 'high' else "yellow"
                        console.print(f"  ‚Ä¢ [{priority_color}]{opportunity['description']}[/{priority_color}]")
                        console.print(f"    Evidence: {opportunity['evidence']}")
                        console.print(f"    Expected improvement: {opportunity['estimated_improvement']}")
                
                # Display framework alternatives if recommended
                if perf.framework_alternatives:
                    console.print(f"\\nüîÑ [bold]Alternative Frameworks (Based on Issues):[/bold]")
                    for alternative in perf.framework_alternatives:
                        console.print(f"  ‚Ä¢ {alternative}")
                
                # Display confidence and recommendation strength
                console.print(f"\\nüéØ [bold]Analysis Confidence:[/bold] {perf.analysis_confidence:.1%}")
                console.print(f"üìà [bold]Recommendation Strength:[/bold] {perf.recommendation_strength}")
                
                # Objective performance analysis with statistical backing
                try:
                    from agent_eval.evaluation.objective_analyzer import ObjectiveFrameworkAnalyzer
                    
                    # Extract performance data for objective analysis
                    perf_data = []
                    if hasattr(perf, 'avg_response_time') and perf.avg_response_time > 0:
                        perf_data.append({"response_time": perf.avg_response_time})
                    if hasattr(perf, 'success_rate'):
                        perf_data.append({"success_rate": perf.success_rate})
                    if hasattr(perf, 'tool_call_failure_rate'):
                        perf_data.append({"error_rate": perf.tool_call_failure_rate})
                    
                    if perf_data:
                        analyzer = ObjectiveFrameworkAnalyzer()
                        objective_recs = analyzer.analyze_framework_performance(framework, perf_data)
                        
                        for rec in objective_recs:
                            if rec.strength.value in ["strong", "moderate"]:
                                console.print(f"\\nüö® [bold red]Statistical Performance Alert:[/bold red]")
                                console.print(f"{rec.recommendation_text}")
                                console.print(f"üìä Evidence: {rec.evidence_summary}")
                                console.print(f"üî¨ Statistical Strength: {rec.strength.value}")
                                
                                if rec.alternative_frameworks:
                                    alternatives = ", ".join(rec.alternative_frameworks)
                                    console.print(f"üìà Better performing alternatives: {alternatives}")
                            elif rec.strength.value == "weak":
                                console.print(f"\\nüü° [bold yellow]Performance Notice:[/bold yellow]")
                                console.print(f"{rec.recommendation_text}")
                                console.print(f"üìä Evidence: {rec.evidence_summary}")
                                console.print(f"‚ö†Ô∏è Note: Limited statistical confidence (more data recommended)")
                    
                except ImportError:
                    # Fallback to threshold-based alerts without subjective recommendations
                    from agent_eval.core.constants import CREWAI_SLOW_RESPONSE_THRESHOLD, LANGCHAIN_ABSTRACTION_OVERHEAD_THRESHOLD
                    
                    if framework == "crewai" and perf.avg_response_time > CREWAI_SLOW_RESPONSE_THRESHOLD:
                        console.print(f"\\nüö® [bold red]Performance Alert:[/bold red]")
                        console.print(f"Response time {perf.avg_response_time:.1f}s exceeds {CREWAI_SLOW_RESPONSE_THRESHOLD}s threshold")
                        console.print("üìä Objective analysis: Response time measurement above established baseline")
                    
                    elif framework == "langchain" and hasattr(perf, 'abstraction_overhead') and perf.abstraction_overhead > LANGCHAIN_ABSTRACTION_OVERHEAD_THRESHOLD:
                        console.print(f"\\nüö® [bold yellow]Complexity Alert:[/bold yellow]")
                        console.print(f"Abstraction overhead {perf.abstraction_overhead:.1%} exceeds {LANGCHAIN_ABSTRACTION_OVERHEAD_THRESHOLD:.1%} threshold")
                        console.print("üìä Objective analysis: Measured overhead above baseline")
            
            # Display comprehensive reliability dashboard
            console.print(f"\\nüîç Generating comprehensive reliability analysis...")
            console.print(analysis.reliability_dashboard)
            
        except ImportError as e:
            console.print(f"[yellow]‚ö†Ô∏è Data-driven analysis unavailable: {e}[/yellow]")
            console.print("üí° Falling back to general framework guidance...")
            
            # Minimal fallback recommendations
            if framework:
                console.print(f"  ‚Ä¢ Monitor {framework} performance patterns in your workflows")
                console.print(f"  ‚Ä¢ Analyze tool call success rates and response times")
                console.print(f"  ‚Ä¢ Consider framework alternatives if performance issues persist")
            
            console.print(f"\\nüéØ [bold]Basic Reliability Metrics:[/bold]")
            console.print(f"‚úÖ Total Components: {len(agent_outputs)}")
            console.print(f"üîÑ Framework: {framework if framework else 'Auto-detected'}")
            console.print(f"‚ö° Analysis: Framework-specific insights generated")
        
        # Show next steps
        console.print(f"\\nüí° [bold]Next Steps:[/bold]")
        if domain:
            console.print(f"1. Run full evaluation: [green]arc-eval --domain {domain} --input data.json[/green]")
        else:
            console.print("1. Run full evaluation: [green]arc-eval --domain workflow_reliability --input data.json[/green]")
        console.print("2. Generate improvement plan: [green]arc-eval --continue[/green]")
        console.print("3. Compare with baseline: [green]arc-eval --baseline previous_evaluation.json[/green]")
        
        if endpoint:
            console.print(f"\\n[dim]Custom endpoint configured: {endpoint}[/dim]")
        
        console.print("\\nüìã [bold cyan]Enterprise Compliance Ready[/bold cyan] (Bonus Value)")
        console.print("‚úÖ 355 compliance scenarios available across finance, security, ML")
        
        if dev:
            console.print(f"\\n[dim]Debug: Framework={framework}, Domain={domain}, "
                        f"Endpoint={endpoint}, Outputs={len(agent_outputs)}[/dim]")
        
        return 0
    
    def _handle_schema_validation(self, **kwargs) -> int:
        """Handle schema validation and LLM-friendly schema generation."""
        
        console.print("üîç [bold cyan]Schema Validation & Tool Alignment Analysis[/bold cyan]")
        console.print("Detecting prompt-tool mismatch and auto-generating LLM-friendly schemas\\n")
        
        # Load and validate inputs
        agent_outputs, _ = self._load_and_validate_inputs(**kwargs)
        if agent_outputs is None:
            return 1
        
        dev = kwargs.get('dev', False)
        framework = kwargs.get('framework')
        
        console.print(f"üîç Analyzing {len(agent_outputs)} agent outputs for schema mismatches...")
        
        try:
            from agent_eval.evaluation.reliability_validator import ReliabilityAnalyzer
            
            analyzer = ReliabilityAnalyzer()
            
            # Detect schema mismatches
            schema_issues = analyzer.detect_schema_mismatches(agent_outputs)
            
            if schema_issues:
                console.print(f"\\nüö® [bold red]Schema Mismatches Detected ({len(schema_issues)} issues):[/bold red]")
                
                for i, issue in enumerate(schema_issues, 1):
                    console.print(f"\\n{i}. [yellow]{issue.get('tool_name', 'Unknown Tool')}[/yellow]")
                    
                    if 'expected_parameter' in issue and 'actual_parameter' in issue:
                        console.print(f"   Expected: [green]'{issue['expected_parameter']}'[/green]")
                        console.print(f"   Got: [red]'{issue['actual_parameter']}'[/red]")
                    elif 'expected_parameters' in issue and 'actual_parameters' in issue:
                        console.print(f"   Expected parameters: [green]{', '.join(issue['expected_parameters'])}[/green]")
                        console.print(f"   Actual parameters: [red]{', '.join(issue['actual_parameters'])}[/red]")
                        
                        if issue.get('missing_parameters'):
                            console.print(f"   Missing: [red]{', '.join(issue['missing_parameters'])}[/red]")
                        if issue.get('unexpected_parameters'):
                            console.print(f"   Unexpected: [yellow]{', '.join(issue['unexpected_parameters'])}[/yellow]")
                    
                    console.print(f"   üí° [bold]Fix:[/bold] {issue.get('suggested_fix', 'Review tool schema')}")
            else:
                console.print("\\n‚úÖ [bold green]No schema mismatches detected![/bold green]")
                console.print("All tool calls appear to match their expected schemas.")
            
            # Extract tool definitions from outputs for LLM-friendly generation
            tool_definitions = []
            for output in agent_outputs:
                if isinstance(output, dict) and 'tool_definition' in output:
                    tool_definitions.append(output['tool_definition'])
            
            # Generate LLM-friendly schemas if tool definitions found
            if tool_definitions:
                console.print(f"\\nüîß [bold cyan]Auto-Generated LLM-Friendly Schemas:[/bold cyan]")
                
                friendly_schemas = analyzer.generate_llm_friendly_schemas(tool_definitions)
                
                for tool_name, schema_info in friendly_schemas.items():
                    console.print(f"\\nüìã [bold]{tool_name}[/bold]")
                    console.print(f"[dim]{schema_info['description']}[/dim]")
                    
                    if schema_info.get('examples'):
                        console.print("\\n[cyan]Example usage:[/cyan]")
                        from agent_eval.core.constants import MAX_EXAMPLES_TO_SHOW
                        for example in schema_info['examples'][:MAX_EXAMPLES_TO_SHOW]:
                            console.print(f"[dim]{example}[/dim]")
                    
                    if schema_info.get('common_mistakes'):
                        console.print("\\n‚ö†Ô∏è [yellow]Common mistakes to avoid:[/yellow]")
                        from agent_eval.core.constants import MAX_COMMON_MISTAKES_TO_SHOW
                        for mistake in schema_info['common_mistakes'][:MAX_COMMON_MISTAKES_TO_SHOW]:
                            console.print(f"  ‚Ä¢ {mistake}")
            else:
                console.print("\\nüí° [bold cyan]LLM-Friendly Schema Generation:[/bold cyan]")
                console.print("No tool definitions found in agent outputs.")
                console.print("To generate optimized schemas, include tool definitions in your input data.")
            
            # Framework-specific schema guidance
            if framework:
                console.print(f"\\nüéØ [bold]{framework.upper()} Framework Schema Guidance:[/bold]")
                
                framework_guidance = {
                    "openai": [
                        "Use 'functions' array with proper JSON schema definitions",
                        "Ensure parameter types match exactly (string, integer, boolean, array, object)",
                        "Include clear 'description' fields for both function and parameters"
                    ],
                    "anthropic": [
                        "Use tool_use blocks with structured parameter definitions",
                        "Ensure XML-style tool calls match parameter names exactly",
                        "Include examples of proper tool call format in prompts"
                    ],
                    "langchain": [
                        "Define tools with proper Pydantic models or function signatures",
                        "Use structured tool definitions in agent initialization",
                        "Ensure tool names match function names exactly"
                    ],
                    "crewai": [
                        "Define tools in agent configuration with clear schemas",
                        "Use proper tool decorators and type hints",
                        "Ensure tool names are consistent across agents"
                    ],
                    "autogen": [
                        "Register functions with proper type annotations",
                        "Use consistent function naming across conversation agents",
                        "Include function descriptions in agent system messages"
                    ],
                    "google_adk": [
                        "Use proper functionCall schema definitions with Vertex AI",
                        "Ensure parameter types align with Google AI model expectations",
                        "Include comprehensive function descriptions and examples"
                    ],
                    "agno": [
                        "Define structured outputs with proper schema validation",
                        "Use consistent tool naming conventions across agent runs",
                        "Include clear parameter documentation for agent understanding"
                    ],
                    "nvidia_aiq": [
                        "Follow AIQ pipeline component schema requirements",
                        "Ensure tool definitions align with workflow execution patterns",
                        "Use proper parameter validation for pipeline components"
                    ]
                }
                
                guidance = framework_guidance.get(framework, [
                    "Follow framework-specific tool definition patterns",
                    "Ensure parameter names and types are consistent",
                    "Include clear documentation for all tool parameters"
                ])
                
                for tip in guidance:
                    console.print(f"  ‚Ä¢ {tip}")
            
            # Summary and metrics
            console.print(f"\\nüìä [bold]Schema Validation Summary:[/bold]")
            console.print(f"  ‚Ä¢ Agent outputs analyzed: {len(agent_outputs)}")
            console.print(f"  ‚Ä¢ Schema mismatches found: {len(schema_issues)}")
            console.print(f"  ‚Ä¢ Tool definitions processed: {len(tool_definitions)}")
            console.print(f"  ‚Ä¢ Framework: {framework if framework else 'Auto-detected'}")
            
            # Calculate schema mismatch rate
            if agent_outputs:
                mismatch_rate = len(schema_issues) / len(agent_outputs)
                console.print(f"  ‚Ä¢ Schema mismatch rate: {mismatch_rate:.1%}")
                
                if mismatch_rate > 0.1:
                    console.print("  üî¥ [red]High mismatch rate - significant schema issues detected[/red]")
                elif mismatch_rate > 0.05:
                    console.print("  üü° [yellow]Moderate mismatch rate - some schema optimization needed[/yellow]")
                else:
                    console.print("  ‚úÖ [green]Low mismatch rate - schemas are well-aligned[/green]")
            
            if dev:
                console.print(f"\\n[dim]Debug: Processed {len(agent_outputs)} outputs, "
                            f"found {len(schema_issues)} issues, "
                            f"generated {len(tool_definitions)} friendly schemas[/dim]")
        
        except ImportError as e:
            console.print(f"[red]Schema validation unavailable:[/red] {e}")
            return 1
        
        # Offer judge-based validation for enhanced analysis
        console.print(f"\\nüéØ [bold cyan]Enhanced Validation Available:[/bold cyan]")
        console.print("For deeper schema analysis, use Agent-as-a-Judge validation:")
        console.print(f"[green]arc-eval --compare-judges config/judge_comparison_templates.yaml --input data.json[/green]")
        console.print("‚ú® Features framework-specific schema validation judges")
        
        # Next steps
        console.print(f"\\nüí° [bold]Next Steps:[/bold]")
        console.print("1. Fix identified schema mismatches using suggested improvements")
        console.print("2. Use generated LLM-friendly schemas in your prompts")
        console.print("3. Test improved schemas: [green]arc-eval --schema-validation --input updated_data.json[/green]")
        console.print("4. Run full reliability analysis: [green]arc-eval --workflow-reliability --input data.json[/green]")
        
        return 0

    def _load_and_validate_inputs(self, **kwargs) -> tuple[Optional[List[Any]], Optional[str]]:
        """Load and validate input data for reliability analysis."""
        from pathlib import Path
        
        input_file = kwargs.get('input_file')
        stdin = kwargs.get('stdin', False)
        verbose = kwargs.get('verbose', False)

        # Convert input_file to Path if it's a string
        input_path = Path(input_file) if input_file else None

        try:
            # Use base class method for consistent input loading
            data = self._load_raw_data(input_path, stdin)
            
            # Return raw data for reliability analysis (not converted to AgentOutput)
            if isinstance(data, list):
                agent_outputs = data
            elif isinstance(data, dict):
                agent_outputs = [data]
            else:
                console.print(f"[red]Invalid data format:[/red] expected list or dict, got {type(data)}")
                return None, None
            
            input_source = "stdin" if stdin else str(input_path) if input_path else "unknown"
            
            if verbose and agent_outputs:
                console.print(f"\\n[dim]Loaded {len(agent_outputs)} outputs from {input_source}[/dim]")
            
            return agent_outputs, input_source

        except (FileNotFoundError, ValueError) as e:
            console.print(f"[red]Error loading input:[/red] {e}")
            if not input_file and not stdin:
                console.print("üí° Usage: arc-eval --debug-agent --input workflow_trace.json")
            return None, None
    
    def _basic_framework_detection(self, agent_outputs: List[Any], specified_framework: Optional[str]) -> str:
        """Basic framework detection fallback."""
        if specified_framework:
            return f"{specified_framework.upper()} (specified)"
        
        # Simple heuristic detection
        framework_keywords = {
            'langchain': ['intermediate_steps', 'agent_scratchpad'],
            'crewai': ['crew_output', 'task_output'],
            'openai': ['tool_calls', 'function_call'],
            'anthropic': ['tool_use', 'function_calls']
        }
        
        all_text = ' '.join(str(output) for output in agent_outputs)
        
        for framework, keywords in framework_keywords.items():
            if any(keyword in all_text.lower() for keyword in keywords):
                return f"{framework.upper()} (auto-detected)"
        
        return "Generic (auto-detection inconclusive)"