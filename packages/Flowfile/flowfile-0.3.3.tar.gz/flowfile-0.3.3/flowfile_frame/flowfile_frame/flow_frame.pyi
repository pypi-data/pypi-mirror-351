# This file was auto-generated to provide type information for FlowFrame
# DO NOT MODIFY THIS FILE MANUALLY
import collections
import typing
import inspect
from typing import List, Dict, Optional, Union, ForwardRef
import polars as pl
from polars.lazyframe.frame import *
from polars._typing import SchemaDict
from polars.type_aliases import ColumnNameOrSelector, FillNullStrategy

import flowfile_frame
from flowfile_core.flowfile.flow_node.flow_node import FlowNode
from flowfile_core.flowfile.FlowfileFlow import FlowGraph
from flowfile_frame import group_frame
from flowfile_frame.expr import Expr, Column
from flowfile_frame.selectors import Selector

T = TypeVar('T')
FlowFrameT = TypeVar('FlowFrameT', bound='FlowFrame')
# Define NoneType to handle type hints with None
NoneType = type(None)

# Module-level functions
def can_be_expr(param: inspect.Parameter) -> bool: ...
def generate_node_id() -> int: ...
def get_method_name_from_code(code: str) -> str | None: ...
def _contains_lambda_pattern(text: str) -> bool: ...
def _to_string_val(v) -> str: ...
def _extract_expr_parts(expr_obj) -> tuple[str, str]: ...
def _check_ok_for_serialization(method_name: str = None, polars_expr: pl.Expr | None = None, group_expr: pl.Expr | None = None) -> None: ...

class FlowFrame:
    data: LazyFrame
    flow_graph: FlowGraph
    node_id: int
    parent_node_id: Optional[int]

    # This special method determines how the object behaves in boolean contexts.
    def __bool__(self, ) -> Any: ...

    # This special method enables the 'in' operator to work with FlowFrame objects.
    def __contains__(self, key) -> Any: ...

    def __eq__(self, other: object) -> typing.NoReturn: ...

    def __ge__(self, other: Any) -> typing.NoReturn: ...

    def __gt__(self, other: Any) -> typing.NoReturn: ...

    # Initialize the FlowFrame with data and graph references.
    def __init__(self, data: Union[LazyFrame, Mapping, Sequence, ForwardRef('np.ndarray[Any, Any]'), ForwardRef('pa.Table'), ForwardRef('pd.DataFrame'), ForwardRef('ArrowArrayExportable'), ForwardRef('ArrowStreamExportable')]=None, schema: Union[Mapping, Sequence, NoneType]=None, schema_overrides: collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType')]] | None=None, strict: bool=True, orient: Union[Literal, NoneType]=None, infer_schema_length: int | None=100, nan_to_null: bool=False, flow_graph=None, node_id=None, parent_node_id=None) -> Any: ...

    def __le__(self, other: Any) -> typing.NoReturn: ...

    def __lt__(self, other: Any) -> typing.NoReturn: ...

    def __ne__(self, other: object) -> typing.NoReturn: ...

    # Create a new FlowFrame instance.
    def __new__(self, cls, data: Union[LazyFrame, Mapping, Sequence, ForwardRef('np.ndarray[Any, Any]'), ForwardRef('pa.Table'), ForwardRef('pd.DataFrame'), ForwardRef('ArrowArrayExportable'), ForwardRef('ArrowStreamExportable')]=None, schema: Union[Mapping, Sequence, NoneType]=None, schema_overrides: collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType')]] | None=None, strict: bool=True, orient: Union[Literal, NoneType]=None, infer_schema_length: int | None=100, nan_to_null: bool=False, flow_graph=None, node_id=None, parent_node_id=None) -> Any: ...

    def __repr__(self, ) -> Any: ...

    # Helper method to add a connection between nodes
    def _add_connection(self, from_id, to_id, input_type: typing.Literal['main', 'left', 'right']='main') -> Any: ...

    def _add_number_of_records(self, new_node_id: int, description: str=None) -> 'FlowFrame': ...

    def _add_polars_code(self, new_node_id: int, code: str, description: str=None, depending_on_ids: Union[List, NoneType]=None, convertable_to_code: bool=True, method_name: str=None, polars_expr: Union[Expr, List, NoneType]=None, group_expr: Union[Expr, List, NoneType]=None, kwargs_expr: Union[Dict, NoneType]=None, group_kwargs: Union[Dict, NoneType]=None) -> Any: ...

    def _comparison_error(self, operator: str) -> typing.NoReturn: ...

    # Helper method to create a new FlowFrame that's a child of this one
    def _create_child_frame(self, new_node_id) -> 'FlowFrame': ...

    # Detect if the expression is a cum_count operation and use record_id if possible.
    def _detect_cum_count_record_id(self, expr: Any, new_node_id: int, description: Union[str, NoneType]=None) -> 'FlowFrame': ...

    # Generates the `input_df.sort(...)` Polars code string using pure expression strings.
    def _generate_sort_polars_code(self, pure_sort_expr_strs: typing.List[str], descending_values: typing.List[bool], nulls_last_values: typing.List[bool], multithreaded: bool, maintain_order: bool) -> str: ...

    def _with_flowfile_formula(self, flowfile_formula: str, output_column_name, description: str=None) -> 'FlowFrame': ...

    # Approximate count of unique values.
    def approx_n_unique(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Return the `k` smallest rows.
    def bottom_k(self, k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool]=False, description: Optional[str] = None) -> 'FlowFrame': ...

    def cache(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Cast LazyFrame column(s) to the specified dtype(s).
    def cast(self, dtypes: Mapping[ColumnNameOrSelector | PolarsDataType, PolarsDataType | PythonDataType] | PolarsDataType, strict: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create an empty copy of the current LazyFrame, with zero to 'n' rows.
    def clear(self, n: int=0, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create a copy of this LazyFrame.
    def clone(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Collect lazy data into memory.
    def collect(self, *args, **kwargs) -> Any: ...

    # Collect DataFrame asynchronously in thread pool.
    def collect_async(self, gevent: bool=False, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, no_optimization: bool=False, slice_pushdown: bool=True, comm_subplan_elim: bool=True, comm_subexpr_elim: bool=True, cluster_with_columns: bool=True, collapse_joins: bool=True, engine: EngineType='auto', _check_order: bool=True) -> Awaitable[DataFrame] | _GeventDataFrameResult[DataFrame]: ...

    # Resolve the schema of this LazyFrame.
    def collect_schema(self, ) -> Schema: ...

    # Combine multiple FlowFrames into a single FlowFrame.
    def concat(self, other: Union[ForwardRef('FlowFrame'), List], how: str='vertical', rechunk: bool=False, parallel: bool=True, description: str=None) -> 'FlowFrame': ...

    # Return the number of non-null elements for each column.
    def count(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Simple naive implementation of creating the frame from any type. It converts the data to a polars frame,
    def create_from_any_type(self, data: Union[Mapping, Sequence, ForwardRef('np.ndarray[Any, Any]'), ForwardRef('pa.Table'), ForwardRef('pd.DataFrame'), ForwardRef('ArrowArrayExportable'), ForwardRef('ArrowStreamExportable')]=None, schema: Union[Mapping, Sequence, NoneType]=None, schema_overrides: collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType')]] | None=None, strict: bool=True, orient: Union[Literal, NoneType]=None, infer_schema_length: int | None=100, nan_to_null: bool=False, flow_graph=None, node_id=None, parent_node_id=None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Creates a summary of statistics for a LazyFrame, returning a DataFrame.
    def describe(self, percentiles: Sequence[float] | float | None=(0.25, 0.5, 0.75), interpolation: RollingInterpolationMethod='nearest') -> DataFrame: ...

    # Read a logical plan from a file to construct a LazyFrame.
    def deserialize(self, source: str | Path | IOBase, format: SerializationFormat='binary', description: Optional[str] = None) -> 'FlowFrame': ...

    # Remove columns from the DataFrame.
    def drop(self, *columns, strict: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop all rows that contain one or more NaN values.
    def drop_nans(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None=None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop all rows that contain one or more null values.
    def drop_nulls(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None=None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create a string representation of the query plan.
    def explain(self, format: ExplainFormat='plain', optimized: bool=True, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, slice_pushdown: bool=True, comm_subplan_elim: bool=True, comm_subexpr_elim: bool=True, cluster_with_columns: bool=True, collapse_joins: bool=True, streaming: bool=False, engine: EngineType='auto', tree_format: bool | None=None, _check_order: bool=True) -> str: ...

    # Explode the dataframe to long format by exploding the given columns.
    def explode(self, columns: Union[str, Column, Iterable], *more_columns, description: str=None) -> 'FlowFrame': ...

    # Collect a small number of rows for debugging purposes.
    def fetch(self, n_rows: int=500, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, no_optimization: bool=False, slice_pushdown: bool=True, comm_subplan_elim: bool=True, comm_subexpr_elim: bool=True, cluster_with_columns: bool=True, collapse_joins: bool=True) -> DataFrame: ...

    # Fill floating point NaN values.
    def fill_nan(self, value: int | float | Expr | None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Fill null values using the specified value or strategy.
    def fill_null(self, value: Any | Expr | None=None, strategy: FillNullStrategy | None=None, limit: int | None=None, matches_supertype: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Filter rows based on a predicate.
    def filter(self, *predicates, flowfile_formula: Union[str, NoneType]=None, description: Union[str, NoneType]=None, **constraints) -> 'FlowFrame': ...

    # Get the first row of the DataFrame.
    def first(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Take every nth row in the LazyFrame and return as a new LazyFrame.
    def gather_every(self, n: int, offset: int=0, description: Optional[str] = None) -> 'FlowFrame': ...

    def get_node_settings(self, description: Optional[str] = None) -> FlowNode: ...

    # Start a group by operation.
    def group_by(self, *by, description: Optional[str] = None, maintain_order: bool = False, **named_by) -> group_frame.GroupByFrame: ...

    # Group based on a time value (or index value of type Int32, Int64).
    def group_by_dynamic(self, index_column: IntoExpr, every: str | timedelta, period: str | timedelta | None=None, offset: str | timedelta | None=None, include_boundaries: bool=False, closed: ClosedInterval='left', label: Label='left', group_by: IntoExpr | Iterable[IntoExpr] | None=None, start_by: StartBy='window', description: Optional[str] = None) -> LazyGroupBy: ...

    def head(self, n: int, description: str=None) -> 'FlowFrame': ...

    # Inspect a node in the computation graph.
    def inspect(self, fmt: str='{}', description: Optional[str] = None) -> 'FlowFrame': ...

    # Interpolate intermediate values. The interpolation method is linear.
    def interpolate(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a join operation to the Logical Plan.
    def join(self, other, on: Union[List, str, Column]=None, how: str='inner', left_on: Union[List, str, Column]=None, right_on: Union[List, str, Column]=None, suffix: str='_right', validate: str=None, nulls_equal: bool=False, coalesce: bool=None, maintain_order: typing.Literal[None, 'left', 'right', 'left_right', 'right_left']=None, description: str=None) -> 'FlowFrame': ...

    # Perform an asof join.
    def join_asof(self, other: LazyFrame, left_on: str | None | Expr=None, right_on: str | None | Expr=None, on: str | None | Expr=None, by_left: str | Sequence[str] | None=None, by_right: str | Sequence[str] | None=None, by: str | Sequence[str] | None=None, strategy: AsofJoinStrategy='backward', suffix: str='_right', tolerance: str | int | float | timedelta | None=None, allow_parallel: bool=True, force_parallel: bool=False, coalesce: bool=True, allow_exact_matches: bool=True, check_sortedness: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Perform a join based on one or multiple (in)equality predicates.
    def join_where(self, other: LazyFrame, *predicates, suffix: str='_right', description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the last row of the DataFrame.
    def last(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Return lazy representation, i.e. itself.
    def lazy(self, description: Optional[str] = None) -> 'FlowFrame': ...

    def limit(self, n: int, description: str=None) -> 'FlowFrame': ...

    # Apply a custom function.
    def map_batches(self, function: Callable[[DataFrame], DataFrame], predicate_pushdown: bool=True, projection_pushdown: bool=True, slice_pushdown: bool=True, no_optimizations: bool=False, schema: None | SchemaDict=None, validate_output_schema: bool=True, streamable: bool=False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their maximum value.
    def max(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their mean value.
    def mean(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their median value.
    def median(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Unpivot a DataFrame from wide to long format.
    def melt(self, id_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None=None, value_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None=None, variable_name: str | None=None, value_name: str | None=None, streamable: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Take two sorted DataFrames and merge them by the sorted key.
    def merge_sorted(self, other: LazyFrame, key: str, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their minimum value.
    def min(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame as the sum of their null value count.
    def null_count(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Offers a structured way to apply a sequence of user-defined functions (UDFs).
    def pipe(self, function: Callable[Concatenate[LazyFrame, P], T], *args, **kwargs) -> T: ...

    # Pivot a DataFrame from long to wide format.
    def pivot(self, on: str | list[str], index: str | list[str] | None=None, values: str | list[str] | None=None, aggregate_function: str | None='first', maintain_order: bool=True, sort_columns: bool=False, separator: str='_', description: str=None) -> 'FlowFrame': ...

    # Profile a LazyFrame.
    def profile(self, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, no_optimization: bool=False, slice_pushdown: bool=True, comm_subplan_elim: bool=True, comm_subexpr_elim: bool=True, cluster_with_columns: bool=True, collapse_joins: bool=True, show_plot: bool=False, truncate_nodes: int=0, figsize: tuple[int, int]=(18, 8), engine: EngineType='auto', _check_order: bool=True, **_kwargs) -> tuple[DataFrame, DataFrame]: ...

    # Aggregate the columns in the LazyFrame to their quantile value.
    def quantile(self, quantile: float | Expr, interpolation: RollingInterpolationMethod='nearest', description: Optional[str] = None) -> 'FlowFrame': ...

    # Run a query remotely on Polars Cloud.
    def remote(self, context: pc.ComputeContext | None=None, plan_type: pc._typing.PlanTypePreference='dot', description: Optional[str] = None) -> 'FlowFrame': ...

    # Remove rows, dropping those that match the given predicate expression(s).
    def remove(self, *predicates, **constraints) -> 'FlowFrame': ...

    # Rename column names.
    def rename(self, mapping: dict[str, str] | Callable[[str], str], strict: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Reverse the DataFrame.
    def reverse(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create rolling groups based on a temporal or integer column.
    def rolling(self, index_column: IntoExpr, period: str | timedelta, offset: str | timedelta | None=None, closed: ClosedInterval='right', group_by: IntoExpr | Iterable[IntoExpr] | None=None, description: Optional[str] = None) -> LazyGroupBy: ...

    # Save the graph
    def save_graph(self, file_path: str, auto_arrange: bool=True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Select columns from the frame.
    def select(self, *columns, description: Union[str, NoneType]=None) -> 'FlowFrame': ...

    # Select columns from this LazyFrame.
    def select_seq(self, *exprs, **named_exprs) -> 'FlowFrame': ...

    # Serialize the logical plan of this LazyFrame to a file or string in JSON format.
    def serialize(self, file: IOBase | str | Path | None=None, format: SerializationFormat='binary', description: Optional[str] = None) -> bytes | str | None: ...

    # Flag a column as sorted.
    def set_sorted(self, column: str, descending: bool=False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Shift values by the given number of indices.
    def shift(self, n: int | IntoExprColumn=1, fill_value: IntoExpr | None=None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Show a plot of the query plan.
    def show_graph(self, optimized: bool=True, show: bool=True, output_path: str | Path | None=None, raw_output: bool=False, figsize: tuple[float, float]=(16.0, 12.0), type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, slice_pushdown: bool=True, comm_subplan_elim: bool=True, comm_subexpr_elim: bool=True, cluster_with_columns: bool=True, collapse_joins: bool=True, streaming: bool=False, engine: EngineType='auto', _check_order: bool=True) -> str | None: ...

    # Write the data to a CSV file.
    def sink_csv(self, file: str, *args, separator: str=',', encoding: str='utf-8', description: str=None) -> 'FlowFrame': ...

    # Evaluate the query in streaming mode and write to an IPC file.
    def sink_ipc(self, path: str | Path, compression: IpcCompression | None='zstd', compat_level: CompatLevel | None=None, maintain_order: bool=True, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, slice_pushdown: bool=True, collapse_joins: bool=True, no_optimization: bool=False, storage_options: dict[str, Any] | None=None, credential_provider: CredentialProviderFunction | Literal['auto'] | None='auto', retries: int=2, sync_on_close: SyncOnCloseMethod | None=None, mkdir: bool=False, lazy: bool=False, engine: EngineType='auto', description: Optional[str] = None) -> 'FlowFrame': ...

    # Evaluate the query in streaming mode and write to an NDJSON file.
    def sink_ndjson(self, path: str | Path, maintain_order: bool=True, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, slice_pushdown: bool=True, collapse_joins: bool=True, no_optimization: bool=False, storage_options: dict[str, Any] | None=None, credential_provider: CredentialProviderFunction | Literal['auto'] | None='auto', retries: int=2, sync_on_close: SyncOnCloseMethod | None=None, mkdir: bool=False, lazy: bool=False, engine: EngineType='auto', description: Optional[str] = None) -> 'FlowFrame': ...

    # Evaluate the query in streaming mode and write to a Parquet file.
    def sink_parquet(self, path: str | Path, compression: str='zstd', compression_level: int | None=None, statistics: bool | str | dict[str, bool]=True, row_group_size: int | None=None, data_page_size: int | None=None, maintain_order: bool=True, type_coercion: bool=True, _type_check: bool=True, predicate_pushdown: bool=True, projection_pushdown: bool=True, simplify_expression: bool=True, slice_pushdown: bool=True, collapse_joins: bool=True, no_optimization: bool=False, storage_options: dict[str, Any] | None=None, credential_provider: CredentialProviderFunction | Literal['auto'] | None='auto', retries: int=2, sync_on_close: SyncOnCloseMethod | None=None, mkdir: bool=False, lazy: bool=False, engine: EngineType='auto', description: Optional[str] = None) -> 'FlowFrame': ...

    # Get a slice of this DataFrame.
    def slice(self, offset: int, length: int | None=None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Sort the dataframe by the given columns.
    def sort(self, by: Union[List, Expr, str], *more_by, descending: Union[bool, List]=False, nulls_last: Union[bool, List]=False, multithreaded: bool=True, maintain_order: bool=False, description: Union[str, NoneType]=None) -> 'FlowFrame': ...

    # Execute a SQL query against the LazyFrame.
    def sql(self, query: str, table_name: str='self', description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their standard deviation value.
    def std(self, ddof: int=1, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their sum value.
    def sum(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the last `n` rows.
    def tail(self, n: int=5, description: Optional[str] = None) -> 'FlowFrame': ...

    # Split text in a column into multiple rows.
    def text_to_rows(self, column: str | flowfile_frame.expr.Column, output_column: str=None, delimiter: str=None, split_by_column: str=None, description: str=None) -> 'FlowFrame': ...

    # Get the underlying ETL graph.
    def to_graph(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Return the `k` largest rows.
    def top_k(self, k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool]=False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop duplicate rows from this dataframe.
    def unique(self, subset: Union[str, ForwardRef('Expr'), List]=None, keep: typing.Literal['first', 'last', 'any', 'none']='any', maintain_order: bool=False, description: str=None) -> 'FlowFrame': ...

    # Decompose struct columns into separate columns for each of their fields.
    def unnest(self, columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns, description: Optional[str] = None) -> 'FlowFrame': ...

    # Unpivot a DataFrame from wide to long format.
    def unpivot(self, on: list[str | flowfile_frame.selectors.Selector] | str | None | flowfile_frame.selectors.Selector=None, index: list[str] | str | None=None, variable_name: str='variable', value_name: str='value', description: str=None) -> 'FlowFrame': ...

    # Update the values in this `LazyFrame` with the values in `other`.
    def update(self, other: LazyFrame, on: str | Sequence[str] | None=None, how: Literal['left', 'inner', 'full']='left', left_on: str | Sequence[str] | None=None, right_on: str | Sequence[str] | None=None, include_nulls: bool=False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their variance value.
    def var(self, ddof: int=1, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add or replace columns in the DataFrame.
    def with_columns(self, exprs: Union[Expr, List[Union[Expr, None]]] = None, *, flowfile_formulas: Optional[List[str]] = None, output_column_names: Optional[List[str]] = None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add columns to this LazyFrame.
    def with_columns_seq(self, *exprs, **named_exprs) -> 'FlowFrame': ...

    # Add an external context to the computation graph.
    def with_context(self, other: Self | list[Self], description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a column at index 0 that counts the rows.
    def with_row_count(self, name: str='row_nr', offset: int=0, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a row index as the first column in the DataFrame.
    def with_row_index(self, name: str='index', offset: int=0, description: str=None) -> 'FlowFrame': ...

    def write_csv(self, file: str | os.PathLike, separator: str=',', encoding: str='utf-8', description: str=None, convert_to_absolute_path: bool=True, **kwargs) -> 'FlowFrame': ...

    # Write the data to a Parquet file. Creates a standard Output node if only
    def write_parquet(self, path: str | os.PathLike, description: str=None, convert_to_absolute_path: bool=True, **kwargs) -> 'FlowFrame': ...
