 <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"> 
<html lang="en">
<head>
<title>
babyGPT-1.0.6.html
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body bgcolor="#f0f0f8">
<table width="100%" cellspacing="0" cellpadding="2" border="0" summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong>babyGPT</strong></big></big> (version 1.0.6, 2025-April-22)</font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial">
</font></td></tr></table>
<p><a href="#babyGPT"><tt>babyGPT</tt></a>.py<br>
<tt>
&nbsp;<br>
Version:&nbsp;1.0.6<br>
&nbsp;&nbsp;&nbsp;<br>
Author:&nbsp;Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)<br>
&nbsp;<br>
Date:&nbsp;2025-April-22<br>
&nbsp;<br>
&nbsp;<br>
</tt>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="2"> 
<TR>
<TH ALIGN=left>
<tt>
<b>Download Version 1.0.6:</b>&nbsp;  
<a HREF="https://engineering.purdue.edu/kak/distBabyGPT/babyGPT-1.0.6.tar.gz?download">gztar</a> 
&nbsp;             
<br>
<br>
&nbsp;
</tt>
</TH>
<TD>
<tt>
&nbsp;&nbsp;&nbsp;&nbsp;
Total number of downloads (all versions): 
<?php   
    $file = fopen("HowManyCounts.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
</tt>
<br>
<center>
<tt>
<font color="red" size="-2">
&nbsp;&nbsp;&nbsp;&nbsp;
This count is automatically updated at every rotation of
<br> 
&nbsp;&nbsp;&nbsp;&nbsp;
the weblogs (normally once every two to four days)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
Last updated:
<?php   
    $file = fopen("LastUpdated.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
</font>
</tt>
</center>
</TD>
</TR>
</TABLE>
<br>
<tt>
<a HREF="babyGPT-1.0.6_CodeOnly.html">View the main module code file in your browser</a> 
&nbsp;<br>
&nbsp;<br>
<a HREF="datasets_for_babyGPT.tar.gz">Download the text datasets for babyGPT</a>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>   
&nbsp;<br>
<font size="+2" color="red">CHANGES:<br>
</font>
<br>

&nbsp;&nbsp;Version&nbsp;1.0.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;fixed&nbsp;the&nbsp;error&nbsp;that&nbsp;caused&nbsp;the&nbsp;predicted&nbsp;tokens&nbsp;to&nbsp;be&nbsp;shifted&nbsp;by&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;position&nbsp;vis-a-vis&nbsp;the&nbsp;ground-truth&nbsp;tokens.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.5:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Had&nbsp;a&nbsp;URL&nbsp;error&nbsp;in&nbsp;the&nbsp;setup.py&nbsp;of&nbsp;the&nbsp;previous&nbsp;version.&nbsp;The&nbsp;rest&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;remains&nbsp;unchanged.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;first&nbsp;public&nbsp;release&nbsp;version&nbsp;of&nbsp;the&nbsp;module.&nbsp;This&nbsp;module&nbsp;was&nbsp;created<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;at&nbsp;Purdue&nbsp;University.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">INTRODUCTION:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;SPECIFIC&nbsp;GOALS&nbsp;FOR&nbsp;THIS&nbsp;MODULE:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;1)&nbsp;To&nbsp;introduce&nbsp;the&nbsp;students&nbsp;in&nbsp;Purdue's&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;to&nbsp;the&nbsp;foundational<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;concepts&nbsp;in&nbsp;how&nbsp;to&nbsp;create&nbsp;a&nbsp;Base&nbsp;Language&nbsp;Model&nbsp;through&nbsp;self-supervised<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning.&nbsp;&nbsp;Large&nbsp;Language&nbsp;Models&nbsp;start&nbsp;out&nbsp;as&nbsp;Base&nbsp;Models&nbsp;that&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subsequently&nbsp;fine-tuned&nbsp;with&nbsp;reinforcement&nbsp;learning.&nbsp;&nbsp;The&nbsp;focus&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;solely&nbsp;on&nbsp;Base&nbsp;Modeling.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;2)&nbsp;To&nbsp;demonstrate&nbsp;small-scale&nbsp;language&nbsp;modeling&nbsp;that,&nbsp;for&nbsp;educational&nbsp;purposes,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;can&nbsp;be&nbsp;run&nbsp;on&nbsp;a&nbsp;typical&nbsp;university&nbsp;lab&nbsp;GPU.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;3)&nbsp;To&nbsp;create&nbsp;self-contained&nbsp;module&nbsp;that,&nbsp;given&nbsp;a&nbsp;set&nbsp;of&nbsp;media&nbsp;URLs,&nbsp;will&nbsp;download<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;articles&nbsp;from&nbsp;those&nbsp;websites,&nbsp;train&nbsp;a&nbsp;BPE&nbsp;tokenizer&nbsp;from&nbsp;the&nbsp;corpus&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;articles&nbsp;collected,&nbsp;and&nbsp;let&nbsp;you&nbsp;create&nbsp;a&nbsp;Base&nbsp;Model&nbsp;from&nbsp;the&nbsp;corpus&nbsp;that&nbsp;you&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;can&nbsp;play&nbsp;with&nbsp;the&nbsp;prompting&nbsp;script&nbsp;in&nbsp;the&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;My&nbsp;main&nbsp;goal&nbsp;in&nbsp;babyGPT&nbsp;is&nbsp;to&nbsp;demonstrate&nbsp;that,&nbsp;for&nbsp;the&nbsp;purpose&nbsp;of&nbsp;teaching&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning,&nbsp;it&nbsp;is&nbsp;possible&nbsp;to&nbsp;create&nbsp;a&nbsp;small-scale&nbsp;end-to-end&nbsp;implementation&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;downloads&nbsp;a&nbsp;corpus&nbsp;of&nbsp;news&nbsp;media&nbsp;articles,&nbsp;trains&nbsp;a&nbsp;BPE&nbsp;tokenizer&nbsp;if&nbsp;you&nbsp;need&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;one&nbsp;for&nbsp;the&nbsp;domain&nbsp;of&nbsp;the&nbsp;corpus&nbsp;you&nbsp;have&nbsp;collected,&nbsp;and,&nbsp;finally,&nbsp;uses&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;corpus&nbsp;for&nbsp;training&nbsp;an&nbsp;autoregressive&nbsp;model&nbsp;for&nbsp;the&nbsp;next&nbsp;token&nbsp;prediction&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;unsupervised&nbsp;learning.&nbsp;After&nbsp;you&nbsp;have&nbsp;trained&nbsp;the&nbsp;model,&nbsp;you&nbsp;can&nbsp;test&nbsp;it&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;prompting&nbsp;script&nbsp;that&nbsp;is&nbsp;included&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory.&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;LANGUAGE&nbsp;MODELING&nbsp;AND&nbsp;UNSUPERVISED&nbsp;LEARNING:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;There&nbsp;is&nbsp;no&nbsp;denying&nbsp;the&nbsp;fact&nbsp;that&nbsp;the&nbsp;recent&nbsp;advances&nbsp;in&nbsp;chatbots&nbsp;have&nbsp;set&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;world&nbsp;on&nbsp;fire.&nbsp;It's&nbsp;truly&nbsp;amazing&nbsp;to&nbsp;see&nbsp;a&nbsp;chatbot&nbsp;returning&nbsp;(most&nbsp;of&nbsp;the&nbsp;time)&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;smooth-reading&nbsp;and&nbsp;well-structured&nbsp;narrative&nbsp;in&nbsp;response&nbsp;to&nbsp;a&nbsp;prompt.&nbsp;As&nbsp;if&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;were&nbsp;not&nbsp;enough,&nbsp;it&nbsp;can&nbsp;also&nbsp;supply&nbsp;you&nbsp;with&nbsp;variants&nbsp;of&nbsp;the&nbsp;same&nbsp;narrative<br>
&nbsp;&nbsp;&nbsp;&nbsp;depending&nbsp;on&nbsp;how&nbsp;you&nbsp;prompt&nbsp;it&nbsp;and&nbsp;your&nbsp;randomization&nbsp;settings&nbsp;for&nbsp;the&nbsp;bot.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;One&nbsp;would&nbsp;think&nbsp;that&nbsp;this&nbsp;degree&nbsp;of&nbsp;competency&nbsp;shown&nbsp;by&nbsp;a&nbsp;chatbot&nbsp;would&nbsp;require&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;vast&nbsp;amount&nbsp;of&nbsp;human&nbsp;annotated&nbsp;data&nbsp;for&nbsp;training&nbsp;the&nbsp;neural&nbsp;networks&nbsp;used&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;bot.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;truth&nbsp;is&nbsp;exactly&nbsp;the&nbsp;opposite.&nbsp;&nbsp;Most&nbsp;of&nbsp;the&nbsp;learning&nbsp;that&nbsp;takes&nbsp;place&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;order&nbsp;to&nbsp;train&nbsp;a&nbsp;chatbot&nbsp;is&nbsp;unsupervised&nbsp;---&nbsp;that&nbsp;is,&nbsp;without&nbsp;any&nbsp;human<br>
&nbsp;&nbsp;&nbsp;&nbsp;supervision.&nbsp;The&nbsp;bot&nbsp;is&nbsp;given&nbsp;the&nbsp;simplest&nbsp;of&nbsp;the&nbsp;goals:&nbsp;To&nbsp;predict&nbsp;the&nbsp;next&nbsp;word<br>
&nbsp;&nbsp;&nbsp;&nbsp;given&nbsp;the&nbsp;words&nbsp;that&nbsp;have&nbsp;been&nbsp;seen&nbsp;so&nbsp;far.&nbsp;&nbsp;To&nbsp;master&nbsp;this&nbsp;goal,&nbsp;the&nbsp;bot&nbsp;needs<br>
&nbsp;&nbsp;&nbsp;&nbsp;zero&nbsp;supervision.&nbsp;&nbsp;All&nbsp;it&nbsp;needs&nbsp;to&nbsp;do&nbsp;use&nbsp;its&nbsp;neural&nbsp;network&nbsp;make&nbsp;a&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;next&nbsp;word.&nbsp;&nbsp;And,&nbsp;at&nbsp;training&nbsp;time,&nbsp;should&nbsp;this&nbsp;prediction&nbsp;be&nbsp;wrong,&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;estimate&nbsp;the&nbsp;error&nbsp;made,&nbsp;to&nbsp;backpropagate&nbsp;that&nbsp;error&nbsp;and&nbsp;thus&nbsp;adjust&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;learnable&nbsp;weights&nbsp;in&nbsp;the&nbsp;network.&nbsp;&nbsp;Until&nbsp;not&nbsp;too&nbsp;long&nbsp;ago&nbsp;most&nbsp;people&nbsp;would&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;thought&nbsp;that&nbsp;this&nbsp;type&nbsp;of&nbsp;learning&nbsp;would&nbsp;be&nbsp;much&nbsp;too&nbsp;weak&nbsp;to&nbsp;be&nbsp;of&nbsp;any&nbsp;practical<br>
&nbsp;&nbsp;&nbsp;&nbsp;use.&nbsp;But,&nbsp;as&nbsp;in&nbsp;all&nbsp;engineering,&nbsp;you&nbsp;cannot&nbsp;argue&nbsp;with&nbsp;something&nbsp;that&nbsp;actually<br>
&nbsp;&nbsp;&nbsp;&nbsp;works.&nbsp;&nbsp;One&nbsp;great&nbsp;thing&nbsp;that&nbsp;has&nbsp;come&nbsp;out&nbsp;of&nbsp;AI&nbsp;research&nbsp;of&nbsp;the&nbsp;last&nbsp;two&nbsp;decades<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;that&nbsp;unsupervised&nbsp;learning&nbsp;not&nbsp;only&nbsp;works,&nbsp;it&nbsp;actually&nbsp;lends&nbsp;itself&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;designing&nbsp;powerful&nbsp;data&nbsp;driven&nbsp;frameworks&nbsp;without&nbsp;too&nbsp;much&nbsp;human&nbsp;intervention.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;TRANSFORMERS:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;unsupervised&nbsp;learning&nbsp;of&nbsp;the&nbsp;sort&nbsp;described&nbsp;above&nbsp;is&nbsp;best&nbsp;implemented&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformers.&nbsp;(See&nbsp;my&nbsp;material&nbsp;for&nbsp;the&nbsp;Week&nbsp;13&nbsp;lecture&nbsp;at&nbsp;Purdue's&nbsp;Deep&nbsp;Learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;for&nbsp;a&nbsp;detailed&nbsp;presentation&nbsp;on&nbsp;how&nbsp;can&nbsp;implement&nbsp;an&nbsp;English-to-Spanish<br>
&nbsp;&nbsp;&nbsp;&nbsp;translation&nbsp;framework&nbsp;using&nbsp;Transformers.)&nbsp;&nbsp;And&nbsp;central&nbsp;to&nbsp;the&nbsp;Transformer<br>
&nbsp;&nbsp;&nbsp;&nbsp;architecture&nbsp;element&nbsp;is&nbsp;the&nbsp;notion&nbsp;of&nbsp;Attention.&nbsp;&nbsp;Attention&nbsp;means&nbsp;how&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;element&nbsp;at&nbsp;the&nbsp;input&nbsp;to&nbsp;a&nbsp;neural&nbsp;network&nbsp;attends&nbsp;to&nbsp;every&nbsp;other&nbsp;element&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;same&nbsp;input.&nbsp;&nbsp;In&nbsp;a&nbsp;network&nbsp;for&nbsp;language&nbsp;translation,&nbsp;the&nbsp;network&nbsp;would&nbsp;also&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention&nbsp;to&nbsp;figure&nbsp;out&nbsp;the&nbsp;significance&nbsp;of&nbsp;each&nbsp;word&nbsp;in&nbsp;a&nbsp;source-language<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;to&nbsp;every&nbsp;other&nbsp;word.&nbsp;For&nbsp;example,&nbsp;if&nbsp;the&nbsp;car&nbsp;was&nbsp;one&nbsp;of&nbsp;the&nbsp;words&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;at&nbsp;the&nbsp;input&nbsp;and&nbsp;a&nbsp;clause&nbsp;in&nbsp;the&nbsp;sentence&nbsp;used&nbsp;the&nbsp;pronoun&nbsp;"it"&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;pointed&nbsp;to&nbsp;that&nbsp;car,&nbsp;the&nbsp;network&nbsp;would&nbsp;be&nbsp;able&nbsp;to&nbsp;figure&nbsp;out&nbsp;the&nbsp;connection<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;"it"&nbsp;and&nbsp;the&nbsp;"car"&nbsp;through&nbsp;attention.&nbsp;&nbsp;Along&nbsp;the&nbsp;same&nbsp;lines,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;would&nbsp;use&nbsp;Cross&nbsp;Attention&nbsp;to&nbsp;figure&nbsp;out&nbsp;the&nbsp;importance&nbsp;of&nbsp;each&nbsp;word&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;source&nbsp;language&nbsp;to&nbsp;the&nbsp;different&nbsp;words&nbsp;in&nbsp;the&nbsp;target&nbsp;language.&nbsp;&nbsp;As&nbsp;you&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;imagine,&nbsp;understanding&nbsp;such&nbsp;connections&nbsp;between&nbsp;the&nbsp;words&nbsp;would&nbsp;be&nbsp;critical&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;any&nbsp;network&nbsp;that&nbsp;is&nbsp;learning&nbsp;how&nbsp;to&nbsp;translate&nbsp;a&nbsp;source&nbsp;language&nbsp;sentence&nbsp;into&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;target&nbsp;language&nbsp;sentence.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">THE&nbsp;MAJOR&nbsp;COMPONENTS&nbsp;of&nbsp;babyGPT:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;module&nbsp;contains&nbsp;the&nbsp;following&nbsp;Python&nbsp;classes:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;ArticleGatherer&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;ArticleDataset&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[supplies&nbsp;the&nbsp;data&nbsp;downloader&nbsp;for&nbsp;training]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)&nbsp;TrainTokenizer&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)&nbsp;TransformerFG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[borrowed&nbsp;from&nbsp;Transformers&nbsp;in&nbsp;DLStudio]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;MasterDecoderWithMasking;&nbsp;&nbsp;&nbsp;[borrowed&nbsp;from&nbsp;Transformers&nbsp;in&nbsp;DLStudio]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;PromptResponder<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;what&nbsp;follows,&nbsp;I'll&nbsp;introduce&nbsp;each&nbsp;of&nbsp;these&nbsp;components&nbsp;one&nbsp;by&nbsp;one.&nbsp;&nbsp;Each<br>
&nbsp;&nbsp;&nbsp;&nbsp;component&nbsp;is&nbsp;a&nbsp;separate&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;main&nbsp;module&nbsp;class&nbsp;babyGPT.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;ArticleGatherer:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;ArticleGatherer,&nbsp;you&nbsp;supply&nbsp;it&nbsp;with&nbsp;a&nbsp;list&nbsp;of&nbsp;URLs&nbsp;to&nbsp;media&nbsp;news&nbsp;sites.<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;then&nbsp;uses&nbsp;the&nbsp;Newspaper&nbsp;module&nbsp;(which&nbsp;understands&nbsp;the&nbsp;structure&nbsp;of&nbsp;a&nbsp;typical<br>
&nbsp;&nbsp;&nbsp;&nbsp;news&nbsp;HTML&nbsp;file)&nbsp;to&nbsp;download&nbsp;the&nbsp;articles&nbsp;from&nbsp;each&nbsp;of&nbsp;those&nbsp;URLs.&nbsp;&nbsp;It&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;important&nbsp;to&nbsp;keep&nbsp;in&nbsp;mind&nbsp;that&nbsp;ArticleGatherer&nbsp;skips&nbsp;over&nbsp;non-HTML&nbsp;article&nbsp;files<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;media&nbsp;websites.&nbsp;Unfortunately,&nbsp;many&nbsp;popular&nbsp;news&nbsp;websites&nbsp;now&nbsp;hide&nbsp;their<br>
&nbsp;&nbsp;&nbsp;&nbsp;content&nbsp;behind&nbsp;paywalls&nbsp;implemented&nbsp;with&nbsp;JavaScript.&nbsp;&nbsp;[Examples&nbsp;of&nbsp;such&nbsp;websites<br>
&nbsp;&nbsp;&nbsp;&nbsp;include&nbsp;www.nyt.com,&nbsp;www.wsj.com,&nbsp;www.bbc.com,&nbsp;etc.]&nbsp;For&nbsp;obvious&nbsp;reasons,&nbsp;if&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;list&nbsp;of&nbsp;the&nbsp;URLs&nbsp;you&nbsp;provide&nbsp;ArticleGatherer&nbsp;consists&nbsp;of&nbsp;mostly&nbsp;such&nbsp;websites,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus&nbsp;you&nbsp;create&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;babyGPT&nbsp;could&nbsp;be&nbsp;much&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;small&nbsp;to&nbsp;be&nbsp;any&nbsp;fun.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;ArticleDataset:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;After&nbsp;you&nbsp;have&nbsp;used&nbsp;ArticleGatherer&nbsp;to&nbsp;download&nbsp;the&nbsp;news&nbsp;articles&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;corpus,&nbsp;the&nbsp;next&nbsp;thing&nbsp;you&nbsp;are&nbsp;going&nbsp;to&nbsp;need&nbsp;is&nbsp;a&nbsp;dataloader.&nbsp;That's<br>
&nbsp;&nbsp;&nbsp;&nbsp;exactly&nbsp;what's&nbsp;provided&nbsp;by&nbsp;the&nbsp;ArticleDataset&nbsp;class.&nbsp;&nbsp;It&nbsp;randomly&nbsp;shuffles&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;articles&nbsp;gathered&nbsp;and&nbsp;creates&nbsp;a&nbsp;number&nbsp;of&nbsp;dataloading&nbsp;streams&nbsp;equal&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch-size&nbsp;that&nbsp;you&nbsp;are&nbsp;using&nbsp;for&nbsp;training&nbsp;babyGPT.&nbsp;The&nbsp;data&nbsp;input&nbsp;for&nbsp;the&nbsp;i^th<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch&nbsp;instance&nbsp;is&nbsp;provided&nbsp;by&nbsp;the&nbsp;i^th&nbsp;stream.&nbsp;Logically&nbsp;speaking,&nbsp;you&nbsp;can&nbsp;think<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;each&nbsp;stream&nbsp;as&nbsp;a&nbsp;concatenation&nbsp;of&nbsp;the&nbsp;news&nbsp;articles&nbsp;that&nbsp;were&nbsp;randomly&nbsp;chosen<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;that&nbsp;batch&nbsp;instance.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;TrainTokenizer:<br>
</font>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Tokenizers&nbsp;play&nbsp;a&nbsp;critical&nbsp;role&nbsp;in&nbsp;language&nbsp;modeling&nbsp;because&nbsp;they&nbsp;create&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;bounded&nbsp;vocabulary&nbsp;of&nbsp;the&nbsp;tokens&nbsp;that&nbsp;the&nbsp;language&nbsp;model&nbsp;must&nbsp;understand.&nbsp;This&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;done&nbsp;by&nbsp;using&nbsp;a&nbsp;split-and-merge&nbsp;approach&nbsp;in&nbsp;which&nbsp;you&nbsp;start&nbsp;by&nbsp;considering&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;different&nbsp;word&nbsp;in&nbsp;your&nbsp;corpus&nbsp;as&nbsp;a&nbsp;sequence&nbsp;of&nbsp;the&nbsp;most&nbsp;basic&nbsp;symbols,&nbsp;which&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;ASCII&nbsp;characters&nbsp;as&nbsp;in&nbsp;the&nbsp;WordPiece&nbsp;tokenizer&nbsp;or&nbsp;the&nbsp;individual&nbsp;bytes,&nbsp;as&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;BPE&nbsp;(Byte&nbsp;Pair&nbsp;Encoding)&nbsp;tokenizer.&nbsp;&nbsp;Subsequently,&nbsp;you&nbsp;form&nbsp;subwords&nbsp;by,<br>
&nbsp;&nbsp;&nbsp;&nbsp;first,&nbsp;merging&nbsp;the&nbsp;most&nbsp;basic&nbsp;constituents&nbsp;like&nbsp;the&nbsp;bytes&nbsp;and,&nbsp;then,&nbsp;merging<br>
&nbsp;&nbsp;&nbsp;&nbsp;smaller&nbsp;subwords&nbsp;into&nbsp;longer&nbsp;subwords,&nbsp;on&nbsp;the&nbsp;basis&nbsp;of&nbsp;the&nbsp;frequencies&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;merged&nbsp;subwords&nbsp;vis-a-vis&nbsp;the&nbsp;frequencies&nbsp;of&nbsp;the&nbsp;components&nbsp;that&nbsp;were&nbsp;merged.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;merging&nbsp;process&nbsp;continues&nbsp;until&nbsp;you&nbsp;have&nbsp;reached&nbsp;the&nbsp;specified&nbsp;vocabulary&nbsp;size.<br>
&nbsp;&nbsp;&nbsp;&nbsp;What&nbsp;this&nbsp;logic&nbsp;implies&nbsp;is&nbsp;that&nbsp;if&nbsp;a&nbsp;long&nbsp;word&nbsp;in&nbsp;the&nbsp;corpus&nbsp;occurs&nbsp;sufficiently<br>
&nbsp;&nbsp;&nbsp;&nbsp;frequently,&nbsp;it&nbsp;will&nbsp;be&nbsp;represented&nbsp;by&nbsp;a&nbsp;single&nbsp;token.&nbsp;&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;relatively&nbsp;short&nbsp;word&nbsp;that&nbsp;occurs&nbsp;rarely&nbsp;in&nbsp;the&nbsp;original&nbsp;corpus&nbsp;could&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;decomposed&nbsp;into&nbsp;shorter&nbsp;tokens.&nbsp;&nbsp;It&nbsp;is&nbsp;in&nbsp;this&nbsp;manner&nbsp;that,&nbsp;with&nbsp;the&nbsp;WordPiece<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer,&nbsp;the&nbsp;BERT&nbsp;LLM&nbsp;has&nbsp;a&nbsp;vocabulary&nbsp;of&nbsp;around&nbsp;30,000&nbsp;tokens&nbsp;and,&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;BPE&nbsp;tokenizer,&nbsp;the&nbsp;GPT-3&nbsp;has&nbsp;a&nbsp;vocabulary&nbsp;of&nbsp;50,000&nbsp;tokens.&nbsp;Without&nbsp;such<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenization,&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary&nbsp;could&nbsp;grow&nbsp;continuously&nbsp;with&nbsp;the&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus.&nbsp;&nbsp;As&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;if&nbsp;a&nbsp;language&nbsp;modeler&nbsp;is&nbsp;ingesting<br>
&nbsp;&nbsp;&nbsp;&nbsp;terabytes&nbsp;of&nbsp;text,&nbsp;the&nbsp;vocabulary&nbsp;of&nbsp;the&nbsp;words&nbsp;it&nbsp;sees&nbsp;could&nbsp;run&nbsp;into&nbsp;millions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;is&nbsp;not&nbsp;possible&nbsp;to&nbsp;devise&nbsp;the&nbsp;probability-based&nbsp;logic&nbsp;for&nbsp;next-word&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;your&nbsp;underlying&nbsp;vocabulary&nbsp;is&nbsp;unbounded.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;module&nbsp;comes&nbsp;with&nbsp;a&nbsp;pre-trained&nbsp;tokenizer&nbsp;with&nbsp;a&nbsp;vocab&nbsp;size&nbsp;of&nbsp;around<br>
&nbsp;&nbsp;&nbsp;&nbsp;50,000&nbsp;tokens.&nbsp;&nbsp;I&nbsp;trained&nbsp;this&nbsp;tokenizer&nbsp;using&nbsp;the&nbsp;babyGPT&nbsp;module&nbsp;on&nbsp;the&nbsp;athlete<br>
&nbsp;&nbsp;&nbsp;&nbsp;news&nbsp;dataset&nbsp;created&nbsp;by&nbsp;Adrien&nbsp;Dubois.&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;tokenizer&nbsp;JSON&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Examples&nbsp;directory&nbsp;is:&nbsp;104_babygpt_tokenizer_49270.json&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;TransformerFG&nbsp;component&nbsp;of&nbsp;babyGPT,&nbsp;as&nbsp;mentioned&nbsp;already,&nbsp;language<br>
&nbsp;&nbsp;&nbsp;&nbsp;modeling&nbsp;is&nbsp;best&nbsp;carried&nbsp;out&nbsp;with&nbsp;Transformer&nbsp;based&nbsp;implementations.&nbsp;To&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;end,&nbsp;I&nbsp;borrowed&nbsp;TransformerFG&nbsp;from&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;concept&nbsp;of&nbsp;the&nbsp;Transformer&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;proposed&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.&nbsp;&nbsp;in&nbsp;their&nbsp;seminal&nbsp;paper&nbsp;"Attention&nbsp;is&nbsp;All&nbsp;You<br>
&nbsp;&nbsp;&nbsp;&nbsp;Need."&nbsp;&nbsp;The&nbsp;suffix&nbsp;"FG"&nbsp;stands&nbsp;for&nbsp;"First&nbsp;Generation."<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;MasterDecoderWithMasking:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;MasterDecoderWithMasking&nbsp;part&nbsp;of&nbsp;babyGPT&nbsp;has&nbsp;also&nbsp;been&nbsp;borrowed&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;To&nbsp;see&nbsp;the&nbsp;need&nbsp;for&nbsp;this&nbsp;component,&nbsp;note&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;unsupervised&nbsp;learning&nbsp;that&nbsp;is&nbsp;needed&nbsp;for&nbsp;autoregressive&nbsp;language&nbsp;modeling&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;uses&nbsp;the&nbsp;Decoder&nbsp;side&nbsp;of&nbsp;the&nbsp;Encode-Decoder&nbsp;paper&nbsp;that&nbsp;would&nbsp;otherwise&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;needed&nbsp;for&nbsp;a&nbsp;Transformer-based&nbsp;framework&nbsp;for&nbsp;translating&nbsp;one&nbsp;language&nbsp;into<br>
&nbsp;&nbsp;&nbsp;&nbsp;another.&nbsp;An&nbsp;example&nbsp;of&nbsp;such&nbsp;a&nbsp;framework&nbsp;is&nbsp;presented&nbsp;in&nbsp;the&nbsp;notes&nbsp;for&nbsp;my&nbsp;Week<br>
&nbsp;&nbsp;&nbsp;&nbsp;14&nbsp;lecture&nbsp;at&nbsp;Purdue's&nbsp;Deep&nbsp;Learning&nbsp;class.&nbsp;That&nbsp;framework&nbsp;has&nbsp;two&nbsp;decoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementations:&nbsp;MasterDecoder&nbsp;and&nbsp;MasterDecoderWithMasking.&nbsp;&nbsp;If&nbsp;you&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;engaged&nbsp;in&nbsp;autoregressive&nbsp;modeling,&nbsp;you&nbsp;have&nbsp;no&nbsp;choice&nbsp;but&nbsp;to&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"WithMasking"&nbsp;version&nbsp;of&nbsp;the&nbsp;decoder.&nbsp;&nbsp;As&nbsp;to&nbsp;the&nbsp;reason&nbsp;for&nbsp;the&nbsp;"Master"<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;decoder,&nbsp;a&nbsp;language&nbsp;modeling&nbsp;code&nbsp;typically&nbsp;requires<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;number&nbsp;of&nbsp;Transformer&nbsp;layers,&nbsp;with&nbsp;each&nbsp;layer&nbsp;using&nbsp;multiple&nbsp;Attention&nbsp;Heads<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;calculate&nbsp;what's&nbsp;known&nbsp;as&nbsp;Self&nbsp;Attention.&nbsp;In&nbsp;my&nbsp;DLStudio&nbsp;code,&nbsp;I&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;refers&nbsp;to&nbsp;this&nbsp;layered&nbsp;organization&nbsp;of&nbsp;the&nbsp;Transformers&nbsp;as&nbsp;MasterEncoder&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;MasterDecoder,&nbsp;and&nbsp;to&nbsp;each&nbsp;Transformer&nbsp;layer&nbsp;as&nbsp;the&nbsp;BasicEncoder&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;BasicDecoder.&nbsp;&nbsp;Note&nbsp;that&nbsp;there's&nbsp;an&nbsp;interesting&nbsp;difference&nbsp;between&nbsp;the&nbsp;decoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;logic&nbsp;as&nbsp;used&nbsp;in&nbsp;language&nbsp;translation&nbsp;and&nbsp;what&nbsp;you&nbsp;need&nbsp;for&nbsp;unsupervised<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning&nbsp;in&nbsp;a&nbsp;GPT:&nbsp;When&nbsp;used&nbsp;for&nbsp;language&nbsp;translation,&nbsp;the&nbsp;decoder&nbsp;would&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;calculate&nbsp;Cross&nbsp;Attention,&nbsp;which&nbsp;is&nbsp;the&nbsp;attention&nbsp;between&nbsp;each&nbsp;element&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;data&nbsp;coursing&nbsp;through&nbsp;the&nbsp;decoder&nbsp;and&nbsp;all&nbsp;the&nbsp;elements&nbsp;at&nbsp;the&nbsp;final&nbsp;output&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;encoder.&nbsp;&nbsp;The&nbsp;decoder&nbsp;as&nbsp;used&nbsp;for&nbsp;unsupervised&nbsp;learning&nbsp;in&nbsp;a&nbsp;GPT&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;needs&nbsp;to&nbsp;calculate&nbsp;Self&nbsp;Attention.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;PromptResponder:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;final&nbsp;component&nbsp;of&nbsp;babyGPT,&nbsp;PromptResponder,&nbsp;its&nbsp;purpose&nbsp;is&nbsp;to&nbsp;put&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;trained&nbsp;babyGPT&nbsp;model&nbsp;to&nbsp;use&nbsp;by&nbsp;having&nbsp;it&nbsp;respond&nbsp;appropriately&nbsp;to&nbsp;the&nbsp;prompts<br>
&nbsp;&nbsp;&nbsp;&nbsp;supplied&nbsp;by&nbsp;a&nbsp;user.&nbsp;&nbsp;Given&nbsp;a&nbsp;prompt&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;a&nbsp;sentence&nbsp;fragment,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;PromptResponder&nbsp;uses&nbsp;its&nbsp;next-token&nbsp;prediction&nbsp;ability&nbsp;to&nbsp;keep&nbsp;on&nbsp;generating&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokens&nbsp;until&nbsp;it&nbsp;reaches&nbsp;the&nbsp;end-of-sentence&nbsp;token&nbsp;or&nbsp;until&nbsp;it&nbsp;has&nbsp;generated&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;number&nbsp;of&nbsp;sentences&nbsp;through&nbsp;this&nbsp;process.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">DEALING&nbsp;WITH&nbsp;THE&nbsp;PROBLEM&nbsp;OF&nbsp;CONTEXT&nbsp;DISRUPTION&nbsp;CAUSED&nbsp;BY&nbsp;THE&nbsp;"&lt;SOS&gt;"&nbsp;TOKEN:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;What&nbsp;comes&nbsp;in&nbsp;the&nbsp;way&nbsp;of&nbsp;training&nbsp;babyGPT&nbsp;are&nbsp;the&nbsp;textual&nbsp;discontinuities&nbsp;created<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;how&nbsp;a&nbsp;batch&nbsp;is&nbsp;constructed&nbsp;for&nbsp;each&nbsp;new&nbsp;iteration&nbsp;of&nbsp;training.&nbsp;&nbsp;As&nbsp;explained<br>
&nbsp;&nbsp;&nbsp;&nbsp;elsewhere&nbsp;in&nbsp;this&nbsp;doc&nbsp;page,&nbsp;the&nbsp;list&nbsp;of&nbsp;all&nbsp;the&nbsp;documents&nbsp;in&nbsp;the&nbsp;training&nbsp;corpus<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;first&nbsp;randomized&nbsp;and&nbsp;then&nbsp;divided&nbsp;into&nbsp;a&nbsp;number&nbsp;of&nbsp;token&nbsp;streams,&nbsp;with&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;stream&nbsp;for&nbsp;each&nbsp;batch&nbsp;instance.&nbsp;(This&nbsp;randomization&nbsp;of&nbsp;the&nbsp;files&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;division&nbsp;into&nbsp;token&nbsp;streams&nbsp;is&nbsp;carried&nbsp;out&nbsp;afresh&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;epoch.)&nbsp;&nbsp;Subsequently,&nbsp;when&nbsp;a&nbsp;fresh&nbsp;batch&nbsp;is&nbsp;needed,&nbsp;for&nbsp;each&nbsp;batch&nbsp;instance&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;"draw"&nbsp;from&nbsp;its&nbsp;corresponding&nbsp;stream&nbsp;a&nbsp;max_seq_length&nbsp;number&nbsp;of&nbsp;tokens.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;special&nbsp;&lt;SOS&gt;&nbsp;token&nbsp;is&nbsp;placed&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;each&nbsp;such&nbsp;token&nbsp;stream&nbsp;segment<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;another&nbsp;special&nbsp;token&nbsp;&lt;EOS&gt;&nbsp;at&nbsp;the&nbsp;end.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;insertion&nbsp;of&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;and&nbsp;&lt;EOS&gt;&nbsp;tokens&nbsp;disrupts&nbsp;the&nbsp;continuity&nbsp;of&nbsp;the&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;streams&nbsp;as&nbsp;you&nbsp;imagine&nbsp;---&nbsp;which&nbsp;runs&nbsp;contrary&nbsp;to&nbsp;the&nbsp;main&nbsp;point&nbsp;of&nbsp;the&nbsp;exercise<br>
&nbsp;&nbsp;&nbsp;&nbsp;which&nbsp;is&nbsp;to&nbsp;learn&nbsp;the&nbsp;continuity&nbsp;properties.&nbsp;Since&nbsp;the&nbsp;narrative&nbsp;continuity<br>
&nbsp;&nbsp;&nbsp;&nbsp;properties&nbsp;are&nbsp;context&nbsp;dependent,&nbsp;it&nbsp;would&nbsp;be&nbsp;fair&nbsp;to&nbsp;say&nbsp;that&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;causes&nbsp;a&nbsp;context&nbsp;disruption&nbsp;for&nbsp;the&nbsp;token&nbsp;that&nbsp;comes&nbsp;after&nbsp;&lt;SOS&gt;&nbsp;at&nbsp;the&nbsp;beginning<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;each&nbsp;batch&nbsp;instance.&nbsp;&nbsp;Over&nbsp;the&nbsp;years,&nbsp;various&nbsp;strategies&nbsp;have&nbsp;been&nbsp;proposed&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;circumvent&nbsp;this&nbsp;problem,&nbsp;one&nbsp;of&nbsp;the&nbsp;most&nbsp;recent&nbsp;being&nbsp;the&nbsp;"sliding-window&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention"&nbsp;as&nbsp;presented&nbsp;by&nbsp;Beltagy,&nbsp;Peters,&nbsp;and&nbsp;Cohan&nbsp;in&nbsp;their&nbsp;2023&nbsp;paper<br>
&nbsp;&nbsp;&nbsp;&nbsp;"Longformer:&nbsp;The&nbsp;Long-Document&nbsp;Transformer".&nbsp;&nbsp;In&nbsp;this&nbsp;approach,&nbsp;a&nbsp;fixed-sized<br>
&nbsp;&nbsp;&nbsp;&nbsp;window&nbsp;is&nbsp;used&nbsp;to&nbsp;calculate&nbsp;the&nbsp;attention&nbsp;at&nbsp;the&nbsp;token&nbsp;that&nbsp;is&nbsp;at&nbsp;the&nbsp;center&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;window.&nbsp;&nbsp;In&nbsp;this&nbsp;manner,&nbsp;what&nbsp;is&nbsp;calculated&nbsp;for&nbsp;Self&nbsp;Attention&nbsp;is&nbsp;the&nbsp;extent<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;which&nbsp;each&nbsp;token&nbsp;attends&nbsp;to&nbsp;the&nbsp;W/2&nbsp;tokens&nbsp;on&nbsp;each&nbsp;side&nbsp;of&nbsp;the&nbsp;token&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;center.&nbsp;&nbsp;As&nbsp;the&nbsp;authors&nbsp;say:&nbsp;"Using&nbsp;multiple&nbsp;stacked&nbsp;layers&nbsp;of&nbsp;such&nbsp;windowed<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;results&nbsp;in&nbsp;a&nbsp;large&nbsp;receptive&nbsp;field,&nbsp;where&nbsp;top&nbsp;layers&nbsp;have&nbsp;access&nbsp;to&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;locations&nbsp;and&nbsp;have&nbsp;the&nbsp;capacity&nbsp;to&nbsp;build&nbsp;representations&nbsp;that&nbsp;incorporate<br>
&nbsp;&nbsp;&nbsp;&nbsp;information&nbsp;across&nbsp;the&nbsp;entire&nbsp;input."<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;keeping&nbsp;with&nbsp;the&nbsp;spirit&nbsp;of&nbsp;babyGPT,&nbsp;I&nbsp;have&nbsp;used&nbsp;a&nbsp;much&nbsp;simpler&nbsp;approach&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;deal&nbsp;with&nbsp;the&nbsp;context-disruption&nbsp;problem&nbsp;created&nbsp;by&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;token.&nbsp;&nbsp;My<br>
&nbsp;&nbsp;&nbsp;&nbsp;solution&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;idea&nbsp;I&nbsp;call&nbsp;"Context&nbsp;Buffer".&nbsp;&nbsp;In&nbsp;the&nbsp;token&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;stream&nbsp;that&nbsp;corresponds&nbsp;to&nbsp;each&nbsp;batch&nbsp;instance,&nbsp;a&nbsp;context&nbsp;buffer&nbsp;is&nbsp;the&nbsp;last&nbsp;n<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokens&nbsp;that&nbsp;are&nbsp;meant&nbsp;to&nbsp;serve&nbsp;as&nbsp;the&nbsp;context&nbsp;for&nbsp;the&nbsp;first&nbsp;real&nbsp;token&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;same&nbsp;instance&nbsp;in&nbsp;the&nbsp;next&nbsp;batch.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;elaborate,&nbsp;let's&nbsp;assume&nbsp;that&nbsp;N&nbsp;is&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;Context&nbsp;Window&nbsp;for&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformer&nbsp;based&nbsp;processing&nbsp;of&nbsp;text.&nbsp;&nbsp;N&nbsp;is&nbsp;the&nbsp;maximum&nbsp;length&nbsp;of&nbsp;the&nbsp;input&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;for&nbsp;which&nbsp;you&nbsp;have&nbsp;designed&nbsp;your&nbsp;Transformer&nbsp;implementation.&nbsp;&nbsp;[This&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;means&nbsp;that&nbsp;your&nbsp;Attention&nbsp;Map&nbsp;will&nbsp;be&nbsp;an&nbsp;array&nbsp;of&nbsp;size&nbsp;NxN.]&nbsp;And&nbsp;let&nbsp;n&nbsp;be&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;smallest&nbsp;number&nbsp;of&nbsp;previous&nbsp;tokens&nbsp;that&nbsp;you&nbsp;think&nbsp;will&nbsp;provide&nbsp;a&nbsp;reasonable<br>
&nbsp;&nbsp;&nbsp;&nbsp;context&nbsp;for&nbsp;predicting&nbsp;the&nbsp;current&nbsp;token.&nbsp;&nbsp;So,&nbsp;during&nbsp;each&nbsp;training&nbsp;iteration,<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;each&nbsp;batch&nbsp;instance&nbsp;at&nbsp;the&nbsp;input,&nbsp;we&nbsp;want&nbsp;to&nbsp;save&nbsp;the&nbsp;last&nbsp;n&nbsp;tokens&nbsp;to&nbsp;serve<br>
&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;the&nbsp;context&nbsp;buffer&nbsp;for&nbsp;the&nbsp;new&nbsp;token&nbsp;sequence&nbsp;in&nbsp;the&nbsp;same&nbsp;batch&nbsp;instance.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Therefore,&nbsp;at&nbsp;the&nbsp;next&nbsp;iteration&nbsp;you&nbsp;will&nbsp;feed&nbsp;n+N&nbsp;tokens&nbsp;into&nbsp;the&nbsp;transformer,<br>
&nbsp;&nbsp;&nbsp;&nbsp;but,&nbsp;as&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;transformer,&nbsp;you&nbsp;would&nbsp;only&nbsp;retain<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;N&nbsp;tokens&nbsp;that&nbsp;come&nbsp;after&nbsp;the&nbsp;context-buffer&nbsp;n&nbsp;tokens.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;is&nbsp;this&nbsp;idea&nbsp;of&nbsp;context&nbsp;buffer&nbsp;that&nbsp;is&nbsp;invoked&nbsp;by&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;second<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;mentioned&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;this&nbsp;section.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;is&nbsp;interesting&nbsp;to&nbsp;note&nbsp;that&nbsp;the&nbsp;above&nbsp;mentioned&nbsp;problem&nbsp;with&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;disruption&nbsp;does&nbsp;NOT&nbsp;arise&nbsp;with&nbsp;sentence-based&nbsp;language&nbsp;modeling&nbsp;(as&nbsp;in&nbsp;BERT)<br>
&nbsp;&nbsp;&nbsp;&nbsp;since&nbsp;&lt;SOS&gt;&nbsp;is&nbsp;what&nbsp;you&nbsp;would&nbsp;want&nbsp;to&nbsp;use&nbsp;for&nbsp;designating&nbsp;the&nbsp;start&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence.&nbsp;&nbsp;(For&nbsp;such&nbsp;learning,&nbsp;you&nbsp;would&nbsp;also&nbsp;use&nbsp;another&nbsp;token,&nbsp;denoted&nbsp;&lt;EOS&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;"End&nbsp;of&nbsp;Sequence",&nbsp;to&nbsp;mark&nbsp;the&nbsp;end&nbsp;of&nbsp;a&nbsp;sentence.)<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">INSTALLATION:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;babyGPT&nbsp;class&nbsp;was&nbsp;packaged&nbsp;using&nbsp;setuptools.&nbsp;&nbsp;For&nbsp;installation,&nbsp;execute<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;the&nbsp;source&nbsp;directory&nbsp;(this&nbsp;is&nbsp;the&nbsp;directory&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;contains&nbsp;the&nbsp;setup.py&nbsp;file&nbsp;after&nbsp;you&nbsp;have&nbsp;downloaded&nbsp;and&nbsp;uncompressed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;gzipped&nbsp;tar&nbsp;archive&nbsp;for&nbsp;the&nbsp;module):<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sudo&nbsp;python3&nbsp;setup.py&nbsp;install<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;Linux&nbsp;distributions,&nbsp;this&nbsp;will&nbsp;install&nbsp;the&nbsp;module&nbsp;file&nbsp;at&nbsp;a&nbsp;location&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;looks&nbsp;like<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/usr/local/lib/python3.10/dist-packages/<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;do&nbsp;not&nbsp;have&nbsp;root&nbsp;access,&nbsp;you&nbsp;have&nbsp;the&nbsp;option&nbsp;of&nbsp;working&nbsp;directly&nbsp;off<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;directory&nbsp;in&nbsp;which&nbsp;you&nbsp;downloaded&nbsp;the&nbsp;software&nbsp;by&nbsp;simply&nbsp;placing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;statements&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;your&nbsp;scripts&nbsp;that&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;class:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;sys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sys.path.append(&nbsp;"pathname_to_babyGPT_directory"&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;uninstall&nbsp;the&nbsp;module,&nbsp;simply&nbsp;delete&nbsp;the&nbsp;source&nbsp;directory,&nbsp;locate&nbsp;where&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;module&nbsp;was&nbsp;installed&nbsp;with&nbsp;"locate<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT"&nbsp;and&nbsp;delete&nbsp;those&nbsp;files.&nbsp;&nbsp;As&nbsp;mentioned&nbsp;above,&nbsp;the&nbsp;full<br>
&nbsp;&nbsp;&nbsp;&nbsp;pathname&nbsp;to&nbsp;the&nbsp;installed&nbsp;version&nbsp;is&nbsp;likely&nbsp;to&nbsp;look&nbsp;like<br>
&nbsp;&nbsp;&nbsp;&nbsp;/usr/local/lib/python2.7/dist-packages/babyGPT*<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;carry&nbsp;out&nbsp;a&nbsp;non-standard&nbsp;install&nbsp;of&nbsp;the&nbsp;babyGPT<br>
&nbsp;&nbsp;&nbsp;&nbsp;module,&nbsp;look&nbsp;up&nbsp;the&nbsp;on-line&nbsp;information&nbsp;on&nbsp;Disutils&nbsp;by&nbsp;pointing&nbsp;your&nbsp;browser<br>
&nbsp;&nbsp;&nbsp;&nbsp;to<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://docs.python.org/dist/dist.html">http://docs.python.org/dist/dist.html</a><br>
&nbsp;<br>
<font size="+2" color="red">USAGE:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;babyGPT&nbsp;for&nbsp;unsupervised&nbsp;learning&nbsp;of&nbsp;a&nbsp;base&nbsp;model&nbsp;for&nbsp;a&nbsp;text<br>
&nbsp;&nbsp;&nbsp;&nbsp;corpus,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;construct&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;main&nbsp;babyGPT&nbsp;class&nbsp;and&nbsp;its<br>
&nbsp;&nbsp;&nbsp;&nbsp;supporting&nbsp;classes&nbsp;as&nbsp;follows:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;baby_gpt&nbsp;=&nbsp;babyGPT(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;=&nbsp;max_seq_length,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;batch_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size&nbsp;=&nbsp;embedding_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;=&nbsp;num_basic_decoders,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;=&nbsp;num_atten_heads,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params&nbsp;=&nbsp;optimizer_params,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps&nbsp;=&nbsp;num_warmup_steps,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;masking&nbsp;=&nbsp;masking,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verify_text_corpus&nbsp;=&nbsp;False,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path_saved_model&nbsp;=&nbsp;{"decoder"&nbsp;:&nbsp;"./saved_decoder",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"embedding_generator"&nbsp;:&nbsp;"./saved_embedding_generator",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;xformer&nbsp;=&nbsp;baby_gpt.TransformerFG(&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;=&nbsp;max_seq_length,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size&nbsp;=&nbsp;embedding_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;=&nbsp;tokenizer_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps&nbsp;=&nbsp;num_warmup_steps,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params&nbsp;=&nbsp;optimizer_params,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;master_decoder&nbsp;=&nbsp;baby_gpt.MasterDecoderWithMasking(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xformer,&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;=&nbsp;num_basic_decoders,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;=&nbsp;num_atten_heads,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;masking&nbsp;=&nbsp;masking<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataloader&nbsp;=&nbsp;baby_gpt.ArticleDatasetWithBufferedContext(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gpt&nbsp;=&nbsp;baby_gpt,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;=&nbsp;tokenizer_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size&nbsp;=&nbsp;context_window_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size&nbsp;=&nbsp;context_buffer_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;articles_dir&nbsp;=&nbsp;articles_dir,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">THE&nbsp;Examples&nbsp;DIRECTORY:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;directory&nbsp;contains&nbsp;the&nbsp;following&nbsp;four&nbsp;scripts&nbsp;for&nbsp;working&nbsp;with&nbsp;babyGPT:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;run_gatherer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;is&nbsp;for&nbsp;collecting&nbsp;a&nbsp;corpus&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;babyGPT.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;requires&nbsp;a&nbsp;list&nbsp;of&nbsp;URLs&nbsp;as&nbsp;article&nbsp;sources&nbsp;as&nbsp;illustrated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;the&nbsp;following&nbsp;example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://finance.yahoo.com','http://cnn.com">https://finance.yahoo.com','http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://sports.yahoo.com">https://sports.yahoo.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://purdueexponent.org','https://slate.com">https://purdueexponent.org','https://slate.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="http://cnn.com">http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://slate.com">https://slate.com</a>'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;train_tokenizer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;the&nbsp;text&nbsp;corpus&nbsp;you&nbsp;have&nbsp;collected&nbsp;is&nbsp;for&nbsp;a&nbsp;specialized&nbsp;domain&nbsp;(such<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;movies,&nbsp;sports,&nbsp;healthcare,&nbsp;etc.),&nbsp;you&nbsp;are&nbsp;likely&nbsp;to&nbsp;get&nbsp;better<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results&nbsp;from&nbsp;babyGPT&nbsp;if&nbsp;you&nbsp;first&nbsp;train&nbsp;a&nbsp;new&nbsp;tokenizer&nbsp;for&nbsp;that&nbsp;domain.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;train&nbsp;a&nbsp;new&nbsp;tokenizer&nbsp;merely&nbsp;by&nbsp;invoking&nbsp;this&nbsp;script&nbsp;after&nbsp;you&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;its&nbsp;variable&nbsp;"articles_dir"&nbsp;so&nbsp;that&nbsp;it&nbsp;points&nbsp;to&nbsp;the&nbsp;corpus&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;create_base_model_with_buffered_context.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;script&nbsp;to&nbsp;run&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;create&nbsp;a&nbsp;Base&nbsp;Model&nbsp;for&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;corpus.&nbsp;&nbsp;By&nbsp;Base&nbsp;Model&nbsp;I&nbsp;mean&nbsp;a&nbsp;language&nbsp;model&nbsp;acquired&nbsp;through<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsupervised&nbsp;learning&nbsp;from&nbsp;a&nbsp;training&nbsp;corpus.&nbsp;&nbsp;Since&nbsp;this&nbsp;script&nbsp;calls&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;core&nbsp;language&nbsp;modeling&nbsp;functionality&nbsp;of&nbsp;babyGPT,&nbsp;you&nbsp;have&nbsp;to&nbsp;set&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;relatively&nbsp;large&nbsp;number&nbsp;of&nbsp;parameters&nbsp;in&nbsp;the&nbsp;script.&nbsp;&nbsp;These&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;shown&nbsp;below:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;articles_dir<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;interact_with_prompts.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;script&nbsp;for&nbsp;interacting&nbsp;with&nbsp;a&nbsp;trained&nbsp;babyGPT&nbsp;model&nbsp;through<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompts.&nbsp;&nbsp;The&nbsp;idea&nbsp;is&nbsp;that&nbsp;you&nbsp;supply&nbsp;a&nbsp;small&nbsp;number&nbsp;of&nbsp;words&nbsp;(as,&nbsp;say,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;beginning&nbsp;of&nbsp;a&nbsp;new&nbsp;thought)&nbsp;as&nbsp;a&nbsp;prompt&nbsp;and&nbsp;the&nbsp;model&nbsp;supplies&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rest&nbsp;of&nbsp;the&nbsp;words&nbsp;to&nbsp;complete&nbsp;the&nbsp;thought.&nbsp;&nbsp;At&nbsp;this&nbsp;time,&nbsp;the&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;extends&nbsp;your&nbsp;prompt&nbsp;until&nbsp;it&nbsp;reaches&nbsp;a&nbsp;period&nbsp;(or&nbsp;the&nbsp;end&nbsp;dictated&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size&nbsp;of&nbsp;the&nbsp;"max_seq_length"&nbsp;parameter.<br>
&nbsp;<br>
<font size="+2" color="red">BUGS:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;notify&nbsp;the&nbsp;author&nbsp;if&nbsp;you&nbsp;encounter&nbsp;any&nbsp;bugs.&nbsp;&nbsp;When&nbsp;sending&nbsp;email,<br>
&nbsp;&nbsp;&nbsp;&nbsp;please&nbsp;place&nbsp;the&nbsp;string&nbsp;'babyGPT'&nbsp;in&nbsp;the&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;author's&nbsp;spam&nbsp;filter.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">ABOUT&nbsp;THE&nbsp;AUTHOR:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;author,&nbsp;Avinash&nbsp;Kak,&nbsp;is&nbsp;a&nbsp;professor&nbsp;of&nbsp;Electrical&nbsp;and&nbsp;Computer&nbsp;Engineering<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;Purdue&nbsp;University.&nbsp;&nbsp;For&nbsp;all&nbsp;issues&nbsp;related&nbsp;to&nbsp;this&nbsp;module,&nbsp;contact&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;author&nbsp;at&nbsp;kak@purdue.edu&nbsp;If&nbsp;you&nbsp;send&nbsp;email,&nbsp;please&nbsp;place&nbsp;the&nbsp;string<br>
&nbsp;&nbsp;&nbsp;&nbsp;"babyGPT"&nbsp;in&nbsp;your&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;the&nbsp;author's&nbsp;spam<br>
&nbsp;&nbsp;&nbsp;&nbsp;filter.<br>
&nbsp;<br>
<font size="+2" color="red">COPYRIGHT:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Python&nbsp;Software&nbsp;Foundation&nbsp;License<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Copyright&nbsp;2025&nbsp;Avinash&nbsp;Kak<br>
&nbsp;<br>
@endofdocs</tt></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Imported Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="matplotlib.animation.html">matplotlib.animation</a><br>
<a href="blingfire.html">blingfire</a><br>
<a href="glob.html">glob</a><br>
<a href="itertools.html">itertools</a><br>
<a href="json.html">json</a><br>
<a href="logging.html">logging</a><br>
</td><td width="25%" valign=top><a href="math.html">math</a><br>
<a href="newspaper.html">newspaper</a><br>
<a href="torch.nn.html">torch.nn</a><br>
<a href="numpy.html">numpy</a><br>
<a href="torch.optim.html">torch.optim</a><br>
<a href="os.html">os</a><br>
</td><td width="25%" valign=top><a href="matplotlib.pyplot.html">matplotlib.pyplot</a><br>
<a href="random.html">random</a><br>
<a href="re.html">re</a><br>
<a href="signal.html">signal</a><br>
<a href="string.html">string</a><br>
<a href="sys.html">sys</a><br>
</td><td width="25%" valign=top><a href="time.html">time</a><br>
<a href="torch.html">torch</a><br>
<a href="torchvision.html">torchvision</a><br>
<a href="torchvision.transforms.html">torchvision.transforms</a><br>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="builtins.html#object">builtins.object</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="babyGPT.html#babyGPT">babyGPT</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="babyGPT">class <strong>babyGPT</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>babyGPT(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="babyGPT-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="babyGPT-run_code_with_buffered_context_for_training_TransformerFG"><strong>run_code_with_buffered_context_for_training_TransformerFG</strong></a>(self, xformer, master_decoder, dataloader, checkpoint_frequency=1000, display_train_loss=False)</dt><dd><tt>Drawn&nbsp;from&nbsp;the&nbsp;training&nbsp;routines&nbsp;in&nbsp;the&nbsp;Transformer&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dd></dl>

<dl><dt><a name="babyGPT-save_checkpoint_decoder"><strong>save_checkpoint_decoder</strong></a>(self, decoder, dir_name, iter_index)</dt><dd><tt>Save&nbsp;the&nbsp;decoder&nbsp;checkpoint</tt></dd></dl>

<dl><dt><a name="babyGPT-save_checkpoint_embedding_generator"><strong>save_checkpoint_embedding_generator</strong></a>(self, embedding_generator, dir_name, iter_index)</dt><dd><tt>save&nbsp;checkpoint&nbsp;for&nbsp;the&nbsp;embedding_generator</tt></dd></dl>

<dl><dt><a name="babyGPT-save_decoder"><strong>save_decoder</strong></a>(self, decoder)</dt><dd><tt>Save&nbsp;the&nbsp;trained&nbsp;decoder&nbsp;to&nbsp;a&nbsp;disk&nbsp;file</tt></dd></dl>

<dl><dt><a name="babyGPT-save_embedding_generator"><strong>save_embedding_generator</strong></a>(self, embedding_generator)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>ArticleDatasetWithBufferedContext</strong> = &lt;class 'babyGPT.babyGPT.ArticleDatasetWithBufferedContext'&gt;<dd><tt>The&nbsp;parameter&nbsp;'context_window_size'&nbsp;is&nbsp;related&nbsp;to&nbsp;how&nbsp;many&nbsp;tokens&nbsp;you&nbsp;can&nbsp;feed&nbsp;into&nbsp;the<br>
transformer&nbsp;at&nbsp;one&nbsp;iteration&nbsp;as&nbsp;the&nbsp;training&nbsp;corpus&nbsp;is&nbsp;being&nbsp;scanned.&nbsp;&nbsp;In&nbsp;my&nbsp;Week&nbsp;14&nbsp;lecture<br>
on&nbsp;Transformers,&nbsp;I&nbsp;used&nbsp;the&nbsp;notation&nbsp;'max_seq_len'&nbsp;for&nbsp;this&nbsp;parameter.</tt></dl>

<dl><dt><strong>ArticleGatherer</strong> = &lt;class 'babyGPT.babyGPT.ArticleGatherer'&gt;<dd><tt>This&nbsp;script&nbsp;is&nbsp;for&nbsp;collecting&nbsp;data&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;Transformer&nbsp;based<br>
unsupervised&nbsp;learning&nbsp;code&nbsp;in&nbsp;baby_gpt.py.&nbsp;&nbsp;<br>
&nbsp;<br>
The&nbsp;articles&nbsp;are&nbsp;downloaded&nbsp;from&nbsp;the&nbsp;URLs&nbsp;that&nbsp;are&nbsp;specified&nbsp;by&nbsp;the&nbsp;argument&nbsp;'urls'&nbsp;in&nbsp;the<br>
constructor&nbsp;shown&nbsp;below.&nbsp;&nbsp;See&nbsp;the&nbsp;script&nbsp;"create_base_model.py"&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory<br>
for&nbsp;how&nbsp;to&nbsp;set&nbsp;the&nbsp;URL&nbsp;strings&nbsp;for&nbsp;this&nbsp;argument.&nbsp;&nbsp;Here&nbsp;are&nbsp;some&nbsp;examples:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://finance.yahoo.com','http://cnn.com">https://finance.yahoo.com','http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://purdueexponent.org','https://slate.com">https://purdueexponent.org','https://slate.com</a>',&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://sports.yahoo.com">https://sports.yahoo.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="http://cnn.com">http://cnn.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://slate.com">https://slate.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>']</tt></dl>

<dl><dt><strong>AttentionHead</strong> = &lt;class 'babyGPT.babyGPT.AttentionHead'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>BasicDecoderWithMasking</strong> = &lt;class 'babyGPT.babyGPT.BasicDecoderWithMasking'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>EmbeddingGenerator</strong> = &lt;class 'babyGPT.babyGPT.EmbeddingGenerator'&gt;</dl>

<dl><dt><strong>MasterDecoderWithMasking</strong> = &lt;class 'babyGPT.babyGPT.MasterDecoderWithMasking'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>PromptResponder</strong> = &lt;class 'babyGPT.babyGPT.PromptResponder'&gt;<dd><tt>Prompting&nbsp;a&nbsp;trained&nbsp;babyGPT&nbsp;models&nbsp;means&nbsp;that&nbsp;you&nbsp;supply&nbsp;a&nbsp;small&nbsp;number&nbsp;of&nbsp;words&nbsp;(as,&nbsp;say,&nbsp;the&nbsp;<br>
beginning&nbsp;of&nbsp;a&nbsp;new&nbsp;thought)&nbsp;as&nbsp;a&nbsp;prompt&nbsp;and&nbsp;the&nbsp;model&nbsp;supplies&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;words&nbsp;to&nbsp;complete&nbsp;<br>
the&nbsp;thought.&nbsp;&nbsp;The&nbsp;class&nbsp;comes&nbsp;with&nbsp;two&nbsp;methods,&nbsp;the&nbsp;first&nbsp;for&nbsp;extending&nbsp;your&nbsp;prompt&nbsp;until&nbsp;it&nbsp;<br>
reaches&nbsp;a&nbsp;period,&nbsp;and&nbsp;the&nbsp;second&nbsp;for&nbsp;going&nbsp;beyond&nbsp;the&nbsp;first&nbsp;period&nbsp;encountered.<br>
&nbsp;<br>
Any&nbsp;interaction&nbsp;with&nbsp;a&nbsp;trained&nbsp;GPT&nbsp;model&nbsp;has&nbsp;to&nbsp;deal&nbsp;with&nbsp;the&nbsp;following&nbsp;issue:&nbsp;&nbsp;What&nbsp;to&nbsp;do&nbsp;with<br>
the&nbsp;context&nbsp;buffer&nbsp;that&nbsp;is&nbsp;meant&nbsp;to&nbsp;be&nbsp;a&nbsp;continuation&nbsp;of&nbsp;the&nbsp;last&nbsp;part&nbsp;of&nbsp;the&nbsp;previous&nbsp;"sentence"<br>
fed&nbsp;into&nbsp;the&nbsp;transformer.&nbsp;<br>
&nbsp;<br>
Ideally,&nbsp;we&nbsp;should&nbsp;be&nbsp;placing&nbsp;in&nbsp;the&nbsp;context&nbsp;buffer&nbsp;words&nbsp;that&nbsp;create&nbsp;a&nbsp;context&nbsp;for&nbsp;the&nbsp;prompt.<br>
But&nbsp;there&nbsp;is&nbsp;no&nbsp;easy&nbsp;way&nbsp;to&nbsp;that&nbsp;without&nbsp;a&nbsp;more&nbsp;elaborate&nbsp;model.&nbsp;An&nbsp;example&nbsp;of&nbsp;more&nbsp;elaborate<br>
modeling&nbsp;would&nbsp;be&nbsp;to&nbsp;have&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;transformer&nbsp;consist&nbsp;of,&nbsp;say,&nbsp;an&nbsp;SOS&nbsp;token,&nbsp;a&nbsp;special<br>
context&nbsp;token&nbsp;consisting&nbsp;possibly&nbsp;of&nbsp;integer&nbsp;index&nbsp;values&nbsp;beyond&nbsp;the&nbsp;tokenizer&nbsp;vocab,&nbsp;followed<br>
by&nbsp;a&nbsp;context&nbsp;buffer&nbsp;that&nbsp;would&nbsp;be&nbsp;the&nbsp;last&nbsp;part&nbsp;of&nbsp;the&nbsp;previous&nbsp;sentence,&nbsp;followed,&nbsp;finally,&nbsp;<br>
by&nbsp;the&nbsp;new&nbsp;input&nbsp;tokens.<br>
&nbsp;<br>
babyGPT&nbsp;gives&nbsp;you&nbsp;two&nbsp;options&nbsp;regarding&nbsp;what&nbsp;to&nbsp;do&nbsp;with&nbsp;the&nbsp;context&nbsp;buffer&nbsp;for&nbsp;your&nbsp;prompt:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;all_zeros<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;get_from_prompt<br>
&nbsp;<br>
With&nbsp;the&nbsp;first&nbsp;option,&nbsp;all&nbsp;of&nbsp;the&nbsp;integer&nbsp;encoding&nbsp;values&nbsp;in&nbsp;the&nbsp;context&nbsp;buffer&nbsp;are&nbsp;set&nbsp;to<br>
the&nbsp;integer&nbsp;zero.&nbsp;&nbsp;And,&nbsp;with&nbsp;the&nbsp;second&nbsp;option,&nbsp;at&nbsp;this&nbsp;time,&nbsp;the&nbsp;context&nbsp;buffer&nbsp;contains<br>
a&nbsp;portion&nbsp;or&nbsp;all&nbsp;of&nbsp;the&nbsp;prompt&nbsp;itself.&nbsp;&nbsp;If&nbsp;the&nbsp;tokenized&nbsp;version&nbsp;of&nbsp;the&nbsp;prompt&nbsp;is&nbsp;shorter<br>
than&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;context&nbsp;buffer,&nbsp;only&nbsp;the&nbsp;context_buffer_size&nbsp;number&nbsp;of&nbsp;elements&nbsp;of&nbsp;the<br>
prompt&nbsp;are&nbsp;retained&nbsp;for&nbsp;the&nbsp;context&nbsp;buffer.&nbsp;&nbsp;In&nbsp;the&nbsp;opposite&nbsp;case,&nbsp;just&nbsp;the&nbsp;initial&nbsp;<br>
context_buffer_size&nbsp;number&nbsp;of&nbsp;elements&nbsp;of&nbsp;the&nbsp;prompt&nbsp;are&nbsp;retained.</tt></dl>

<dl><dt><strong>ScheduledOptim</strong> = &lt;class 'babyGPT.babyGPT.ScheduledOptim'&gt;<dd><tt>As&nbsp;in&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio,&nbsp;for&nbsp;the&nbsp;scheduling&nbsp;of&nbsp;the&nbsp;learning&nbsp;rate<br>
during&nbsp;the&nbsp;warm-up&nbsp;phase&nbsp;of&nbsp;training&nbsp;TransformerFG,&nbsp;I&nbsp;have&nbsp;borrowed&nbsp;the&nbsp;class&nbsp;shown&nbsp;below<br>
from&nbsp;the&nbsp;GitHub&nbsp;code&nbsp;made&nbsp;available&nbsp;by&nbsp;Yu-Hsiang&nbsp;Huang&nbsp;at:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">https://github.com/jadore801120/attention-is-all-you-need-pytorch</a></tt></dl>

<dl><dt><strong>SelfAttention</strong> = &lt;class 'babyGPT.babyGPT.SelfAttention'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>TrainTokenizer</strong> = &lt;class 'babyGPT.babyGPT.TrainTokenizer'&gt;<dd><tt>Tokenizers&nbsp;play&nbsp;a&nbsp;critical&nbsp;role&nbsp;in&nbsp;language&nbsp;modeling&nbsp;because&nbsp;they&nbsp;create&nbsp;a<br>
fixed-sized&nbsp;vocabulary&nbsp;for&nbsp;the&nbsp;corpus&nbsp;you&nbsp;are&nbsp;working&nbsp;with&nbsp;---&nbsp;regardless&nbsp;of<br>
the&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus&nbsp;itself.&nbsp;&nbsp;Unless&nbsp;your&nbsp;text&nbsp;corpus&nbsp;is&nbsp;based&nbsp;on&nbsp;a&nbsp;set&nbsp;of<br>
documents&nbsp;frozen&nbsp;in&nbsp;time,&nbsp;ordinarily,&nbsp;as&nbsp;the&nbsp;size&nbsp;of&nbsp;a&nbsp;text&nbsp;corpus&nbsp;goes&nbsp;up,<br>
so&nbsp;does&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary&nbsp;---&nbsp;despite&nbsp;the&nbsp;illusion&nbsp;to&nbsp;the&nbsp;contrary<br>
created&nbsp;by&nbsp;the&nbsp;fixed&nbsp;sizes&nbsp;of&nbsp;the&nbsp;language&nbsp;dictionaries&nbsp;you&nbsp;have&nbsp;seen&nbsp;all<br>
your&nbsp;life.&nbsp;&nbsp;How&nbsp;we&nbsp;express&nbsp;ourselves&nbsp;is&nbsp;a&nbsp;living&nbsp;thing.&nbsp;&nbsp;We&nbsp;are&nbsp;constantly<br>
inventing&nbsp;new&nbsp;words&nbsp;and&nbsp;new&nbsp;expressions;&nbsp;these&nbsp;form&nbsp;important&nbsp;components&nbsp;of<br>
what's&nbsp;referred&nbsp;to&nbsp;as&nbsp;the&nbsp;zeitgeist.<br>
&nbsp;<br>
Having&nbsp;a&nbsp;fixed-sized&nbsp;vocab&nbsp;is&nbsp;important&nbsp;because&nbsp;the&nbsp;loss&nbsp;functions&nbsp;used&nbsp;in<br>
deep-learning&nbsp;network&nbsp;used&nbsp;for&nbsp;language&nbsp;processing&nbsp;are&nbsp;based&nbsp;on<br>
maximum-likelihood&nbsp;prediction&nbsp;of&nbsp;the&nbsp;next&nbsp;token&nbsp;given&nbsp;the&nbsp;tokens&nbsp;seen<br>
previously.&nbsp;&nbsp;That&nbsp;requires&nbsp;estimating&nbsp;the&nbsp;probabilities&nbsp;associated&nbsp;with&nbsp;all<br>
possible&nbsp;tokens&nbsp;at&nbsp;the&nbsp;next&nbsp;position.&nbsp;&nbsp;As&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;it&nbsp;would&nbsp;be<br>
impossible&nbsp;to&nbsp;engage&nbsp;in&nbsp;such&nbsp;probabilistic&nbsp;reasoning&nbsp;if&nbsp;you&nbsp;did&nbsp;not&nbsp;know&nbsp;in<br>
advance&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary.</tt></dl>

<dl><dt><strong>TransformerFG</strong> = &lt;class 'babyGPT.babyGPT.TransformerFG'&gt;<dd><tt>I&nbsp;have&nbsp;borrowed&nbsp;from&nbsp;the&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;"FG"&nbsp;stands&nbsp;for&nbsp;"First&nbsp;Generation"&nbsp;---&nbsp;which&nbsp;is&nbsp;the&nbsp;Transformer<br>
as&nbsp;originally&nbsp;proposed&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.</tt></dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-ctrl_c_handler"><strong>ctrl_c_handler</strong></a>(signum, frame)</dt></dl>
 <dl><dt><a name="-dev"><strong>dev</strong></a>()</dt></dl>
 <dl><dt><a name="-gen"><strong>gen</strong></a>(container)</dt></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>

p<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>__author__</strong> = 'Avinash Kak (kak@purdue.edu)'<br>
<strong>__copyright__</strong> = '(C) 2025 Avinash Kak. Python Software Foundation.'<br>
<strong>__date__</strong> = '2025-April-22'<br>
<strong>__url__</strong> = 'https://engineering.purdue.edu/kak/distBabyGPT/babyGPT-1.0.6.html'<br>
<strong>__version__</strong> = '1.0.6'</td></tr></table>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#7799ee">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Author</strong></big></font></td></tr>
<tr><td bgcolor="#7799ee"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%">Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)</td></tr></table>
</body></html>

