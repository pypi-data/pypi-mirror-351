{

	// This file was created by copying ds2databricks_dbsql_main.json

	// ******** Databricks output options *******
	"code_generation_language" : "SQL", // can be SQL or PYTHON
	"target_file_extension" : "py", // make sure this is in sync with code_generation_language setting.
	"use_notebook_md" : 1, //indicate if databricks notebook markdowns should be used on the output or if we are generating a plain SQL or PYTHON file
	"md_section_start" : "#", //start the markdowns with the pound

	"output_title" : 1,
	"title_template" : "Folder %FOLDERNAME%, Job %JOBNAME%",
	"output_description" : 1,
	"consolidate_nb_statements" : 1, // for ETL source, will group variables, pre/post sql, prerequisites (unconnected lookups)

	"default_schema" : "TEMP", // in case the attribute DEFAULT_SCHEMA is not populated by HANA reader
	"script_header" : "%sql\nUSE %DEFAULT_SCHEMA%;",
	"rank_template" : "RANK() OVER ( PARTITION BY %PARTITION_COLUMNS% ORDER BY %ORDER_BY% )",

	//"target_file_extension:" : "py",
	"generate_variable_declaration" : 1,
	// "variable_declaration_template" : "dbutils.widgets.text(name = '%VARNAME%', defaultValue = '%DEFAULT_VALUE%')",
	"variable_declaration_template" : "dbutils.widgets.text(name = '%VARNAME%', defaultValue = %DEFAULT_VALUE%)",
	"variable_declaration_comment" : "Variable declaration section",

	// ******** DBSQL Generation Options *******
	//"dataset_creation_method" : "CTE",
	"dataset_creation_method" : "TABLE",
	"table_creation_statement" : "%TABLE_NAME% = spark.sql(\"\"\"\n%INNER_SQL%\"\"\"%FORMAT_SPEC%)\n%TABLE_NAME%.createOrReplaceTempView(\"%TABLE_NAME%\")", //ensure there are %TABLE_NAME% and %INNER_SQL% tokens present in this spec
	"target_table_creation_statement" : "%TABLE_NAME% = spark.sql(\"\"\"\n%INNER_SQL%\"\"\"%FORMAT_SPEC%)\n%TABLE_NAME%.write.mode('overwrite').format(\"delta\").saveAsTable(\"%OBJECT_NAME%\")", //ensure there are %TABLE_NAME% and %INNER_SQL% tokens present in this spec
	"leaf_node_table_creation_statement" : "%TABLE_NAME% = spark.sql(\"\"\"\n%INNER_SQL%\"\"\"%FORMAT_SPEC%)\n%TABLE_NAME%.write.mode('overwrite').format(\"delta\").saveAsTable(\"%OBJECT_NAME%\")", //ensure there are %TABLE_NAME% and %INNER_SQL% tokens present in this spec

	"apply_format_instruction" : 1, // will replace %FORMAT% token in table_creation_statement.  Will look for query parameters using {\w+} pattern by default
	"format_spec" : "%PARAM_NAME%=dbutils.widgets.get('%PARAM_NAME%')",

	"sql_statement_separator" : "\n",
	"component_header_comment" : "# Component %COMPONENT%, Type %TYPE% %ADDITIONAL_COMMENT%",

	//"sql_converter_config_file" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/oracle2databricks.json",
	//"etl_converter_config_file" : "C:/Work/Projects/DWS/BBPerl/backend-perl-shared/Config/Converter/datastage2deltalake.json",
	
	"generate_statement_for_source_dataset" : 1, //datastage specific.

	"create_target_ddl" : "1",
	"create_source_ddl" : "1",
	"ddl_filename_template" : "DDL_%OBJECT_NAME%.sql",
	"ddl_eliminate_extensions" : ["txt","dat","ds"],
	"transformer_vars_use_cte_flag" : "1", //tells SQL writer to use CTEs when there are variables in tranformer components
	"concat_operator" : "||",


	//"skip_variable_token_prefixes" : "1",

	"KEEP_UNUSED_TARGET_FIELDS" : "1",

	"ignore_connection_profiles" : "1", //if ON, will blank out %CONNECTION_PROFILE%. prefixes coming from agnostic layer
	
	// System settings.

	"SYS_TYPE_CONF" : {"FLAT FILE" : "FLATFILE", "TERADATA" : "Teradata", "ORACLE" : "ORACLE","Oracle" : "ORACLE"}, // add more if needed

	"line_subst" : [
		// Convert "IN (" to "FUNCTION_IN", so that we can handle it in function_subst 
		{"from" : "\bIN\s*\(", "to" : "FUNCTION_IN ("},
		{"from" : "\- (\d)", "to" : "-$1"}, // space between a minus and a digit

		// Convert concat symbol "+" to "||"
		{"from": "'\s*\+\s*'", "to": "' || '"}
	],

	"block_subst": [
		// Change "TRIM (LEADING...FROM...)" to "TRIM_LEADING(...)" (same for "TRAILING" and "BOTH").
		// We will then handle "TRIM_LEADING(...) (and "TRAILING" and "BOTH") in function_subst
		{"from": "TRIM\s*(LEADING|TRAILING|BOTH)(\s+.*?)\s+FROM", "to": "TRIM_$1($2"}
	],
	"function_subst": [
		{"from": "leftstr", "output_template": "substring($1, 0, $2)"},
		{"from": "rightstr", "output_template": "substring($1, -1, $2)"},
		{"from": "IF", "to": "IFF"},

		// This is the conversion of the original "IN" function (we chanegd it to "FUNCTION_IN" in line_subst
		{"from" : "FUNCTION_IN", "output_template" : "$1 IN ( $ARGS_AFTER_1ST_ARG )"},

		{"from": "ADDDAYS",     "output_template": "DATEADD(DAY,$2,$1)"},
		{"from": "ADD_DAYS",    "output_template": "DATEADD(DAY,$2,$1)"},
		{"from": "ADDMONTHS",   "to":              "ADD_MONTHS"},
		{"from": "ADDSECONDS",  "output_template": "DATEADD(SECOND,$2,$1)"},
		{"from": "ADD_SECONDS", "output_template": "DATEADD(SECOND,$2,$1)"},

////////// To do:
		// {"from": "CASE",               "output_template": ""}, // ???

		// {"from": "COMPONENT",          "output_template": ""}, // Not documented
		// {"from": "DATE",               "output_template": ""}, // Not documented
		{"from": "DAYSBETWEEN",        "output_template": "DATEDIFF(DAY,$1,$2)"},
		{"from": "DAYS_BETWEEN",        "output_template": "DATEDIFF(DAY,$1,$2)"},
		// {"from": "DECFLOAT",           "output_template": ""}, // Not documented
		// {"from": "DOUBLE",             "output_template": ""}, // Not documented
		// {"from": "FORMAT",             "output_template": ""}, // Not documented

////////// To do:
		// {"from": "IF",                 "output_template": ""}, // ???
		// {"from": "IN",                 "output_template": ""}, // ???

		// {"from": "INSTR",              "output_template": ""}, // Not documented
		// {"from": "INT",                "output_template": ""}, // Not documented
		// {"from": "ISNULL",             "output_template": ""}, // Not documented
		{"from": "ISOWEEK",            "to": "WEEKISO"},
		// {"from": "LEFTSTR",            "output_template": ""}, // Not documented
		{"from": "LOCALTOUTC",         "output_template": "CONVERT_TIMEZONE('UTC', $1)"},
		// {"from": "LONGDATE",           "output_template": ""}, // Not documented
		// {"from": "MATCH",              "output_template": ""}, // Not documented
		// {"from": "MIDSTR",             "output_template": ""}, // Not documented
		// {"from": "MIDSTRU",            "output_template": ""}, // Not documented
		{"from": "NOW",                "to": "CURRENT_TIMESTAMP"},
		// {"from": "RAWTOHEX",           "output_template": ""}, // Not documented
		// {"from": "RIGHTSTR",           "output_template": ""}, // Not documented
		// {"from": "ROUNDDOWN",          "output_template": ""}, // Not documented
		// {"from": "STRING",             "output_template": ""}, // Not documented
		// {"from": "STRLEN",             "output_template": ""}, // Not documented
		{"from": "SUBSTR_BEFORE",      "output_template": "SPLIT_PART($1,$2,1)"},
		{"from": "SUBSTR_AFTER",       "output_template": "SPLIT_PART($1,$2,2)"},
		// {"from": "TIME",               "output_template": ""}, // Not documented
		{"from": "TO_INT",             "to": "TO_NUMBER"},
		{"from": "TO_INTEGER",         "to": "TO_NUMBER"},
		{"from": "TO_NVARCHAR",        "to": "TO_VARCHAR"},

		// TRIM_LEADING / TRIM_TRAILING / TRIM_BOTH are "fake" funcs that we created in block_subst
		{"from": "TRIM_LEADING",       "output_template": "LTRIM($2,$1)"},
		{"from": "TRIM_TRAILING",      "output_template": "RTRIM($2,$1)"},
		{"from": "TRIM_BOTH",          "output_template": "TRIM($2,$1)"},
		
////////// To do:
		{"from": "UTCTOLOCAL",         "output_template": ""},
		{"from": "WEEKDAY",            "output_template": ""}
	]
}
