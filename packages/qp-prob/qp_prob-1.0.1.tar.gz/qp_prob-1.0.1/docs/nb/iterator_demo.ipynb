{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33f6eae",
   "metadata": {},
   "source": [
    "# Iteration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cbd8b",
   "metadata": {},
   "source": [
    "`qp` has a built in method to create a generator object that can be used to iterate through a `qp` file. In this notebook we will test this out by reading and writing Ensembles from a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dcfc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb62175",
   "metadata": {},
   "source": [
    "## Reading Ensembles from file\n",
    "\n",
    "Let's read in our file and see what the Ensemble looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the file\n",
    "data_file = \"../assets/test.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08398f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble(the_class=mixmod,shape=(100, 3))\n"
     ]
    }
   ],
   "source": [
    "ens = qp.read(data_file)\n",
    "print(ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66408dad",
   "metadata": {},
   "source": [
    "We have an Ensemble of 100 Gaussian mixed model distributions, with 3 Gaussian components each. That's a lot to handle at once. However, instead of reading in the whole file at once we can use the `iterator` method to create a generator, which we can then use to iterate through a subset of Ensembles at a time. We would still want to know how many distributions are in the file, though, so we know what chunk size to pick. To do that we can use the `qp.data_length` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d493280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp.data_length(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5f1fe",
   "metadata": {},
   "source": [
    "Since we have 100 distributions, let's pick a chunk size of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2928f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itr = qp.iterator(data_file, chunk_size=10)\n",
    "type(itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6c3bb",
   "metadata": {},
   "source": [
    "Now that we have our generator, we can iterate through each set of 10 Ensembles and get whatever we need from them. Let's check that the PDFs of the chunks we get match the PDFs for the chunk we expect. We'll evaluate the PDF at `test_vals` for each of the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d908b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals = np.linspace(0., 1., 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d00c68fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk indices are: (0:10)\n",
      "The PDF values match\n",
      "Chunk indices are: (10:20)\n",
      "The PDF values match\n",
      "Chunk indices are: (20:30)\n",
      "The PDF values match\n",
      "Chunk indices are: (30:40)\n",
      "The PDF values match\n",
      "Chunk indices are: (40:50)\n",
      "The PDF values match\n",
      "Chunk indices are: (50:60)\n",
      "The PDF values match\n",
      "Chunk indices are: (60:70)\n",
      "The PDF values match\n",
      "Chunk indices are: (70:80)\n",
      "The PDF values match\n",
      "Chunk indices are: (80:90)\n",
      "The PDF values match\n",
      "Chunk indices are: (90:100)\n",
      "The PDF values match\n"
     ]
    }
   ],
   "source": [
    "for start, end, ens_i in itr:\n",
    "    print(f\"Chunk indices are: ({start}:{end})\")\n",
    "    if np.allclose(ens[start:end].pdf(test_vals), ens_i.pdf(test_vals)):\n",
    "        print(f\"The PDF values match\")\n",
    "    else:\n",
    "        print(f\"The PDF values for the iterated chunk do not match the values for the chunk from the whole Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c8a43",
   "metadata": {},
   "source": [
    "You can also do this all in one line, as shown below. This time we use a chunk size of 11 to demonstrate how the iteration behaves when the number of distributions is not evenly divided by the given chunk size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b2a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices are: (0, 11)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (11, 22)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (22, 33)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (33, 44)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (44, 55)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (55, 66)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (66, 77)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (77, 88)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (88, 99)\n",
      "Ensemble(the_class=mixmod,shape=(11, 3))\n",
      "Indices are: (99, 100)\n",
      "Ensemble(the_class=mixmod,shape=(1, 3))\n"
     ]
    }
   ],
   "source": [
    "for start, end, ens_chunk in qp.iterator(data_file, chunk_size=11):\n",
    "    print(f\"Indices are: ({start}, {end})\")\n",
    "    print(ens_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43276959",
   "metadata": {},
   "source": [
    "If the number of distributions is not easily divisible by the chunk size, then the last chunk will contain any remaining distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cde0f2",
   "metadata": {},
   "source": [
    "## Writing Ensembles to file\n",
    "\n",
    "Now that we know how to read in an Ensemble iteratively, let's take a look at how to write one out a chunk at a time to an HDF5 file. First, let's set up a file path to write our Ensemble to, and a `chunk_size`, or number of distributions to write at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0dd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "td = tempfile.TemporaryDirectory()\n",
    "\n",
    "new_file_path = os.path.join(td.name, \"test-write.hdf5\") # file to write to\n",
    "chunk_size = 5 # number of distributions to write at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f04977",
   "metadata": {},
   "source": [
    "Now we initialize our new HDF5 file. This creates an HDF5 file with the groups and datasets we need to store this Ensemble, but empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c4590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, fout = ens.initializeHdf5Write(new_file_path, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124dd38",
   "metadata": {},
   "source": [
    "Next, we can iterate through the distributions in our Ensemble, one chunk at a time, and write their data to the HDF5 file. Let's only write half of the distributions, so we'll set our iteration to go from 0 to 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd3bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 50, chunk_size):\n",
    "    ens[i:i+chunk_size].writeHdf5Chunk(groups, i, i+chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f13fe",
   "metadata": {},
   "source": [
    "Now that all of our distribution data is written, we can add in our metdata and close the file. This is done by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf39f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.finalizeHdf5Write(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571369f",
   "metadata": {},
   "source": [
    "We can check that this successfully wrote out 50 of our distributions by getting the number of distributions in our new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6907fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp.data_length(new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc205750",
   "metadata": {},
   "source": [
    "Great, we've successfully iterated through part of an Ensemble and written it to file in chunks. Let's delete the file, we don't need duplicates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bba2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf838fa",
   "metadata": {},
   "source": [
    "### Iteration in parallel\n",
    "\n",
    "This can also be done in parallel, by passing an MPI Communicator to the `comm` argument of `initializeHdf5Write()`. This sets up the HDF5 file for parallel writing, so each process will be able to write chunks of data to the file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
