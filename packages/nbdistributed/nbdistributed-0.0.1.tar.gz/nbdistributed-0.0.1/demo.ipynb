{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Jupyter Distributed Extension Demo\n",
                "\n",
                "This notebook demonstrates how to use the jupyter_distributed extension for interactive distributed PyTorch training."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load the Extension"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext jupyter_distributed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Distributed Debug Information ===\n",
                        "Process manager exists: False\n",
                        "Communication manager exists: False\n",
                        "Number of ranks: 0\n",
                        "=====================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_debug"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Distributed Workers\n",
                "\n",
                "Start 2 worker processes for distributed training:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distributed workers already running. Use %dist_shutdown to stop them first.\n"
                    ]
                }
            ],
            "source": [
                "%dist_init --num-ranks 2 --gpu-ids 1,2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Distributed Debug Information ===\n",
                        "Process manager exists: True\n",
                        "Communication manager exists: True\n",
                        "Number of ranks: 2\n",
                        "Process manager is_running(): True\n",
                        "Number of processes tracked: 2\n",
                        "  Process 0 (PID: 3650458): Running\n",
                        "  Process 1 (PID: 3650459): Running\n",
                        "=====================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_debug"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== DISTRIBUTED ENVIRONMENT RESET ===\n",
                        "Performing nuclear shutdown...\n",
                        "All state cleared\n",
                        "You can now run %dist_init to start fresh\n",
                        "=======================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_reset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Distributed Debug Information ===\n",
                        "Process manager exists: False\n",
                        "Communication manager exists: False\n",
                        "Number of ranks: 0\n",
                        "=====================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_debug"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== DISTRIBUTED ENVIRONMENT RESET ===\n",
                        "Performing nuclear reset...\n",
                        "All state cleared\n",
                        "You can now run %dist_init to start fresh\n",
                        "=======================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_reset --nuclear"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Distributed Debug Information ===\n",
                        "Process manager exists: False\n",
                        "Communication manager exists: False\n",
                        "Number of ranks: 0\n",
                        "=====================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_debug"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Check Worker Status"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distributed cluster status (2 ranks):\n",
                        "============================================================\n",
                        "Rank 0: ✓ PID 3648619\n",
                        "  ├─ GPU: 1 (NVIDIA GeForce RTX 4090)\n",
                        "  ├─ Memory: 0.0GB / 23.5GB (0.0% used)\n",
                        "  └─ Status: Running\n",
                        "\n",
                        "Rank 1: ✓ PID 3648620\n",
                        "  ├─ GPU: 2 (NVIDIA GeForce RTX 4090)\n",
                        "  ├─ Memory: 0.0GB / 23.5GB (0.0% used)\n",
                        "  └─ Status: Running\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%dist_status"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execute Code on All Ranks\n",
                "\n",
                "The `%%distributed` magic runs code on all worker processes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== All ranks ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "Hello from rank 0/1\n",
                        "PyTorch version: 2.4.0+cu121\n",
                        "CUDA available: True\n",
                        "\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "Hello from rank 1/1\n",
                        "PyTorch version: 2.4.0+cu121\n",
                        "CUDA available: True\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%distributed\n",
                "import torch\n",
                "import torch.distributed as dist\n",
                "\n",
                "print(f\"Hello from rank {rank}/{world_size-1}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create and Distribute Tensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== All ranks ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "Rank 0 tensor:\n",
                        "tensor([1, 2, 3], device='cuda:1')\n",
                        "\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "Rank 1 tensor:\n",
                        "tensor([2, 3, 4], device='cuda:2')\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%distributed\n",
                "# Create a tensor on each rank\n",
                "tensor = torch.tensor([0+device, 1+device, 2+device])\n",
                "tensor = tensor.to(\"cuda\")\n",
                "print(f\"Rank {rank} tensor:\\n{tensor}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Collective Operations\n",
                "\n",
                "Perform all-reduce to sum tensors across all ranks:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== All ranks ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "Rank 0 after all-reduce:\n",
                        "tensor([3, 5, 7], device='cuda:1')\n",
                        "\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "Rank 1 after all-reduce:\n",
                        "tensor([3, 5, 7], device='cuda:2')\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%distributed\n",
                "# All-reduce: sum tensors across all ranks\n",
                "dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
                "print(f\"Rank {rank} after all-reduce:\\n{tensor}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Execute Code on Specific Ranks\n",
                "\n",
                "Use `%%rank [...]` to run code only on specific ranks:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Ranks [0] ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "This only runs on rank 0 (master rank)\n",
                        "Master data: tensor([1, 2, 3, 4, 5])\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%rank [0]\n",
                "print(\"This only runs on rank 0 (master rank)\")\n",
                "master_data = torch.tensor([1, 2, 3, 4, 5])\n",
                "print(f\"Master data: {master_data}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Ranks [1] ===\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "This only runs on rank 1\n",
                        "Worker data: tensor([10, 20, 30, 40, 50])\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%rank [1]\n",
                "print(\"This only runs on rank 1\")\n",
                "worker_data = torch.tensor([10, 20, 30, 40, 50])\n",
                "print(f\"Worker data: {worker_data}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Synchronization\n",
                "\n",
                "Use `%sync` to synchronize all ranks before continuing:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Synchronized 2 ranks\n"
                    ]
                }
            ],
            "source": [
                "%sync"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Broadcast Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== All ranks ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "Rank 0 before broadcast: tensor([1., 2., 3.], device='cuda:1')\n",
                        "Rank 0 after broadcast: tensor([1., 2., 3.], device='cuda:1')\n",
                        "\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "Rank 1 before broadcast: tensor([0., 0., 0.], device='cuda:2')\n",
                        "Rank 1 after broadcast: tensor([1., 2., 3.], device='cuda:2')\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%distributed\n",
                "if rank == 0:\n",
                "    broadcast_tensor = torch.tensor([1,2,3], dtype=torch.float32).to(device)\n",
                "else:\n",
                "    broadcast_tensor = torch.zeros(3, dtype=torch.float32).to(device)\n",
                "\n",
                "print(f\"Rank {rank} before broadcast: {broadcast_tensor}\")\n",
                "\n",
                "dist.barrier()\n",
                "\n",
                "dist.broadcast(broadcast_tensor, src=0)\n",
                "\n",
                "dist.barrier()\n",
                "\n",
                "print(f\"Rank {rank} after broadcast: {broadcast_tensor}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Error Handling\n",
                "\n",
                "The extension handles errors gracefully and shows which rank failed:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== All ranks ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "Rank 0 executes successfully\n",
                        "\n",
                        "\n",
                        "--- Rank 1 ---\n",
                        "❌ Error: name 'undefined_variable' is not defined\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/zach/distributed-learning-zoo/jupyter_distributed/src/jupyter_distributed/worker.py\", line 145, in _execute_code\n",
                        "    exec(code, self.namespace)\n",
                        "  File \"<string>\", line 3, in <module>\n",
                        "NameError: name 'undefined_variable' is not defined\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%distributed\n",
                "if rank == 1:\n",
                "    # This will cause an error on rank 1\n",
                "    undefined_variable\n",
                "else:\n",
                "    print(f\"Rank {rank} executes successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Complex Rank Specifications\n",
                "\n",
                "You can specify complex rank patterns:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Force shutting down distributed workers...\n",
                        "Force shutdown completed\n",
                        "Distributed workers shutdown\n"
                    ]
                }
            ],
            "source": [
                "# Initialize with more ranks for this demo\n",
                "%dist_shutdown --force"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Distributed Debug Information ===\n",
                        "Process manager exists: True\n",
                        "Communication manager exists: True\n",
                        "Number of ranks: 2\n",
                        "Process manager is_running(): True\n",
                        "Number of processes tracked: 2\n",
                        "  Process 0 (PID: 3648619): Running\n",
                        "  Process 1 (PID: 3648620): Running\n",
                        "=====================================\n"
                    ]
                }
            ],
            "source": [
                "%dist_debug"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distributed workers already running. Use %dist_shutdown to stop them first.\n"
                    ]
                }
            ],
            "source": [
                "%dist_init --num-ranks 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distributed cluster status (2 ranks):\n",
                        "============================================================\n",
                        "Rank 0: ✓ PID 3646622\n",
                        "  ├─ GPU: 1 (NVIDIA GeForce RTX 4090)\n",
                        "  ├─ Memory: 0.0GB / 23.5GB (0.0% used)\n",
                        "  ├─ Reserved: 0.0GB\n",
                        "  └─ Status: Running\n",
                        "\n",
                        "Rank 1: ✓ PID 3646623\n",
                        "  ├─ GPU: 2 (NVIDIA GeForce RTX 4090)\n",
                        "  ├─ Memory: 0.0GB / 23.5GB (0.0% used)\n",
                        "  ├─ Reserved: 0.0GB\n",
                        "  └─ Status: Running\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%dist_status"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Ranks [0] ===\n",
                        "\n",
                        "--- Rank 0 ---\n",
                        "❌ Error: 'NoneType' object has no attribute 'magics_manager'\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/zach/distributed-learning-zoo/jupyter_distributed/src/jupyter_distributed/worker.py\", line 145, in _execute_code\n",
                        "    exec(code, self.namespace)\n",
                        "  File \"<string>\", line 4, in <module>\n",
                        "AttributeError: 'NoneType' object has no attribute 'magics_manager'\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "%%rank [0,2]\n",
                "\n",
                "from IPython import get_ipython\n",
                "ip = get_ipython()\n",
                "magic = ip.magics_manager.magics['line']['dist_status']\n",
                "magic(ip, '')\n",
                "\n",
                "print(f\"This runs on even ranks: {rank}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%rank[1-3]\n",
                "print(f\"This runs on ranks 1 through 3: {rank}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Cleanup\n",
                "\n",
                "Always shutdown the distributed workers when done:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%dist_shutdown"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "The jupyter_distributed extension provides:\n",
                "\n",
                "- **%dist_init**: Initialize distributed workers\n",
                "- **%dist_status**: Check worker status\n",
                "- **%dist_shutdown**: Shutdown workers\n",
                "- **%%distributed**: Execute code on all ranks\n",
                "- **%%rank[...]**: Execute code on specific ranks\n",
                "- **%sync**: Synchronize all ranks\n",
                "\n",
                "This enables interactive experimentation with distributed PyTorch algorithms directly in Jupyter notebooks!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "torch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
