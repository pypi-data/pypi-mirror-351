# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gb8EKmVgAa2u7qW6ONnsbMaBBqVblgJ1
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from math import sqrt
from xgboost import XGBRegressor
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

from statsmodels.tsa.stattools import adfuller

from .Processor import replace_negatives

#Yushu
#Models.py
#### Function for Statistics Model
def stats_data(df_use):
    # generating dataset for statistics model, which includes only the weekcase and the covariates
    # df_use: the current dataset to process
    # data - input for ARIMA model (single variate), only contains the weekcase
    # finaldf_poi - input for other statis models (multivariate), contains the covariates
    data = pd.DataFrame(df_use.weekcase)#only weekcase extracted
    data.index= df_use.index
    data.index = pd.to_datetime(data.index)
    data = data.reset_index(drop=True)
    data_poi = df_use[["weekcase", "Sin_Time1","Cos_Time1","Sin_Time2","Cos_Time2"]]
    finaldf_poi = data_poi.reset_index(drop=True)
    return data, finaldf_poi, pd.DataFrame(df_use.index, columns = ['week'])

#Yushu
def ARIMA_model(ARMAmodel,test,predict_length,confidence_interval):
    ARMAmodel = ARMAmodel.fit() #ARMAmodel is already trained and passed as a parameter
    test_length = len(test)
    y_pred = ARMAmodel.get_forecast(test_length)#forecast for the length
    y_pred_df = y_pred.conf_int(alpha = confidence_interval)
    y_pred_df["Predictions"] = ARMAmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
    y_pred_df.index = test.index
    y_pred_out = y_pred_df["Predictions"] ### Prediction of ARIMA model
    arima_mse = mean_squared_error(test,y_pred_out) ### Calculate MSE
    arima_rmse = np.sqrt(mean_squared_error(test,y_pred_out)) ### Calculate rMSE
    arima_mae = mean_absolute_error(test,y_pred_out) ### Calculate MAE
    arima_mape = mean_absolute_percentage_error(test,y_pred_out) ### Calculate the MAPE
    arima_list = np.abs(np.array(y_pred_out) - np.array(test)) / np.array(test)#np.array(np.abs(np.array(y_pred_out)-np.transpose(np.array(test)))/np.transpose(np.array(test))) ### Calculate the APE for each prediction value
    #print('y_pred_out[Predictions]', y_pred_df['Predictions'])
    list_mape = np.abs((test['weekcase'] - y_pred_df['Predictions']) / test['weekcase'])
    #print('arima_list', arima_list)
    #print('test', test['weekcase'])
    #print('y_pred_out', y_pred_out)
    #print('list_mape', list_mape)
    arimap= y_pred_df["Predictions"] # length - test_length
    #print('arima fitting size: ',predict_length-test_length)

    arimaf = ARMAmodel.fittedvalues.tail(predict_length-test_length) # length 25, always fixed cz, during parameter passing I added test_length to prediction_length, now sub
    arimap_df = arimap.rename("ARIMA").to_frame()


    arimaf_df = pd.DataFrame(arimaf, columns=["ARIMA"])


    arima = pd.concat([arimaf_df,arimap_df],axis=0) ### Combine fitting value with prediction value
    arima.columns = ['ARIMA']
    #print('arima fitting size: ',len(arima))
    return arima_mse, arima_rmse,arima_mae,arima_mape, list_mape,arima, arimap, arimaf


def generate_ml_features(data_ml, parameter_length):

    dataframe = pd.DataFrame()
    for i in range(parameter_length, 0, -1):
        dataframe['t+' + str(i)] = data_ml.weekcase.shift(i)
    final_data = pd.concat([data_ml,dataframe], axis=1)
    final_data.dropna(inplace=True)#fisr 'lag_reserve' points gone here
    finaldf = final_data.copy()

    data_index = pd.DataFrame(final_data.index, columns = ['week'])
    #print('data_index')
    #print(data_index)
    finaldf = finaldf.reset_index(drop=True)
    return finaldf, data_index

# def gen_ml(wave,shift_flag, test_length,parameter_length,lag_reserve, predictions, resid, shift=0):#generate train, test from whole data
#     max_prediction_length = max(predictions)

#     data_ml = pd.DataFrame(wave.weekcase)#I added this#
#     dataframe = pd.DataFrame()
#     for i in range(parameter_length, 0, -1):
#         dataframe['t+' + str(i)] = data_ml.weekcase.shift(i)
#     final_data = pd.concat([data_ml,dataframe], axis=1)
#     final_data.dropna(inplace=True)#fisr 'lag_reserve' points gone here
#     finaldf = final_data
#     data_index = pd.DataFrame(final_data.index, columns = ['week'])
#     #print('data_index')
#     #print(data_index)
#     finaldf = finaldf.reset_index(drop=True)
#     end_point = len(final_data.index)

#     #print('endpoint ', end_point)
#     #print('gen_ml test_length:',test_length)#, finaldf.tail(1))
#     if resid == False :
#         x = end_point-max_prediction_length#this part is now true. as not doing resid
#     else:
#         x = end_point - test_length
#     #print('x:',x,'x+test_length-1', (x+test_length-1))#JMM Changed here
#     #x:x+test_length--1
#     #finaldf_test = finaldf.loc[end_point - test_length:end_point,:]#changed JMM definition here
#     finaldf_test = finaldf.loc[x:x+test_length-1,:]#Yushu
#     #print('finaldf_test')
#     #print(finaldf_test)
#     finaldf_train = finaldf.loc[:x - 1-shift, :]
#     #test_index = data_index.loc[end_point - test_length:end_point,:]#JMM
#     test_index = data_index.loc[x:x+test_length-1,:]#Yushu
#     train_index = data_index.loc[:x - 1-shift, :]

#     #train_index = pd.DataFrame(finaldf_train.index, columns = ['week'])


#     finaldf_test_x = finaldf_test.loc[:, finaldf_test.columns != 'weekcase']
#     finaldf_test_y = finaldf_test['weekcase']
#     finaldf_train_x = finaldf_train.loc[:, finaldf_train.columns != 'weekcase']
#     finaldf_train_y = finaldf_train['weekcase']

#     #print('train_index')
#     #print(train_index)
#     #print('test_index')
#     #print(test_index)
#     return finaldf_train_x,finaldf_test_x,finaldf_train_y,finaldf_test_y, train_index, test_index

def gen_ml(wave,test_length,parameter_length,lag_reserve, predictions, resid, adjustment=0):#generate train, test from whole data
    max_prediction_length = max(predictions)
    # print('adjustment: ', adjustment)
    # print('test_length length, ', test_length)
    # data_ml = pd.DataFrame(wave.weekcase)#I added this#
    # dataframe = pd.DataFrame()
    # for i in range(parameter_length, 0, -1):
    #     dataframe['t+' + str(i)] = data_ml.weekcase.shift(i)
    # final_data = pd.concat([data_ml,dataframe], axis=1)
    # final_data.dropna(inplace=True)#fisr 'lag_reserve' points gone here
    # finaldf = final_data
    # data_index = pd.DataFrame(final_data.index, columns = ['week'])
    # #print('data_index')
    # #print(data_index)
    # finaldf = finaldf.reset_index(drop=True)
    data_ml = pd.DataFrame(wave.weekcase)#I added this#
    finaldf, data_index = generate_ml_features(data_ml, parameter_length)
    end_point = len(finaldf.index)

    #print('endpoint ', end_point)
    #print('gen_ml test_length:',test_length)#, finaldf.tail(1))
    if resid == False :
        x = end_point-max_prediction_length#this part is now true. as not doing resid
    else:
        x = end_point - test_length
    #print('x:',x,'x+test_length-1', (x+test_length-1))#JMM Changed here
    #x:x+test_length--1
    #finaldf_test = finaldf.loc[end_point - test_length:end_point,:]#changed JMM definition here
    finaldf_test = finaldf.loc[x:x+test_length-1,:]#Yushu
    # print('finaldf_test')
    # print(finaldf_test)
    finaldf_train = finaldf.loc[:x - 1-adjustment, :]
    # print('finaldf_train')
    # print(finaldf_train.tail())
    #test_index = data_index.loc[end_point - test_length:end_point,:]#JMM
    test_index = data_index.loc[x:x+test_length-1,:]#Yushu
    train_index = data_index.loc[:x - 1-adjustment, :]

    #train_index = pd.DataFrame(finaldf_train.index, columns = ['week'])


    finaldf_test_x = finaldf_test.loc[:, finaldf_test.columns != 'weekcase']
    finaldf_test_y = finaldf_test['weekcase']
    finaldf_train_x = finaldf_train.loc[:, finaldf_train.columns != 'weekcase']
    finaldf_train_y = finaldf_train['weekcase']

    #print('train_index')
    #print(train_index)
    #print('test_index')
    #print(test_index)
    return finaldf_train_x,finaldf_test_x,finaldf_train_y,finaldf_test_y, train_index, test_index


def ml_model(test_x,test_y,model,data):
    predict = model.predict(test_x) # prediction
    mape_list = np.abs(predict - test_y)/test_y # MAPE for each prediction value??? not sure what's this
    mse = mean_squared_error(data['weekcase'][-len(predict):].values,predict[:]) # MSE for prediction model
    rmse = sqrt(mean_squared_error(data['weekcase'][-len(predict):].values,predict[:]))
    mae = mean_absolute_error(data['weekcase'][-len(predict):].values,predict[:])
    mape = mean_absolute_percentage_error(data['weekcase'][-len(predict):].values,predict[:])
    return mape_list,mse,rmse,mae,mape,predict


##Yushu
#### Prepare for Esemble Model
def arima_res_xgb(ARMAmodel,data,train,test,fit_length,cv,training_period, overlap,confidence_interval, lag_reserve, resid,booster,learning_rate, n_estimators):
    test_length = len(test)
    data_length = len(train) + test_length
    arima_mse, arima_rmse,arima_mae,arima_mape, arima_list,arima, arimap, arimaf = ARIMA_model(ARMAmodel,test,data_length,confidence_interval)
    residual =  data.reset_index(drop = True).head(len(arima)).sub(arima.reset_index(drop = True),axis = 0) ## Residual = Real value - Fitted/Predicted
    finaldf_train_resid_x,finaldf_test_resid_x,finaldf_train_resid_y,finaldf_test_resid_y, train_index, test_index = gen_ml(residual,test_length,cv, lag_reserve,resid)#I should pass the lag reserve amount here
    xgb_resid = XGBRegressor(booster = booster, learning_rate = learning_rate, n_estimators = n_estimators).fit(finaldf_train_resid_x, finaldf_train_resid_y)
    xgb_resid_predict1 = xgb_resid.predict(finaldf_test_resid_x) #Predictions on Testing data
    A =(arima.reset_index(drop = True).tail(test_length) + xgb_resid_predict1)# Adding predicted residual on the predictted ARIMA value
    xgb_resid_list= np.abs(A.reset_index(drop = True)- test.weekcase)/ test.weekcase
    xgb_resid_p = A.reset_index(drop=True)
    xgb_resid_f = pd.DataFrame(xgb_resid.predict(finaldf_train_resid_x))
    xgb_resid_f = xgb_resid_f.add(pd.DataFrame(arima).tail(-overlap).reset_index(drop=True))
    xgb_resid_f  = xgb_resid_f.head(-test_length)
    xgb_resid_f = xgb_resid_f.tail(training_period).reset_index(drop=True)
    #print('**xgb_resid_predict1')
    xgb_resid = pd.concat([xgb_resid_f, xgb_resid_p], axis=0)

    #print(xgb_resid_predict1.shape)
    return xgb_resid, xgb_resid_predict1,arima, xgb_resid_list, xgb_resid_p, xgb_resid_f

def check_stationarity(time_series):
    result = adfuller(time_series)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    for key, value in result[4].items():
        print(f'Critical Value {key}:', value)
    if result[1] <= 0.05:
        print("The series is stationary.")
    else:
        print("The series is not stationary.")