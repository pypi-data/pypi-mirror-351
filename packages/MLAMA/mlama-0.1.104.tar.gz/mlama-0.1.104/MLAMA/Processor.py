# -*- coding: utf-8 -*-
"""Processor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNuWsNHT4SNtLZi9xbpT-5hrJii9BdXM
"""

import pandas as pd
import numpy as np
import os

#filename = "PHO weekly case.csv"
#filename = "PHO weekly case - EXTEND PREDICTION.csv"
#tuned_on_filename = "Martin.csv"#filename#same for now, will be user defined #the file on which the models were tuned

#path = '/path/to/your/file.txt'
#file_name = os.path.basename(path)  # Returns 'file.txt'
#folder_name = os.path.dirname(path)

#Visualization.py previous
def find_model_week_values_sum_shift(df, shift, weeks):
  """
  find values from the dataframe given shift for all weeks

  Args:
    df: the dataframe for a specific model
    shift: specific shift integer
    weeks: The list containing week values
  Returns:
    all_week_listTTT: Transposed according to week
    values: sum value for each week
  """
  #print(df[0])
  all_week_listTT = pd.DataFrame()
  values = pd.DataFrame()
  all_week_list = df[shift]#correct
  all_week_listT = all_week_list.T
  for week in weeks:
    all_week_listTT[str(week+1)] = all_week_listT.apply(lambda x: x[0][week], axis = 1)#to get the weeks as column names
  all_week_listTTT = all_week_listTT.T
  values = all_week_listTT.sum()
  return all_week_listTTT,values

# Generalized prevoius Visualization.py
def create_MAPE_df(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, inFoldername_pre, task_name, filename, models):
    visualization_data_dictionary = {}
    for WAVE in WAVES:
        waveID = WAVE.waveID
        for prediction_length in predictions:
            for shift in trend_adjustment_steps:
                # print("prediction_length: ", prediction_length, "shift: ", shift, "Wave: ", waveID)
                wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]
                print(metric_row)
                model_dataframes = {}
                for model_name, model_class in models.items():

                    model_data = metric_row[model_name]['Fit_Pred']


                    model_mape = metric_row[model_name]['MAPE']#.values[0]
                    week_values = metric_row[model_name]['Weeks']#.values[0]
                    observed_values = metric_row[model_name]['Observed']#.values[0]

                    model_data['week'] = week_values
                    model_data['Observed'] = observed_values
                    model_data.columns = [model_name, 'week', 'Observed']
                    #print(model_data)
                    model_dataframes[model_name] = model_data

                # Merge the dataframes for all models
                task = model_dataframes[list(models.keys())[0]]
                for model_name in list(models.keys())[1:]:
                    task = pd.merge(task, model_dataframes[model_name], on=['week', 'Observed'], how='inner')

                task = task.set_index('week')

                # Capture the MAPE values for each model to track performance per wave
                #for model_name in models.keys():
                #    model_task = metric_row['MAPE']#.values[0]

                visualization_data_dictionary[wave_shift_prediction_length_key] = task

    return visualization_data_dictionary

#previously in Visualization.py


def process_metrics_df(WAVE, frame, recent_week_count):
    # Concatenate the frames along the columns and get the tail based on recent_week_count
    metrics_df = pd.concat(frame, axis=1).tail(recent_week_count)

    # Reset the index to ensure it's a clean, sequential index
    metrics_df.reset_index(drop=True, inplace=True)

    # Set the 'Weeks' column as the index
    # Assuming 'Weeks' is one of the columns in the DataFrame
    metrics_df.set_index('week', inplace=True)

    # Replace negative values in the DataFrame
    metrics_df = metrics_df.map(replace_negatives)

    return metrics_df

def replace_negatives(x):
    if x < 0:
        return 0
    else:
        return x

# #generalized version previosuly in Visualization

# # Visualization function
# def create_prediction_df(WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename):
#     base_columns = ['Wave', 'Shift', 'Length', 'Observed']

#     # Dynamically create visualization_columns
#     visualization_columns = base_columns + list(models.keys())
#     visualization_data = pd.DataFrame(columns=visualization_columns)
#     visualization_data_dictionary = {}

#     for WAVE in WAVES:
#         waveID = WAVE.waveID
#         for prediction_length in predictions:
#             for shift in trend_adjustment_steps:
#                 # print("prediction_length: ", prediction_length, "shift: ", shift, "Wave: ", waveID)
#                 wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

#                 metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]

#                 # Initialize list for model DataFrames
#                 model_dfs = []
#                 for model_name in models:
#                     #print(metric_row[model_name])
#                     model_data = metric_row[model_name]['Predictions']#.values[0]
#                     #model_week_key = 'weekSTAT' if model_name == 'ARIMA' else 'weekML'
#                     #observed_key = 'Observed_fit_train_pred_test_STAT' if model_name == 'ARIMA' else 'Observed_fit_train_pred_test_ML'

#                     week_values = metric_row[model_name]['Weeks']#.values[0]#weekSTAT and weekML
#                     observed_values = metric_row[model_name]['Observed']#.values[0]#Observed_fit_train_pred_test_STAT and Observed_fit_train_pred_test_ML

#                     #print('OK')
#                     #print(week_values)
#                     #print(observed_values)
#                     # Create DataFrame for each model
#                     model_df = pd.DataFrame(model_data).reset_index(drop=True)
#                     #print('OKK')
#                     model_df['Weeks'] = week_values
#                     # model_df['Observed'] = observed_values
#                     model_df['Observed'] = observed_values.astype(np.int64)#.astype(float)

#                     #model_df['Observed'] = pd.to_numeric(model_df['Observed'] , errors='coerce')
#                     model_df.columns = [model_name, 'Weeks', 'Observed']
#                     model_dfs.append(model_df)

#                 # Merge all model DataFrames on 'week' and 'Observed'#all model merge here, not only three
#                 for i in range(len(model_dfs)):
#                       model_dfs[i]['Observed'] = model_dfs[i]['Observed'].astype('int64')

#                 result = model_dfs[0]
#                 for df in model_dfs[1:]:
#                     result = pd.merge(result, df, on=['Weeks', 'Observed'], how='inner')

#                 # print('result before ', result.dtypes)
#                 # Add metadata columns
#                 result['Wave'] = waveID
#                 result['Shift'] = shift
#                 result['Length'] = prediction_length

#                 # print('result all ', result.dtypes)

#                 # for df in model_dfs:
#                 #     print(df.dtypes)

#                 # Save the result
#                 # visualization_data_dictionary[wave_shift_prediction_length_key] = result
#                 # visualization_data = pd.concat([visualization_data, result])
#                 # #print('visualization_data', visualization_data.dtypes)
#                 # visualization_data.reset_index(drop=True, inplace=True)

#                 # Store original dtypes before concatenation
#                 original_dtypes = result.dtypes

#                 # Concatenate while preserving dtypes
#                 visualization_data_dictionary[wave_shift_prediction_length_key] = result
#                 visualization_data = pd.concat([visualization_data, result])

#                 # Restore original data types
#                 for col, dtype in original_dtypes.items():
#                     if col in visualization_data.columns:
#                         try:
#                             visualization_data[col] = visualization_data[col].astype(dtype)
#                         except Exception as e:
#                             print(f"Warning: Could not convert column '{col}' to original dtype {dtype} due to {e}")
#                 # print('visualization_data', visualization_data.dtypes)

#     # Save final visualization data
#     visualization_data.to_csv(inFoldername_pre + task_name + '_model_predictions_' + filename)
#     return visualization_data, visualization_data_dictionary #one is dictionary, one is dataframe columns as keys


# Visualization function
# def create_prediction_df(WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename):
#     base_columns = ['Wave', 'Shift', 'Length', 'Observed']

#     # Dynamically create visualization_columns
#     visualization_columns = base_columns + list(models.keys())
#     visualization_data = pd.DataFrame(columns=visualization_columns)
#     visualization_data_dictionary = {}

#     for WAVE in WAVES:
#         waveID = WAVE.waveID
#         for prediction_length in predictions:
#             for shift in trend_adjustment_steps:
#                 print("create_prediction_df prediction_length: ", prediction_length, "shift: ", shift, "Wave: ", waveID)
#                 wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

#                 metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]

#                 # Initialize list for model DataFrames
#                 model_dfs = []
#                 for model_name in models:
#                     print(metric_row[model_name])
#                     model_data = metric_row[model_name]['Predictions']
#                     week_values = metric_row[model_name]['Weeks']
#                     observed_values = metric_row[model_name]['Observed']

#                     # === HANDLE None VALUES ===
#                     if model_data is None or week_values is None or observed_values is None:
#                         print(f"Skipping model '{model_name}' for wave {waveID} shift {shift} prediction length {prediction_length} due to missing data.")
#                         continue  # Skip this model

#                     # === USE LAST N DATA POINTS ===

#                     week_values = week_values.iloc[-prediction_length:].reset_index(drop=True)#changed
#                     observed_values = observed_values.iloc[-prediction_length:].reset_index(drop=True)#changed

#                     model_df = pd.DataFrame(model_data).reset_index(drop=True)
#                     model_df['Weeks'] = week_values
#                     model_df['Observed'] = observed_values.astype(np.int64)

#                     model_df.columns = [model_name, 'Weeks', 'Observed']
#                     print('model_df', model_df)
#                     model_dfs.append(model_df)

#                 # Skip merging if less than two valid models
#                 if len(model_dfs) < 1:
#                     print(f"No valid models found for wave {waveID}, shift {shift}, prediction_length {prediction_length}.")
#                     continue  # <-- ADDED

#                 for i in range(len(model_dfs)):
#                     model_dfs[i]['Observed'] = model_dfs[i]['Observed'].astype('int64')
#                     model_dfs[i]['Weeks'] = pd.to_datetime(model_dfs[i]['Weeks'])

#                 result = model_dfs[0]
#                 for df in model_dfs[1:]:
#                     result = pd.merge(result, df, on=['Weeks', 'Observed'], how='inner')

#                 # Add metadata columns
#                 result['Wave'] = waveID
#                 result['Shift'] = shift
#                 result['Length'] = prediction_length

#                 # Store original dtypes before concatenation
#                 original_dtypes = result.dtypes

#                 # Concatenate while preserving dtypes
#                 visualization_data_dictionary[wave_shift_prediction_length_key] = result
#                 visualization_data = pd.concat([visualization_data, result])

#                 # Restore original data types
#                 for col, dtype in original_dtypes.items():
#                     if col in visualization_data.columns:
#                         try:
#                             visualization_data[col] = visualization_data[col].astype(dtype)
#                         except Exception as e:
#                             print(f"Warning: Could not convert column '{col}' to original dtype {dtype} due to {e}")

#     # Save final visualization data
#     visualization_data.to_csv(inFoldername_pre + task_name + '_model_predictions_' + filename)
#     return visualization_data, visualization_data_dictionary

#new from MLAMA Current 2025 file

# Visualization function
def create_prediction_df(WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename):
    base_columns = ['Wave', 'Shift', 'Length', 'Observed']
    #print(model_evaluation_dictionary)
    # Dynamically create visualization_columns
    visualization_columns = base_columns + list(models.keys())
    visualization_data = pd.DataFrame(columns=visualization_columns)
    visualization_data_dictionary = {}

    for WAVE in WAVES:
        waveID = WAVE.waveID
        for prediction_length in predictions:
            for shift in trend_adjustment_steps:
                print("create_prediction_df prediction_length: ", prediction_length, "shift: ", shift, "Wave: ", waveID)
                wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]

                # Initialize list for model DataFrames
                model_dfs = []
                for model_name in models:
                    print(metric_row[model_name])
                    model_data = metric_row[model_name]['Predictions']
                    week_values = metric_row[model_name]['Weeks']
                    observed_values = metric_row[model_name]['Observed']
                    print('week_values', week_values)
                    # === HANDLE None VALUES ===
                    if model_data is None or week_values is None or observed_values is None:
                        print(f"Skipping model '{model_name}' for wave {waveID} shift {shift} prediction length {prediction_length} due to missing data.")
                        continue  # Skip this model

                    # === USE LAST N DATA POINTS ===
                    # === USE LAST N DATA POINTS WITHOUT CONVERTING TO pd.Series ===
                    week_values = week_values.iloc[-prediction_length:].reset_index(drop=True)#changed
                    observed_values = observed_values.iloc[-prediction_length:].reset_index(drop=True)#changed


                    model_df = pd.DataFrame(model_data).reset_index(drop=True)
                    model_df['Weeks'] = week_values
                    model_df['Observed'] = observed_values.astype(np.int64)

                    model_df.columns = [model_name, 'Weeks', 'Observed']
                    print('model_df', model_df)
                    model_dfs.append(model_df)

                # Skip merging if less than two valid models
                if len(model_dfs) < 1:
                    print(f"No valid models found for wave {waveID}, shift {shift}, prediction_length {prediction_length}.")
                    continue  # <-- ADDED

                for i in range(len(model_dfs)):
                    model_dfs[i]['Observed'] = model_dfs[i]['Observed'].astype('int64')
                    model_dfs[i]['Weeks'] = pd.to_datetime(model_dfs[i]['Weeks'])

                result = model_dfs[0]
                for df in model_dfs[1:]:
                    result = pd.merge(result, df, on=['Weeks', 'Observed'], how='inner')

                # Add metadata columns
                result['Wave'] = waveID
                result['Shift'] = shift
                result['Length'] = prediction_length
                print('result', result)
                # Store original dtypes before concatenation
                original_dtypes = result.dtypes

                # Concatenate while preserving dtypes
                visualization_data_dictionary[wave_shift_prediction_length_key] = result
                visualization_data = pd.concat([visualization_data, result])

                # Restore original data types
                for col, dtype in original_dtypes.items():
                    if col in visualization_data.columns:
                        try:
                            visualization_data[col] = visualization_data[col].astype(dtype)
                        except Exception as e:
                            print(f"Warning: Could not convert column '{col}' to original dtype {dtype} due to {e}")

    # Save final visualization data
    visualization_data.to_csv(inFoldername_pre + task_name + '_model_predictions_' + filename)
    return visualization_data, visualization_data_dictionary

def add_time_features(df):#new
    """
    Adds seasonal time-based features to a time series DataFrame with datetime index.

    Assumes:
        - df.index is datetime-like (e.g., pd.to_datetime has been applied)
        - df contains a column 'weekcase'

    Adds:
        - Sin_Time1, Cos_Time1: short-term seasonality (e.g., annual)
        - Sin_Time2, Cos_Time2: longer-term seasonality (e.g., biannual)
    """

    # Convert datetime index to numerical representation (days since start)
    time_numeric = (df.index - df.index[0]).days

    # Example period definitions (adjust based on your seasonality understanding)
    period1 = 365.25      # yearly seasonality
    period2 = 365.25 / 2  # half-year seasonality

    # Compute features
    df["Sin_Time1"] = np.sin(2 * np.pi * time_numeric / period1)
    df["Cos_Time1"] = np.cos(2 * np.pi * time_numeric / period1)
    df["Sin_Time2"] = np.sin(2 * np.pi * time_numeric / period2)
    df["Cos_Time2"] = np.cos(2 * np.pi * time_numeric / period2)

    return df

def read_data(path):#change
  filename = os.path.basename(path)  # Returns 'file.txt'
  inFoldername_pre = os.path.dirname(path)

  # Read the first few rows without index to check the header
  preview = pd.read_csv(path, nrows=5)
  first_col = preview.columns[0]

  # Check if the first column is a duplicate of the index or not
  if first_col.lower() in ['unnamed: 0', 'index']:  # heuristics for common index column names
      df = pd.read_csv(path, index_col=0)
  else:
      df = pd.read_csv(path)
  # df = pd.read_csv(inFoldername_pre+'/'+filename,index_col=0)


  df.index = df.week # Transferring index as timeline
  #df.index = pd.to_datetime(df.index, format = '%m/%d/%Y').strftime("%Y-%m-%d")

  # Fix 'weekcase' format (remove commas and convert to int)
  df['weekcase'] = df['weekcase'].astype(str).str.replace(',', '', regex=False).astype(int)


  data = df.weekcase # single variable data, with index as timeline

  df.index = pd.to_datetime(df.index)
  df = add_time_features(df)#not used in the prediction anyways. we use single_test, full_test contains the time features



  #To make sure different Date formats work
  #need to be in the same cell as read, otherwise running the cell more than once after reading would cause error
  df['week'] = pd.to_datetime(df['week'], errors='coerce')
  df['week'] = df['week'].dt.strftime('%Y-%m-%d')

  #df.index = df.week
  df.set_index('week', inplace=True)
  return df

def get_hyperparameters(inFoldername_pre, tuned_on_filename, tuned_model_names):
  #exception if file not available , then provide default?
  #exception if not proper format
  #tuned_on_filename = os.path.basename(path)
  #inFoldername_pre = os.path.dirname(path)

  tuned_model_dict = {}

  for name in tuned_model_names:
      param_filename = inFoldername_pre_param+name+'_param_tunning_'+tuned_on_filename
      #param_filename = inFoldername_pre+'param/'+name+'_param_tunning.csv'
      data = pd.read_csv(param_filename)
      #print(data.shape)
      data = data.drop_duplicates(subset=['Dataset'])
      #print(data.shape)
      for wave_shift in data['Dataset']:
        tuned_model_dict[wave_shift+name] = data.loc[data['Dataset']==wave_shift]
        tuned_model_dict[wave_shift+name] = tuned_model_dict[wave_shift+name].drop(columns=['Dataset'])

  return tuned_model_dict



import json
import os

# Function to read a single JSON file for a specific model, wave, and shift
# def read_model_results(base_path, tuned_on_filename, model_name, wave, shift, n_splits, test_size, file_template):
#     """
#     Reads a JSON file for the given model, wave, and shift.

#     Parameters:
#         base_path (str): The directory where the JSON files are stored.
#         model_name (str): The name of the model (e.g., 'xgb', 'rf').
#         wave (integer): The wave identifier.
#         shift (integer): The shift identifier.
#         file_template (str): Template for the filename, containing placeholders for `wave`, `shift`, and `model`.

#     Returns:
#         dict: Parsed JSON data if the file exists, otherwise an empty dictionary.
#     """
#     # Generate the filename using the template
#     file_name = file_template.format(wave=wave, shift=shift, model=model_name, n_splits=n_splits, test_size=test_size, tuned_on_filename = tuned_on_filename)
#     file_path = os.path.join(base_path, file_name)
#     print(file_path)

#     # Attempt to read the file
#     if os.path.exists(file_path):
#         with open(file_path, 'r') as file:
#             return json.load(file)
#     else:
#         print(f"File not found: {file_path}")
#         return {}

def read_model_results(
    base_path,
    tuned_on_filename,
    model_name,
    wave,
    shift,
    n_splits,
    test_size,
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Reads the saved grid search results from JSON and returns the loaded dictionary.

    Parameters:
        base_path (str): Directory where the JSON file is stored.
        tuned_on_filename (str): Descriptor for the tuning dataset.
        model_name (str): Name of the model (e.g., 'xgb').
        wave (int): Wave identifier.
        shift (int): Shift identifier.
        n_splits (int): Number of cross-validation splits.
        test_size (int): Size of the test set.
        file_template (str): Template string for the filename.

    Returns:
        dict: Dictionary containing 'input_params' and 'output_params' loaded from JSON.
    """
    file_name = file_template.format(
        wave=wave,
        n_splits=n_splits,
        test_size=test_size,
        shift=shift,
        model=model_name,
        tuned_on_filename=tuned_on_filename
    )
    file_path = os.path.join(base_path, file_name)

    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return None

    with open(file_path, "r") as file:
        data = json.load(file)


    return data


# # Outer function to populate the tuned_model_dict
# def populate_tuned_model_dict(
#     base_path,
#     tuned_on_filename,
#     models,
#     waves,
#     shifts,
#     n_splits,
#     test_size,
#     file_template = "best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
#     ):
#     """
#     Populates a dictionary with model tuning results for each combination of model, wave, and shift.

#     Parameters:
#         base_path (str): The directory where the JSON files are stored.
#         models (list of str): List of model names (e.g., ['xgb', 'rf']).
#         waves (list of integer): List of wave identifiers.
#         shifts (list of integer): List of shift identifiers.
#         file_template (str): Template for the filename, containing placeholders for `wave`, `shift`, and `model`.

#     Returns:
#         dict: A dictionary containing the tuned model results, keyed by `wave+shift+model`.
#     """
#     tuned_model_dict = {}

#     for model in models:
#         for WAVE in waves:
#             for shift in shifts:
#                 # Generate the key for the dictionary
#                 #key = f"{wave}{shift}{model}"
#                 key = f'wave {WAVE.waveID} shift {shift}{model}'

#                 # Read the JSON file and save the data
#                 json_load = read_model_results(
#                     base_path=base_path,
#                     tuned_on_filename=tuned_on_filename,
#                     model_name=model,
#                     wave=WAVE.waveID,
#                     shift=shift,
#                     n_splits=n_splits,
#                     test_size=test_size,
#                     file_template=file_template
#                 )
#                 tuned_model_dict[key] = json_load
#                 print(json_load)

#     return tuned_model_dict


import json

def populate_tuned_model_dict(
    base_path,
    tuned_on_filename,
    models,
    waves,
    shifts,
    n_splits,
    test_size,
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Populates two dictionaries: one with tuned model hyperparameters and another with tuning input parameters.
    If input_params/output_params are missing, treats the entire content as output_params.

    Parameters:
        base_path (str): The directory where the JSON files are stored.
        models (list of str): List of model names (e.g., ['xgb', 'rf']).
        waves (list of objects with 'waveID' attribute): List of wave objects.
        shifts (list of int): List of shift identifiers.
        n_splits (int): Number of CV splits.
        test_size (int): Test set size.
        file_template (str): Template for the filename.

    Returns:
        tuple: (tuned_model_dict, input_params_dict)
    """
    tuned_model_dict = {}
    input_params_dict = {}

    for model in models:
        for WAVE in waves:
            for shift in shifts:
                key = f'wave {WAVE.waveID} shift {shift}{model}'

                # Read the JSON file
                json_load = read_model_results(
                    base_path=base_path,
                    tuned_on_filename=tuned_on_filename,
                    model_name=model,
                    wave=WAVE.waveID,
                    shift=shift,
                    n_splits=n_splits,
                    test_size=test_size,
                    file_template=file_template
                )

                print(f"\n=== Model: {model} | Wave: {WAVE.waveID} | Shift: {shift} ===")

                if json_load is not None:
                    if "input_params" in json_load and "output_params" in json_load:
                        input_params = json_load["input_params"]
                        output_params = json_load["output_params"]
                    else:
                        input_params = {}
                        output_params = json_load  # Assume the full dict is output

                    print("Input Parameters:")
                    print(json.dumps(input_params, indent=4))
                    print("Tuned Output Parameters:")
                    print(json.dumps(output_params, indent=4))
                else:
                    # File not found — use default params and log the message
                    print(f"Parameter file not found for {key}. Using default parameters.")
                    input_params = None
                    output_params = default_params.get(model, {})

                    print("Default Parameters:")
                    print(json.dumps(output_params, indent=4))

                # Save into the dictionaries
                input_params_dict[key] = input_params
                tuned_model_dict[key] = output_params


    return tuned_model_dict, input_params_dict



# def populate_tuned_model_dict_with_defaults(
#     base_path,
#     tuned_on_filename,
#     models,
#     waves,
#     shifts,
#     default_params,
#     file_template="best_params_wave {wave} shift {shift}_{model}_results_{tuned_on_filename}.json",
# ):
#     """
#     Populates a dictionary with model tuning results for each combination of model, wave, and shift.
#     If a parameter file is not found, it assigns default parameters for the specific combination.

#     Parameters:
#         base_path (str): The directory where the JSON files are stored.
#         tuned_on_filename (str): Filename for the data the models are tuned on.
#         models (list of str): List of model names (e.g., ['xgb', 'rf']).
#         waves (list of integer): List of wave identifiers.
#         shifts (list of integer): List of shift identifiers.
#         default_params (dict): Dictionary containing default parameters for models.
#         file_template (str): Template for the filename, containing placeholders for `wave`, `shift`, and `model`.

#     Returns:
#         dict: A dictionary containing the tuned model results, keyed by `wave+shift+model`.
#     """
#     tuned_model_dict = {}

#     for model in models:
#         for WAVE in waves:
#             for shift in shifts:
#                 # Generate the key for the dictionary
#                 key = f'wave {WAVE.waveID} shift {shift}{model}'

#                 # Construct the file path
#                 file_path = os.path.join(
#                     base_path,
#                     file_template.format(
#                         wave=WAVE.waveID,
#                         shift=shift,
#                         model=model,
#                         tuned_on_filename=tuned_on_filename
#                     )
#                 )

#                 # Check if the file exists
#                 if os.path.exists(file_path):
#                     # Read the JSON file and save the data
#                     with open(file_path, 'r') as f:
#                         json_load = json.load(f)
#                         print(f"Loaded parameters for {key} from {file_path}")
#                 else:
#                     # Use default parameters and log the message
#                     print(f"Parameter file not found for {key}. Using default parameters.")
#                     json_load = default_params.get(model, {})

#                 # Save the parameters (either loaded or default) in the dictionary
#                 tuned_model_dict[key] = json_load

#     return tuned_model_dict

import os
import json

def populate_tuned_model_dict_with_defaults(
    base_path,
    tuned_on_filename,
    models,
    waves,
    shifts,
    n_splits,
    test_size,
    default_params=None,  # Allow default_params to be optional
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Populates a dictionary with model tuning results for each combination of model, wave, and shift.
    If a parameter file is not found, it assigns default parameters for the specific combination.
    If default_params is not provided, it falls back to the original model defaults.

    Parameters:
        base_path (str): The directory where the JSON files are stored.
        tuned_on_filename (str): Filename for the data the models are tuned on.
        models (list of str): List of model names (e.g., ['xgb', 'rf']).
        waves (list of integer): List of wave identifiers.
        shifts (list of integer): List of shift identifiers.
        default_params (dict, optional): Dictionary containing default parameters for models.
                                         If None, will attempt to use the model's internal defaults.
        file_template (str): Template for the filename, containing placeholders for `wave`, `shift`, and `model`.

    Returns:
        dict: A dictionary containing the tuned model results, keyed by `wave+shift+model`.
    """
    tuned_model_dict = {}

    for model in models:
        for WAVE in waves:
            for shift in shifts:
                key = f'wave {WAVE.waveID} shift {shift}{model}'
                file_path = os.path.join(
                    base_path,
                    file_template.format(
                        wave=WAVE.waveID,
                        shift=shift,
                        model=model,
                        tuned_on_filename=tuned_on_filename
                    )
                )

                if os.path.exists(file_path):
                    with open(file_path, 'r') as f:
                        json_load = json.load(f)
                        print(f"Loaded parameters for {key} from {file_path}")
                else:
                    print(f"Parameter file not found for {key}. Using default parameters.")

                    # If default_params is provided, get model-specific defaults; otherwise, use an empty dict.
                    json_load = (default_params.get(model, {}) if default_params else {})

                tuned_model_dict[key] = json_load

    return tuned_model_dict

import os
import json

def read_grid_search_results(output_folder, output_filename):
    """
    Reads the grid search results from a JSON file and extracts useful information.

    Parameters:
        output_folder: Folder where the results are stored.
        output_filename: Filename of the saved grid search results.

    Returns:
        results: A dictionary containing:
            - best_params: Dictionary of the best hyperparameters.
            - best_score: The best score achieved.
            - all_results: Full content of the JSON file (for additional inspection).
    """
    result_path = os.path.join(output_folder, output_filename)

    # Check if the file exists
    if not os.path.exists(result_path):
        raise FileNotFoundError(f"File not found at {result_path}. Please check the folder and filename.")

    # Load the JSON file
    with open(result_path, "r") as file:
        grid_results = json.load(file)

    # Extract best parameters and best score
    if "rank_test_score" in grid_results and "mean_test_score" in grid_results:
        best_index = grid_results["rank_test_score"].index(1)  # Rank 1 corresponds to the best parameters
        best_params = {param: grid_results["params"][best_index][param] for param in grid_results["params"][best_index]}
        best_score = grid_results["mean_test_score"][best_index]
    else:
        raise ValueError("The JSON file does not contain expected fields: 'rank_test_score' or 'mean_test_score'.")

    # Return the parsed results
    return {
        "best_params": best_params,
        "best_score": best_score,
        "all_results": grid_results
    }

#Gnereralized
# Function to calculate MAPE
def calculate_mape(predicted, observed):
    return abs(predicted - observed) / observed

def calculate_model_MAPE(wave_map, visualization_data, models):
    """
    Generalized function to calculate MAPE for multiple ML models.

    Args:
        wave_map (dict): Mapping of wave numbers to labels.
        visualization_data (DataFrame): DataFrame containing model predictions and observed values.
        models (dict): Dictionary of models with their keys (e.g., {'ARIMA': SARIMAX, 'RF': RandomForestRegressor}).

    Returns:
        DataFrame: Final grouped DataFrame with average MAPE values.
    """
    # Map the Wave values to specified labels
    visualization_data['Wave'] = visualization_data['Wave'].map(wave_map)

    # Dynamically calculate MAPE for each model
    for model_name in models.keys():
        column_name = f"{model_name}_MAPE"
        visualization_data[column_name] = calculate_mape(
            visualization_data[model_name], visualization_data['Observed']
        )

    # Melt the DataFrame to long format for all MAPE columns
    mape_columns = [f"{model_name}_MAPE" for model_name in models.keys()]
    melted_df = visualization_data.melt(
        id_vars="Wave",
        value_vars=mape_columns,
        var_name="Model",
        value_name="MAPE"
    )

    # Clean up Model column by removing '_MAPE'
    melted_df['Model'] = melted_df['Model'].str.replace('_MAPE', '')

    # Rename 'Wave' column to 'Input'
    melted_df.rename(columns={'Wave': 'Input'}, inplace=True)

    # Calculate mean MAPE for each wave and model combination
    final_task_df = melted_df.groupby(['Input', 'Model'], as_index=False)['MAPE'].mean()

    # Display the final DataFrame
    print(final_task_df)
    return final_task_df

### Someone else's code.

"""
This module provides a class to split time-series data for back-testing and evaluation.
The aim was to extend the current sklearn implementation and extend it's uses.

Might be useful for some ;)
"""

import logging
from typing import Optional

import numpy as np
from sklearn.model_selection._split import _BaseKFold
from sklearn.utils import indexable
from sklearn.utils.validation import _num_samples

LOGGER = logging.getLogger(__name__)


class TimeSeriesSplit(_BaseKFold):  # pylint: disable=abstract-method
    """Time Series cross-validator

    Provides train/test indices to split time series data samples that are observed at fixed time intervals,
    in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is
    inappropriate.

    This cross_validation object is a variation of :class:`TimeSeriesSplit` from the popular scikit-learn package.
    It extends its base functionality to allow for expanding windows, and rolling windows with configurable train and
    test sizes and shifts between each. i.e. train on weeks 1-8, skip week 9, predict week 10-11.

    In this implementation we specifically force the test size to be equal across all splits.

    Expanding Window:

            Idx / Time  0..............................................n
            1           |  train  | shift |  test  |                   |
            2           |       train     | shift  |  test  |          |
            ...         |                                              |
            last        |            train            | shift |  test  |

    Rolling Windows:
            Idx / Time  0..............................................n
            1           | train   | shift |  test  |                   |
            2           | step |  train  | shift |  test  |            |
            ...         |                                              |
            last        | step | ... | step |  train  | shift |  test  |

    Parameters:
        n_splits : int, default=5
            Number of splits. Must be at least 4.

        train_size : int, optional
            Size for a single training set.

        test_size : int, optional, must be positive
            Size of a single testing set

        shift : int, default=0, must be positive
            Number of index shifts to make between train and test sets
            e.g,
            shift=0
                TRAIN: [0 1 2 3] TEST: [4]
            shift=1
                TRAIN: [0 1 2 3] TEST: [5]
            shift=2
                TRAIN: [0 1 2 3] TEST: [6]

        force_step_size : int, optional
            Ignore split logic and force the training data to shift by the step size forward for n_splits
            e.g
            TRAIN: [ 0  1  2  3] TEST: [4]
            TRAIN: [ 0  1  2  3  4] TEST: [5]
            TRAIN: [ 0  1  2  3  4  5] TEST: [6]
            TRAIN: [ 0  1  2  3  4  5  6] TEST: [7]

    Examples
    --------
    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4],[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
    >>> y = np.array([1, 2, 3, 4, 5, 6,7,8,9,10,11,12])
    >>> tscv = TimeSeriesSplit(train_size=3,n_splits=5)
    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
    TimeSeriesSplit(shift=0, force_step_size=None, n_splits=5, test_size=None,
        train_size=None)
    >>> for train_index, test_index in tscv.split(X):
    ...    print('TRAIN:', train_index, 'TEST:', test_index)
    ...    X_train, X_test = X[train_index], X[test_index]
    ...    y_train, y_test = y[train_index], y[test_index]
    TRAIN: [0] TEST: [1]
    TRAIN: [0 1] TEST: [2]
    TRAIN: [0 1 2] TEST: [3]
    TRAIN: [0 1 2 3] TEST: [4]
    TRAIN: [0 1 2 3 4] TEST: [5]
    """

    def __init__(self,
                 n_splits: Optional[int] = 5,
                 train_size: Optional[int] = None,
                 test_size: Optional[int] = None,
                 shift: int = 0,
                 force_step_size: Optional[int] = None):
        print('Custom TimeSeriesSplit')
        if n_splits and n_splits < 5:
            raise ValueError(f'Cannot have n_splits less than 5 (n_splits={n_splits})')
        super().__init__(n_splits, shuffle=False, random_state=None)

        self.train_size = train_size

        if test_size and test_size < 0:
            raise ValueError(f'Cannot have negative values of test_size (test_size={test_size})')
        self.test_size = test_size

        if shift < 0:
            raise ValueError(f'Cannot have negative values of shift (shift={shift})')
        self.shift = shift

        if force_step_size and force_step_size < 1:
            raise ValueError(f'Cannot have zero or negative values of force_step_size '
                             f'(force_step_size={force_step_size}).')

        self.force_step_size = force_step_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set.

        Parameters:
            X : array-like, shape (n_samples, n_features)
                Training data, where n_samples is the number of samples  and n_features is the number of features.

            y : array-like, shape (n_samples,)
                Always ignored, exists for compatibility.

            groups : array-like, with shape (n_samples,), optional
                Always ignored, exists for compatibility.

        Yields:
            train : ndarray
                The training set indices for that split.

            test : ndarray
                The testing set indices for that split.
        """
        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        shift = self.shift

        if n_folds > n_samples:
            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')

        indices = np.arange(n_samples)
        split_size = n_samples // n_folds

        train_size = self.train_size or split_size #* self.n_splits
        test_size = self.test_size or n_samples // n_folds
        full_test = test_size + shift

        if full_test + n_splits > n_samples:
            raise ValueError(f'test_size\\({test_size}\\) + shift\\({shift}\\) = {test_size + shift} + '
                             f'n_splits={n_splits} \n'
                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')

        # Generate logic for splits.
        # Overwrite fold test_starts ranges if force_step_size is specified.
        if self.force_step_size:
            step_size = self.force_step_size
            final_fold_start = n_samples - (train_size + full_test)
            range_start = (final_fold_start % step_size) + train_size

            test_starts = range(range_start, n_samples, step_size)

        else:
            if not self.train_size:
                step_size = split_size
                range_start = (split_size - full_test) + split_size + (n_samples % n_folds)
            else:
                step_size = (n_samples - (train_size + full_test)) // n_folds
                final_fold_start = n_samples - (train_size + full_test)
                range_start = (final_fold_start - (step_size * (n_splits - 1))) + train_size

            test_starts = range(range_start, n_samples, step_size)

        # Generate data splits.
        for test_start in test_starts:
            idx_start = test_start - train_size if self.train_size is not None else 0
            # Ensure we always return a test set of the same size
            if indices[test_start:test_start + full_test].size < full_test:
                continue
            yield (indices[idx_start:test_start],
                   indices[test_start + shift:test_start + full_test])

import pandas as pd
import numpy as np
from collections import defaultdict

from collections import defaultdict
import numpy as np
#updated
def extract_model_weights_from_dict(weekly_prediction_future):
    """
    Extracts model weights per date and shift-length combination from weekly_prediction_future.

    Parameters:
        weekly_prediction_future (dict): Dictionary mapping each week (str) to a DataFrame containing model weights.

    Returns:
        dict: Nested dictionary structured as:
              {
                '2025-01-05': {
                    '0 1': (['ARIMA', 'RF', 'xgb'], [0.29, 0.33, 0.37]),
                    '0 2': (...),
                    ...
                },
                ...
              }
    """
    model_weights = {}

    for date_key, df in weekly_prediction_future.items():
        shift_weight_dict = {}
        for _, row in df.iterrows():
            shift_length = f"{int(row['Shift'])} {int(row['Length'])}"
            weights = np.array([row['weight_ARIMA'], row['weight_RF'], row['weight_xgb']])
            shift_weight_dict[shift_length] = (['ARIMA', 'RF', 'xgb'], weights)
        model_weights[date_key] = shift_weight_dict

    return model_weights



def extract_predictions_from_dict(result_dict):
    """
    Extracts weekly predictions and observed values from a nested dictionary of model evaluation results.

    Parameters:
    result_dict (dict): Dictionary where keys are week start dates and values are DataFrames
                        containing prediction results for multiple weeks.

    Returns:
    pd.DataFrame: Consolidated DataFrame with columns: 'Week', 'Observed', 'ARIMA', 'RF', 'xgb', 'MLAMA'
    """
    rows = []

    for week_start, df in result_dict.items():
        for i, row in df.iterrows():
            week = row['Week']
            observed = row['Observed']

            # Extract predictions: handle Series or list
            arima_pred = row['ARIMA'].values[0] if hasattr(row['ARIMA'], 'values') else row['ARIMA']
            rf_pred = row['RF'][0] if isinstance(row['RF'], list) else row['RF']
            xgb_pred = row['xgb'][0] if isinstance(row['xgb'], list) else row['xgb']
            mlama_pred = row['MLAMA'].values[0] if hasattr(row['MLAMA'], 'values') else row['MLAMA']

            rows.append({
                'Week': pd.to_datetime(week),
                'Observed': observed,
                'ARIMA': arima_pred,
                'RF': rf_pred[0],
                'xgb': xgb_pred[0],
                'MLAMA': mlama_pred
            })

    result_df = pd.DataFrame(rows)
    result_df.sort_values(by='Week', inplace=True)
    result_df.reset_index(drop=True, inplace=True)
    return result_df

def extract_evaluation_from_dict(result_dict, models, metric="MAPE"):
    """
    Extracts evaluation metrics (e.g., MAPE, MSE) from a nested dictionary of model evaluation results.

    Parameters:
        result_dict (dict): Dictionary where keys are week start dates and values are DataFrames
                            containing prediction results and evaluation metrics.
        models (dict): Dictionary of models with their names as keys.
        metric (str): Evaluation metric to extract (e.g., 'MAPE' or 'MSE').

    Returns:
        pd.DataFrame: Consolidated DataFrame with columns: 'Week', '<model1>_<metric>', ..., '<modelN>_<metric>'
    """
    rows = []

    for week_start, df in result_dict.items():
        for i, row in df.iterrows():
            week = pd.to_datetime(row['Week'])
            eval_row = {'Week': week}

            for model_name in models.keys():
                metric_key = f"{model_name}_{metric}"
                if metric_key in row:
                    metric_val = row[metric_key]
                    # If value is inside a list or Series, extract the scalar
                    if hasattr(metric_val, 'values'):
                        metric_val = metric_val.values[0]
                    elif isinstance(metric_val, list):
                        metric_val = metric_val[0]
                    eval_row[metric_key] = metric_val
                else:
                    eval_row[metric_key] = None  # In case key doesn't exist

            rows.append(eval_row)

    result_df = pd.DataFrame(rows)
    result_df.sort_values(by='Week', inplace=True)
    result_df.reset_index(drop=True, inplace=True)
    return result_df

import numpy as np
import pandas as pd

def extract_predictions_from_dict(result_dict, target_length, models):
    import numpy as np
    import pandas as pd

    rows = []

    required_models = ['MLAMA', 'ARIMA']
    optional_models = list(models.keys()) if models else []
    all_models = list(dict.fromkeys(required_models + optional_models))

    for week_start, df in result_dict.items():
        filtered_df = df[df['Length'] == target_length]

        for _, row in filtered_df.iterrows():
            prediction_row = {
                'CurrentWeek': pd.to_datetime(week_start),        # ✅ This is your dictionary key
                'Week': pd.to_datetime(row['Week']),              # ✅ This is the prediction target week
                'Shift': row['Shift'],
                'Length': row['Length'],
                'Observed': row['Observed']
            }

            for model_name in all_models:
                value = row.get(model_name, None)
                if isinstance(value, (list, np.ndarray)):
                    prediction_row[model_name] = value[0] if len(value) > 0 else None
                elif hasattr(value, 'values') and len(value.values) == 1:
                    prediction_row[model_name] = value.values[0]
                else:
                    prediction_row[model_name] = value

            rows.append(prediction_row)

    result_df = pd.DataFrame(rows)
    result_df.sort_values(by='Week', inplace=True)
    result_df.reset_index(drop=True, inplace=True)
    return result_df


def extract_all_predictions_from_dict(result_dict, models):
    """
    Extracts weekly predictions for all prediction lengths from a nested dictionary of results.

    Parameters:
        result_dict (dict): Dictionary where keys are current week and values are DataFrames.
        models (dict): Dictionary of model objects with names as keys.

    Returns:
        pd.DataFrame: Combined DataFrame for all prediction lengths across all models.
    """
    all_lengths = set()
    for df in result_dict.values():
        all_lengths.update(df['Length'].unique())

    all_dfs = []
    for length in sorted(all_lengths):
        df = extract_predictions_from_dict(result_dict, target_length=length, models=models)
        all_dfs.append(df)

    return pd.concat(all_dfs, axis=0).reset_index(drop=True)