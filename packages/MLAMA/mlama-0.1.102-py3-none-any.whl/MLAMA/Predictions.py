# -*- coding: utf-8 -*-
"""Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_OWOjyFLje8iOhoCfpmJmMRDhOX6tLu
"""

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, arima_res_xgb, check_stationarity, generate_ml_features
from .Processor import process_metrics_df, create_prediction_df, replace_negatives
from .Weighted import find_diff, find_stan_optimal_weights, find_weighted_predictions

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import pandas as pd
import numpy as np

#Currently Prediction.py, I might move this to Models

#### Prediction on Wave 1,2,3 Data [Prediction 1,2,3,4,5,6 weeks]

#tuned_model_dict contains param for specific shift, wave all models
#generalized version
def wave_prediction(shift_type, wave_start_shift_matrix, predictions, prediction_length, shift, WAVE, waveID,
                    training_period, lag_reserve, lagged_amount, overlap, confidence_interval, tuned_model_dict, models):
    """
    Predicts weekly cases for multiple models dynamically.

    Args:
        shift_type: Flag for shift definition. 'Delay'/'Responsiveness'.
        predictions: List of prediction steps.
        prediction_length: Length of the prediction.
        shift: shift in weeks.
        WAVE: Wave data object.
        waveID: Identifier for the wave.
        training_period: Training period length.
        lag_reserve: Reserve for lag data.
        lagged_amount: How much past data is used.
        tuned_model_dict: Dictionary of tuned parameters for all models.
        models: Dictionary of models with their initialization functions. Example: {'ARIMA': SARIMAX, 'RF': RandomForestRegressor}.

    Returns:
        metric_row: Dictionary containing metrics and predictions for all models.
    """
    print("Prediction Length:", prediction_length, "Shift:", shift, "Wave:", waveID)

    # Generate wave, train, and test data with shift
    wave, train, test = WAVE.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)
    single_train, full_train, fit_weeks_stat = stats_data(train) #single_ means only one feature weekcase is used
    single_test, full_test, pred_weeks_stat = stats_data(test.head(prediction_length))
    #print('**fit_pred_weeks_stat')
    fit_pred_weeks_stat = pd.concat([fit_weeks_stat.tail(training_period), pred_weeks_stat], axis = 0).reset_index(drop = True)
    #print('sungle_test ', single_test.shape)
    #print(fit_pred_weeks_stat.shape)
    fit_pred_y_STAT = pd.concat([single_train[-training_period:], single_test], axis = 0).reset_index(drop = True)
    # print('fit_pred_y_STAT', fit_pred_y_STAT)
    # Output metrics container
    metric_row = {}

    #### ARIMA Model
    if 'ARIMA' in models:
        arima_params = tuned_model_dict['ARIMA']
        print("ARIMA Parameters:", arima_params)

        ARMAmodel = SARIMAX(single_train,
                            order=arima_params['order'],
                            seasonal_order=arima_params['seasonal_order'])
        arima_mse, arima_rmse, arima_mae, arima_mape, arima_list, arima, arimap, arimaf = ARIMA_model(ARMAmodel, single_test,
                                                                                       training_period + prediction_length,
                                                                                       confidence_interval)

        arimap = arimap.reset_index(drop=True)
        arima = arima.reset_index(drop=True)

        #arima.columns = ['ARIMA']

        #arima_error = pd.DataFrame(arima_list[0])
        print(arima_list)
        metric_row['ARIMA'] = {
            'MSE': arima_mse,
            'RMSE': arima_rmse,
            'MAE': arima_mae,
            'MAPE': arima_mape,
            'Predictions': arimap,
            'Fit': arimaf,
            'List': arima_list,
            'Weeks': fit_pred_weeks_stat,
            'Observed': fit_pred_y_STAT,
            'Fit_Pred': arima
        }

    #### Generate ML Data
    adjustment = 0
    if shift_type=='Delay':
        adjustment = shift

    train_x, test_x, train_y, test_y, fit_weeks_ml, pred_weeks_ml = gen_ml(wave, prediction_length, lagged_amount,
                                                                           prediction_length, predictions, False, adjustment)
    fit_pred_weeks_ml = pd.concat([fit_weeks_ml.tail(training_period), pred_weeks_ml], axis=0).reset_index(drop=True)
    fit_pred_y_ML = pd.concat([train_y[-training_period:], test_y], axis=0).reset_index(drop=True)
    # print('ARIMA, ', arima_list)
    #### Machine Learning Models
    for model_name, model_class in models.items():
        if model_name == 'ARIMA':
            continue  # ARIMA is handled separately

        # Extract parameters for the current model
        tuned_params = tuned_model_dict.get(model_name, {})
        print(f"Training {model_name} with Parameters:", tuned_params)

        # Initialize and fit the model dynamically
        model = model_class(**tuned_params)
        model.fit(train_x, train_y)

        # Predictions and metrics
        predictions = model.predict(test_x)

        mse = mean_squared_error(test_y, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(test_y, predictions)
        list_mape = np.abs((test_y - predictions) / test_y)
        mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100
        #mape = mean_absolute_percentage_error(data['weekcase'][-len(predict):].values,predict[:])
        #xgb_mape = mean_absolute_percentage_error(wave.weekcase[-len(xgb_predict):].values,xgb_predict[:])
        fit = model.predict(train_x)
        #fit = fit.reset_index(drop = True)
        fit = pd.DataFrame(fit).reset_index(drop = True).tail(training_period)
        fit = fit.reset_index(drop=True)

        predictions = pd.DataFrame(predictions).reset_index(drop = True)

        fit_pred = pd.concat([fit,predictions],axis=0)

        fit_pred = fit_pred.reset_index(drop=True)
        #print(fit_pred.head())

        #_list,_mse,_rmse,_mae,_mape, _predict = ml_model(test_x,test_y,model,train)

        # print(model_name, 'ML, ', list_mape)

        # Store metrics
        metric_row[model_name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'Predictions': predictions,
            'Fit': fit,
            'List': list_mape,
            'Weeks': fit_pred_weeks_ml,
            'Observed': fit_pred_y_ML,
            'Fit_Pred': fit_pred
        }
    #print(metric_row['ARIMA'])
    return metric_row



#generalized
#should we return the models from the wave_prediction

#waveID = 1
#predictions.py

def seperate_model_eval(WAVES, models, predictions, trend_adjustment_steps, inFoldername_pre, filename, tuned_model_names, tuned_model_dict, shift_type, wave_start_shift_matrix, training_period, lag_reserve, lagged_amount, overlap, confidence_interval):
  '''
  COLUMN_NAMES not needed, later baad dibo

  '''
  #metrics_df = pd.DataFrame(columns=COLUMN_NAMES)#initialize in the cell, so that any previous values if any are removed

  model_evaluation_dictionary = {}

  for WAVE in WAVES:
    waveID = WAVE.waveID

    for prediction_length in predictions:
      wave_shift_tuned_params = {}
      #print(prediction_length)
      for shift in trend_adjustment_steps:#all shift done here
        for tuned_model_name in tuned_model_names:
          wave_shift_model_key = 'wave '+str(waveID)+' shift '+str(shift)+tuned_model_name
          wave_shift_tuned_params[tuned_model_name]=tuned_model_dict[wave_shift_model_key]
        #print(waveID, prediction_length, shift)

        #wave, train, test = WAVE.get_wave_dates_with_shift(shift)#this should be done inside the function?
        metric_row =  wave_prediction(
            shift_type, wave_start_shift_matrix, predictions, prediction_length, shift, WAVE, waveID, training_period, lag_reserve, lagged_amount, overlap, confidence_interval, wave_shift_tuned_params, models)
        #print(metric_row)

        wave_shift_prediction_length_key = 'wave '+str(waveID)+' shift '+str(shift)+' prediction_length '+str(prediction_length)
        model_evaluation_dictionary[wave_shift_prediction_length_key] = metric_row
        #metrics_df = pd.concat([metrics_df, metric_row])#, ignore_index=True


  base_columns = ['Wave', 'Shift', 'Length', 'Observed']

  # Dynamically create visualization_columns
  visualization_columns = base_columns + list(models.keys())

  #visualization_columns = ['Wave', 'Shift', 'Length', 'Observed', 'XGBoost', 'RF', 'ARIMA']# xgb_resid = arima_boost, sequential arima and xgb
  task_name = 'all'
  concise_data, concise_data_dictionary = create_prediction_df(WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename)

  return model_evaluation_dictionary, concise_data

#generalized version
#predictions.py


from collections import defaultdict
import pandas as pd

def individual_model_weekly_count(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, models, recent_week_count):
    """
    Generalized function to process weekly counts for multiple models, storing results by model, wave, and shift.

    Args:
        WAVES (list): List of WAVE objects.
        predictions (list): List of prediction lengths.
        trend_adjustment_steps (list): List of shifts.
        model_evaluation_dictionary (dict): Dictionary containing model evaluation results.
        models (dict): Dictionary of model names and their corresponding classes.

    Returns:
        model_results (dict): A dictionary where keys are model names, and values are dictionaries
                              containing processed results for each WAVE and shift.
                              model_results[model_name][waveID][shift] = processed DataFrame
    """
    # Initialize dictionary to store results for each model, wave, and shift
    model_results = {model_name: defaultdict(dict) for model_name in models.keys()}

    for WAVE in WAVES:
        waveID = WAVE.waveID  # Extract wave ID

        for shift in trend_adjustment_steps:
            # Initialize storage for predictions of each model
            model_frames = {model_name: [] for model_name in models.keys()}
            # Initialize a dictionary to store the most recent fit_pred_weeks for each model
            most_recent_fit_pred_weeks = {model_name: None for model_name in models.keys()}

            for prediction_length in predictions:
                # Generate key for accessing the metrics
                wave_shift_prediction_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                # Check if key exists in the dictionary
                if wave_shift_prediction_key not in model_evaluation_dictionary:
                    print(f"Warning: Key {wave_shift_prediction_key} not found in model_evaluation_dictionary.")
                    continue

                metrics_row = model_evaluation_dictionary[wave_shift_prediction_key]

                # Extract results for each model dynamically
                for model_name in models.keys():
                    if model_name not in metrics_row:
                        print(f"Warning: Model {model_name} not found in metrics for {wave_shift_prediction_key}.")
                        continue

                    model_prediction = metrics_row[model_name]['Fit_Pred']
                    fit_pred_weeks = metrics_row[model_name]['Weeks']

                    if isinstance(model_prediction, pd.DataFrame):  # Ensure it's a DataFrame
                        model_prediction.columns = [f'{model_name}_{prediction_length}']
                        model_frames[model_name].append(model_prediction)

                        # Store the most recent fit_pred_weeks for each model
                        most_recent_fit_pred_weeks[model_name] = fit_pred_weeks
                    else:
                        print(f"Warning: Model {model_name} prediction is not a DataFrame for {wave_shift_prediction_key}.")

            # Append the most recent fit_pred_weeks for each model after the loop
            for model_name in models.keys():
                if most_recent_fit_pred_weeks[model_name] is not None:
                    model_frames[model_name].append(most_recent_fit_pred_weeks[model_name])

            # Process predictions for each model
            for model_name, frames in model_frames.items():
                if frames:  # Only process if there are valid frames
                    processed_df = process_metrics_df(WAVE, frames, recent_week_count)
                    #processed_df = processed_df.applymap(replace_negatives)
                    model_results[model_name][waveID][shift] = processed_df  # Store with shift key

    return model_results

#generalized
def individual_model_weekly_MAPE(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, models):
    """
    Generalized function for calculating weekly MAPE values for multiple models.

    Args:
        WAVES: List of wave objects, each with a unique waveID.
        predictions: List of prediction lengths.
        trend_adjustment_steps: List of shifts.
        model_evaluation_dictionary: Dictionary containing evaluation metrics for models.
        models: Dictionary mapping model names to their respective prediction keys in model_evaluation_dictionary.

    Returns:
        A dictionary with model names as keys and corresponding weekly MAPE dataframes as values.
    """
    # Initialize a dictionary to hold results for all models
    model_results_wave_collection = {model: {} for model in models}

    weeks_column = [x for x in predictions]

    #print(weeks_column)
    #print(predictions)

    for WAVE in WAVES:  # Process each wave
        waveID = WAVE.waveID

        # Initialize per-wave collections for each model
        model_results_collection = {model: {} for model in models}

        for shift in trend_adjustment_steps:  # Process each shift
            print('Shift:', shift)

            # Initialize per-shift lists for each model
            all_week_model_lists = {model: [] for model in models}

            for week in predictions:  # Process each week
                print('WEEK:', week)

                # Initialize per-week lists for each model
                week_model_lists = {model: [] for model in models}

                for prediction_length in predictions:  # Process each prediction length
                    print('PREDICTION LENGTH:', prediction_length)

                    # Initialize weekly values for each model
                    weekly_model_shift_values = {model: np.nan for model in models}

                    if prediction_length >= week:
                        # Construct the key to fetch metrics
                        wave_shift_prediction_length_key = f'wave {WAVE.waveID} shift {shift} prediction_length {prediction_length}'
                        metrics_row = model_evaluation_dictionary[wave_shift_prediction_length_key]
                        #print(metrics_row)
                        for model_name in models.keys():
                            model_list = metrics_row[model_name]['List']
                            print(model_name, model_list)
                            # Calculate weekly value
                            weekly_model_shift_values[model_name] = (
                                np.array(model_list)[week-1] / prediction_length * 100
                            )

                    # Append weekly values to the week lists for each model
                    for model in models:
                        week_model_lists[model].append(weekly_model_shift_values[model])

                # Append week lists to the all-week lists for each model
                for model in models:
                    all_week_model_lists[model].append(week_model_lists[model])

            # Create dataframes for all-week lists and store them in the shift collections
            for model in models:
                model_results_collection[model][shift] = pd.DataFrame(
                    [all_week_model_lists[model]], columns=weeks_column
                )

        # Store shift collections in the wave collections
        for model in models:

            model_results_wave_collection[model][waveID] = model_results_collection[model]

    return model_results_wave_collection

def find_prediction_current_week(trained_models, current_week_data, current_week_data_ml, lagged_amount, confidence_interval=0.05):
    """
    Generates predictions for the current week for a specific shift and prediction length.

    Parameters:
    - trained_models (dict): Dictionary containing trained models for a specific shift and prediction length.
    - current_week_data (DataFrame): Current week's observed data.
    - current_week_data_ml (DataFrame): ML feature dataset for the current week.
    - lagged_amount (int): Number of lagged features to use.
    - confidence_interval (float): Confidence interval for ARIMA predictions.

    Returns:
    - DataFrame: Predictions for the given shift and prediction length.
    """
    current_week_features, data_index = generate_ml_features(current_week_data_ml[['weekcase']], lagged_amount)
    current_week_features = current_week_features.drop(columns=['weekcase'])

    current_week_prediction = {}

    # Ensure ARIMA is always predicted first
    if 'ARIMA' in trained_models:
        ARMAmodel = trained_models['ARIMA']
        ARMAmodel = ARMAmodel.fit()  # Fit the trained model
        arima_pred = ARMAmodel.get_forecast(current_week_data.shape[0])  # Forecast

        arima_pred_df = arima_pred.conf_int(alpha=confidence_interval)
        arima_pred_df["Predictions"] = ARMAmodel.predict(start=arima_pred_df.index[0], end=arima_pred_df.index[-1])

        arima_pred_df.index = current_week_data.index
        current_week_prediction['ARIMA'] = arima_pred_df["Predictions"]
        #print(f"ARIMA Prediction: {current_week_prediction['ARIMA']}")

    # Predict using all available ML models
    for model_name, model in trained_models.items():
        if model_name == 'ARIMA':
            continue  # Skip ARIMA as it's already handled

        ml_pred = model.predict(current_week_features)
        current_week_prediction[model_name] = ml_pred

    # Convert predictions to DataFrame
    current_week_prediction_df = pd.DataFrame([current_week_prediction])

    # Insert metadata columns
    current_week_prediction_df.insert(0, 'Week', current_week_data['Week'].values[0])
    current_week_prediction_df.insert(1, 'Observed', current_week_data['weekcase'].values[0])

    return current_week_prediction_df

def wave_prediction_with_models(shift_type, wave_start_shift_matrix, predictions, prediction_length, shift, WAVE, waveID,
                    training_period, lag_reserve, lagged_amount, overlap, confidence_interval, tuned_model_dict, models):
    """
    Predicts weekly cases for multiple models dynamically and returns trained models.

    Args:
        shift_type: Flag for shift definition. True=Yushu, False=JMM.
        predictions: List of prediction steps.
        prediction_length: Length of the prediction.
        shift: shift in weeks.
        WAVE: Wave data object.
        waveID: Identifier for the wave.
        training_period: Training period length.
        lag_reserve: Reserve for lag data.
        lagged_amount: How much past data is used.
        tuned_model_dict: Dictionary of tuned parameters for all models.
        models: Dictionary of models with their initialization functions.

    Returns:
        metric_row: Dictionary containing metrics and predictions for all models.
        trained_models: Dictionary containing trained models.
    """
    #print("wave_prediction_with_models: Prediction Length:", prediction_length, "shift:", shift, "Wave:", waveID, "shift_type: ", shift_type)

    # Generate wave, train, and test data with shift
    #print("WAVE.df.", WAVE.df.tail())
    wave, train, test = WAVE.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)

    #print("train", train)
    #print("test", test)
    single_train, full_train, fit_weeks_stat = stats_data(train)
    single_test, full_test, pred_weeks_stat = stats_data(test.head(prediction_length))
    # if (shift == 4 and prediction_length == 1 and waveID == 1):
    #   print("tarin", train.tail())
    #   print("test", test.tail())
    #   print("single_train", single_train)
    #   print("single_test", single_test)
    fit_pred_weeks_stat = pd.concat([fit_weeks_stat.tail(training_period), pred_weeks_stat], axis=0).reset_index(drop=True)
    fit_pred_y_STAT = pd.concat([single_train[-training_period:], single_test], axis=0).reset_index(drop=True)
    #
    # if (shift == 4 and prediction_length == 1 and waveID == 1):
    #   print("fit_pred_weeks_stat", fit_pred_weeks_stat)
    #print("fit_pred_y_STAT", fit_pred_y_STAT)
    # Output containers
    metric_row = {}
    trained_models = {}  # Store trained models

    #### ARIMA Model
    if 'ARIMA' in models:
        arima_params = tuned_model_dict['ARIMA']

        ARMAmodel = SARIMAX(single_train,
                            order=arima_params['order'],
                            seasonal_order=arima_params['seasonal_order'])
        arima_mse, arima_rmse, arima_mae, arima_mape, arima_list, arima, arimap, arimaf = ARIMA_model(
            ARMAmodel, single_test, training_period + prediction_length, confidence_interval
        )

        # Store trained ARIMA model

        trained_models['ARIMA'] = ARMAmodel

        metric_row['ARIMA'] = {
            'MSE': arima_mse,
            'RMSE': arima_rmse,
            'MAE': arima_mae,
            'MAPE': arima_mape,
            'Predictions': arimap.reset_index(drop=True),
            'Fit': arimaf.reset_index(drop=True),
            'List': arima_list,
            'Weeks': fit_pred_weeks_stat,
            'Observed': fit_pred_y_STAT,
            'Fit_Pred': arima.reset_index(drop=True)
        }

    #### Generate ML Data
    adjustment = shift if shift_type == 'Delay' else 0
    train_x, test_x, train_y, test_y, fit_weeks_ml, pred_weeks_ml = gen_ml(
        wave, prediction_length, lagged_amount, prediction_length, predictions, False, adjustment
    )


    fit_pred_weeks_ml = pd.concat([fit_weeks_ml.tail(training_period), pred_weeks_ml], axis=0).reset_index(drop=True)
    fit_pred_y_ML = pd.concat([train_y[-training_period:], test_y], axis=0).reset_index(drop=True)


    # if (shift == 4 and prediction_length == 1 and waveID == 1):
    #   print("fit_pred_weeks_ml", fit_pred_weeks_ml)
    #print("fit_pred_y_ML", fit_pred_y_ML)
    #### Machine Learning Models
    for model_name, model_class in models.items():
        if model_name == 'ARIMA':
            continue  # ARIMA is handled separately

        # Extract parameters for the current model
        tuned_params = tuned_model_dict.get(model_name, {})
        #print(f"Training {model_name} with Parameters:", tuned_params)

        # Initialize and fit the model
        model = model_class(**tuned_params)
        model.fit(train_x, train_y)

        # Store trained model

        trained_models[model_name] = model

        # Predictions and metrics
        predictions = model.predict(test_x)
        mse = mean_squared_error(test_y, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(test_y, predictions)
        mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100
        list_mape = np.abs((test_y - predictions) / test_y)

        fit = model.predict(train_x)
        #fit = fit.reset_index(drop = True)
        fit = pd.DataFrame(fit).reset_index(drop = True).tail(training_period)
        fit = fit.reset_index(drop=True)

        predictions = pd.DataFrame(predictions).reset_index(drop = True)

        fit_pred = pd.concat([fit,predictions],axis=0)

        fit_pred = fit_pred.reset_index(drop=True)

        metric_row[model_name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'Predictions': predictions,
            'Fit': fit,
            'List': list_mape,
            'Weeks': fit_pred_weeks_ml,
            'Observed': fit_pred_y_ML,
            'Fit_Pred': fit_pred
        }
    # if (shift == 4 and prediction_length == 1 and waveID == 1):
    #   print("metric_row", metric_row)
    return metric_row, trained_models  # Now also returns trained models

def model_eval_on_future(
    WAVE_history, WAVES_future, model_update, weight_update, models, observed_column_name,
    predictions, tuned_model_names, tuned_model_dict, wave_start_shift_matrix, training_period, lag_reserve,
    lagged_amount, overlap, confidence_interval, inFoldername_pre, filename):

    model_evaluation_dictionary = {}
    trained_models_dictionary = {}
    weekly_prediction_future = {}

    waveID_history = WAVE_history.waveID
    WAVE_history = copy.deepcopy(WAVE_history)

    for wave in WAVES_future:
        waveID_future = wave.waveID
        wave_df = wave.get_wave_df()

        for index, row in wave_df.iterrows():
            week = pd.to_datetime(index).strftime('%Y-%m-%d')
            row_df = pd.DataFrame([row], index=[index])
            wave_history_df = WAVE_history.get_wave_df()
            # Create a DataFrame with Week and weekcase
            current_week_data = pd.DataFrame({'Week': [week], 'weekcase': [row['weekcase']]})
            #print('current_week_data....', current_week_data)

            # Get the previous 'lagged_amount' rows
            lagged_data = wave_history_df.loc[:index].tail(lagged_amount+1)  # Ensure we get the latest 'lagged_amount' rows
            #print('lagged_data....', lagged_data)
            lagged_data = lagged_data[~lagged_data.index.duplicated(keep='first')]  # Remove duplicate indices

            #print('lagged_data....', lagged_data)
            if week in lagged_data.index:
                #print(week, 'in lagged data week')
                continue
            if index in lagged_data.index:
                #print(index, 'in lagged data index')
                continue
            if len(lagged_data) < lagged_amount+1:#what I can do is, start from history wave to get the ML data
                print(f"Not enough historical data for ML predictions at {week}. Required: {lagged_amount}, Available: {len(lagged_data)}")
                current_week_data_ml = None  # ML predictions can't be done
                WAVE_history.add_one_week(row_df)
                continue
            else:
                # Include lagged data in the new DataFrame
                current_week_data_ml = lagged_data.copy()

                current_week_data_ml = current_week_data_ml[['weekcase']]  # Keep only relevant columns

                #current_week_data_ml = current_week_data_ml.rename(columns={'week': 'Week'})
            #print('current_week_data_ml....', current_week_data_ml)
            trend_adjustment_steps = [0]
            shift_type = 'Delay'

            for prediction_length in predictions:
                wave_shift_tuned_params = {}

                for shift in trend_adjustment_steps:  # Iterate over all shift values
                    for tuned_model_name in tuned_model_names:
                        wave_shift_model_key = f'wave {waveID_history} shift {shift}{tuned_model_name}'
                        wave_shift_tuned_params[tuned_model_name] = tuned_model_dict[wave_shift_model_key]

                    # Perform wave prediction

                    metric_row, trained_models = wave_prediction_with_models(
                        shift_type, wave_start_shift_matrix, predictions,
                        prediction_length, shift, WAVE_history, waveID_history,
                        training_period, lag_reserve, lagged_amount, overlap,
                        confidence_interval, wave_shift_tuned_params, models)

                    wave_shift_prediction_length_key = f'wave {waveID_history} shift {shift} prediction_length {prediction_length}'

                    # Store evaluation metrics and trained models
                    model_evaluation_dictionary[wave_shift_prediction_length_key] = metric_row
                    trained_models_dictionary[wave_shift_prediction_length_key] = trained_models

            base_columns = ['Wave', 'Shift', 'Length', 'Observed']
            visualization_columns = base_columns + list(models.keys())
            #derive weights for history data
            task_name = 'weekly'
            concise_data, concise_data_dictionary = create_prediction_df(
                [WAVE_history], predictions, trend_adjustment_steps, models, model_evaluation_dictionary,
                inFoldername_pre, task_name, filename)
            #WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename
            # print('concise done')
            #print('concise_data....\n', concise_data.dtypes)
            diff_data = find_diff(concise_data, models, observed_column_name)
            # print('diffdata done')
            # print(diff_data.dtypes)
            stan_optimal_weights = find_stan_optimal_weights(
                diff_data, predictions, trend_adjustment_steps, observed_column_name, models)
            # print('stan_optimal_weights')
            # Predict the current week's data
            all_predictions = []  # List to store individual DataFrames
            current_week_predictions = {}  # Dictionary to store predictions
            #weekly_prediction_future = {}  # Dictionary to store predictions

            for prediction_length in predictions:
                for shift in trend_adjustment_steps:
                    wave_shift_prediction_length_key = f'wave {waveID_history} shift {shift} prediction_length {prediction_length}'
                    print(wave_shift_prediction_length_key)

                    if wave_shift_prediction_length_key not in trained_models_dictionary:
                        print(f"Warning: No trained models found for {wave_shift_prediction_length_key}")
                        continue

                    trained_models = trained_models_dictionary[wave_shift_prediction_length_key]
                    #here i update the the current_week_data, and current_week_data_ml as well

                    future_weeks = wave_df.loc[index:].head(prediction_length)  # Selects the next n weeks

                    future_weeks_df = future_weeks.reset_index()[['week', 'weekcase']].rename(columns={'week': 'Week'})
                    current_prediction_week = future_weeks_df.tail(1)
                    #print('current_prediction_week', current_prediction_week)
                    # Append future weeks to current_week_data
                    #current_week_data = pd.concat([row_df, future_weeks_df], ignore_index=True)
                    # Call function to get predictions
                    current_week_prediction_df = find_prediction_current_week(
                        trained_models, current_prediction_week, current_week_data_ml, lagged_amount
                    )

                    # Add metadata
                    current_week_prediction_df.insert(2, 'Wave', waveID_history)
                    current_week_prediction_df.insert(3, 'Shift', shift)
                    current_week_prediction_df.insert(4, 'Length', prediction_length)

                    # print(f"Current Week Prediction for Wave {waveID_history}, Shift {shift}, Length {prediction_length}:")
                    # print(current_week_prediction_df)

                    # Store results
                    all_predictions.append(current_week_prediction_df)
                    #weekly_prediction_future[(shift, prediction_length)] = current_week_prediction_df  # Save in dictionary

            # Concatenate all results into a single DataFrame
            if all_predictions:
                current_week_predictions = pd.concat(all_predictions, ignore_index=True)
            else:
                current_week_predictions = pd.DataFrame()

            diff_data_future = find_diff(current_week_predictions, models, observed_column_name)
            # print('diff_data_future do e')

            #print(stan_optimal_weights)
            theDD, theD_long, current_week_prediction_MLAMA_df = find_weighted_predictions(
                diff_data_future, stan_optimal_weights, observed_column_name, models)
            #print('current_week_prediction_MLAMA_df')
            # print(current_week_prediction_MLAMA_df)
            # Add `prediction_length` and `shift` columns
            #I am here only doing one week prediction no shift in future
            # current_week_prediction_MLAMA_df['Length'] = prediction_length_future
            # current_week_prediction_MLAMA_df['Shift'] = shift_future

            # Store the result in the dictionary
            weekly_prediction_future[week] = current_week_prediction_MLAMA_df#for this week all shift, length should be here
            # print('WAVE_history.df before adding one week')
            # print(WAVE_history.df.tail())
            # Append current week's data to historical WAVE data
            WAVE_history.add_one_week(row_df)
            #but now I need to update the dates as well, wave end date, train start date, train end date, test start date, test end date all.
            # print('WAVE_history.df')
            # print(WAVE_history.df.tail())
            #print('weekly_prediction_future', weekly_prediction_future.keys())

    return weekly_prediction_future