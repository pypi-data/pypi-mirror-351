# -*- coding: utf-8 -*-
"""GridSearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVqTLEJnqhQUIeFdt23d1GtBEOgo51AN
"""

from .Wave import Wave
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, arima_res_xgb, check_stationarity
from .Processor import TimeSeriesSplit

# Commented out IPython magic to ensure Python compatibility.
### Import Library
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV#TimeSeriesSplit,
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
# %matplotlib inline
pd.set_option('display.max_rows', None)
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import TimeSeriesSplit
from tqdm import tqdm
import datetime
import os.path
from os import path
from pmdarima.arima import auto_arima
import os

import json

#grid search.py
def get_wave_data_dictionary(WAVES, trend_adjustment_steps, wave_start_shift_matrix, shift_type, n_splits, test_size):
    """
    Constructs a dictionary containing wave data with various shift values.

    Parameters:
        WAVES (list): A list of wave objects, each with a method `get_wave_dates_with_shift`.
        trend_adjustment_steps (list): A list of shift values to apply to each wave.
        wave_start_shift_matrix(dict): A dictionary mapping wave IDs to lists of shifted start values.
        shift_type: The definition or object specifying the shift settings for each wave.

    Returns:
        dict: A dictionary with keys in the format 'wave {waveID} shift {shift}', containing data for each wave and shift.
    """
    data_dict = {}

    # Loop through each wave and shift to populate the dictionary
    for wave in WAVES:
        for shift in trend_adjustment_steps:
            # Get wave data, train data, and test data for each shift using get_wave_dates_with_shift
            wave_data, train_data, test_data = wave.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)

            # Construct the key for each combination of wave and shift
            key = f'wave {wave.waveID} n{n_splits}_t{test_size}_shift {shift}'

            # Populate the dictionary with the data for each wave and shift
            data_dict[key] = {
                'all': wave_data,
                'train': train_data,
                'test': test_data
            }

    return data_dict

# def run_auto_arima_for_each_key(
#     data_dict,
#     output_folder,
#     tuned_on_filename,
#     model_name='ARIMA',
#     skip_existing_files=False,
#     output_filename_template='{key}_{model_name}_results_{tuned_on_filename}.json'
# ):
#     """
#     Run the auto-ARIMA model for each dataset in the provided dictionary
#     and save the ARIMA parameters to JSON files.

#     Parameters:
#         data_dict (dict): Dictionary containing training and testing data for each dataset.
#         output_folder (str): Folder path where the JSON files will be saved.
#         tuned_on_filename (str): Filename for the data the models are tuned on.
#         model_name (str): Name of the model (default: 'ARIMA').
#         skip_existing_files (bool): Whether to skip datasets with existing results (default: False).
#         output_filename_template (str): Template for the output filenames.

#     Returns:
#         dict: A dictionary containing the ARIMA parameters for each dataset.
#     """
#     # Ensure the output folder exists
#     os.makedirs(output_folder, exist_ok=True)

#     arima_results = {}

#     for key, data in tqdm(data_dict.items(), desc="Running auto-ARIMA"):
#         print(f"Processing dataset: {key}")

#         # Define file paths
#         output_filename = output_filename_template.format(
#             key=key, model_name=model_name, tuned_on_filename=tuned_on_filename
#         )
#         result_path_all = os.path.join(output_folder, output_filename)
#         result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')

#         # Skip if files already exist and skipping is enabled
#         if skip_existing_files and os.path.exists(result_path_best) and os.path.exists(result_path_all):
#             print(f"Skipping {key}: results already exist at {result_path_best}")
#             with open(result_path_best, "r") as file:
#                 arima_results[key] = json.load(file)
#             continue

#         # Prepare the training data
#         arima_train = pd.DataFrame(data['all']['weekcase'])

#         # Fit the auto-ARIMA model
#         print(f"Fitting auto-ARIMA for dataset: {key}")
#         model = auto_arima(
#             arima_train,
#             start_p=1, start_q=1, max_p=10, max_q=10,
#             m=52, start_P=0, seasonal=True,
#             d=0, D=0, trace=True,
#             error_action='ignore',
#             suppress_warnings=True,
#             stepwise=True
#         )

#         # Extract model parameters
#         arima_parameters = model.get_params()
#         print(f"ARIMA Parameters for {key}: {arima_parameters}")

#         # Save results to JSON files
#         for path in [result_path_best, result_path_all]:
#             with open(path, "w") as file:
#                 json.dump(arima_parameters, file, indent=4)
#             print(f"Saved ARIMA parameters to: {path}")

#         # Update results dictionary
#         arima_results[key] = arima_parameters

#     return arima_results

# def run_auto_arima_for_each_key(
#     data_dict,
#     output_folder,
#     tuned_on_filename,
#     model_name='ARIMA',
#     skip_existing_files=False,
#     output_filename_template='{key}_{model_name}_results_{tuned_on_filename}.json',
#     auto_arima_params=None
# ):
#     """
#     Run the auto-ARIMA model for each dataset in the provided dictionary
#     and save the ARIMA parameters to JSON files.

#     Parameters:
#         data_dict (dict): Dictionary containing training and testing data for each dataset.
#         output_folder (str): Folder path where the JSON files will be saved.
#         tuned_on_filename (str): Filename for the data the models are tuned on.
#         model_name (str): Name of the model (default: 'ARIMA').
#         skip_existing_files (bool): Whether to skip datasets with existing results (default: False).
#         output_filename_template (str): Template for the output filenames.
#         auto_arima_params (dict): Optional dictionary of parameters to pass to auto_arima.

#     Returns:
#         dict: A dictionary containing the ARIMA parameters for each dataset.
#     """


#     if auto_arima_params is not None:
#         print('Custom auto_arima parameter ', auto_arima_params)
#     else:
#         print('Default auto_arima parameter')

#     os.makedirs(output_folder, exist_ok=True)
#     arima_results = {}

#     for key, data in tqdm(data_dict.items(), desc="Running auto-ARIMA"):
#         print(f"Processing dataset: {key}")

#         output_filename = output_filename_template.format(
#             key=key, model_name=model_name, tuned_on_filename=tuned_on_filename
#         )
#         result_path_all = os.path.join(output_folder, output_filename)
#         result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')

#         if skip_existing_files and os.path.exists(result_path_best) and os.path.exists(result_path_all):
#             print(f"Skipping {key}: results already exist at {result_path_best}")
#             with open(result_path_best, "r") as file:
#                 arima_results[key] = json.load(file)
#                 print(f"ARIMA Parameters for {key}: {arima_results[key]}")
#             continue

#         arima_train = pd.DataFrame(data['all']['weekcase'])

#         print(f"Fitting auto-ARIMA for dataset: {key}")

#         if auto_arima_params is not None:
#             model = auto_arima(arima_train, **auto_arima_params)
#         else:
#             model = auto_arima(arima_train)

#         arima_parameters = model.get_params()
#         print(f"ARIMA Parameters for {key}: {arima_parameters}")

#         for path in [result_path_best, result_path_all]:
#             with open(path, "w") as file:
#                 json.dump(arima_parameters, file, indent=4)
#             print(f"Saved ARIMA parameters to: {path}")

#         arima_results[key] = arima_parameters

#     return arima_results


import os
import json
import pandas as pd
from tqdm import tqdm
from pmdarima import auto_arima

def run_auto_arima_for_each_key(
    data_dict,
    output_folder,
    tuned_on_filename,
    model_name='ARIMA',
    skip_existing_files=False,
    output_filename_template='{key}_{model_name}_results_{tuned_on_filename}.json',
    auto_arima_params=None
):
    """
    Run the auto-ARIMA model for each dataset in the provided dictionary
    and save the ARIMA parameters and user-defined parameters to JSON files.

    Parameters:
        data_dict (dict): Dictionary containing training and testing data for each dataset.
        output_folder (str): Folder path where the JSON files will be saved.
        tuned_on_filename (str): Filename for the data the models are tuned on.
        model_name (str): Name of the model (default: 'ARIMA').
        skip_existing_files (bool): Whether to skip datasets with existing results (default: False).
        output_filename_template (str): Template for the output filenames.
        auto_arima_params (dict): Optional dictionary of parameters to pass to auto_arima.

    Returns:
        dict: A dictionary containing the ARIMA parameters for each dataset.
    """
    if auto_arima_params is not None:
        print('Custom auto_arima parameter:', auto_arima_params)
    else:
        print('Default auto_arima parameter')

    os.makedirs(output_folder, exist_ok=True)
    arima_results = {}

    for key, data in tqdm(data_dict.items(), desc="Running auto-ARIMA"):
        print(f"Processing dataset: {key}")

        output_filename = output_filename_template.format(
            key=key, model_name=model_name, tuned_on_filename=tuned_on_filename
        )
        result_path_all = os.path.join(output_folder, output_filename)
        result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')

        if skip_existing_files and os.path.exists(result_path_best) and os.path.exists(result_path_all):
            print(f"Skipping {key}: results already exist at {result_path_best}")
            with open(result_path_best, "r") as file:
                arima_results[key] = json.load(file)
                print(f"ARIMA Parameters for {key}: {arima_results[key]}")
            continue

        #changed all to train
        arima_train = pd.DataFrame(data['train']['weekcase'])

        print(f"Fitting auto-ARIMA for dataset: {key}")
        if auto_arima_params is not None:
            model = auto_arima(arima_train, **auto_arima_params)
        else:
            model = auto_arima(arima_train)

        arima_parameters = model.get_params()
        print(f"ARIMA Parameters for {key}: {arima_parameters}")

        result_dict = {
            "output_params": arima_parameters,
            "input_params": auto_arima_params if auto_arima_params is not None else "default"
        }

        for path in [result_path_best, result_path_all]:
            with open(path, "w") as file:
                json.dump(result_dict, file, indent=4)
            print(f"Saved ARIMA result (input & output parameters) to: {path}")

        arima_results[key] = result_dict

    return arima_results



#changed all to train
def stat_train(data):
  return pd.DataFrame(data['train'].weekcase)

from sklearn.model_selection import GridSearchCV
import os
import json

import joblib

# def run_grid_search(
#     model,
#     cv,
#     param_grid,
#     lagged_amount,
#     key,
#     data,
#     output_folder,
#     output_filename,
#     scoring='neg_mean_absolute_percentage_error',
#     use_random_state=None,
#     custom_steps=None,
#     skip_existing_files = True
# ):
#     """
#     Generalized function for grid search on any model.

#     Parameters:
#         model: Machine learning model instance.
#         cv: Cross-validation object.
#         param_grid: Dictionary of hyperparameters for grid search.
#         lagged_amount: Number of lagged points to include in the dataset.
#         data_dict: Dictionary containing training and testing data splits (train_X, train_y, test_X, test_y).
#         output_folder: Folder to save results.
#         output_filename: Filename for saving grid search results.
#         scoring: Scoring metric for grid search (default is 'accuracy').
#         use_random_state: Optional; Boolean to control whether random state should be used (for specific models).
#         custom_steps: Optional; Function for any custom preprocessing or steps specific to the model.

#     Returns:
#         best_model: Trained model with the best parameters from the grid search.
#         best_params: Dictionary of the best hyperparameters.
#     """

#     #for key, data in tqdm(data_dict.items(), desc='Dataset'):

#     # Generate training and testing datasets for ML

#     print(data['all'].shape)

#     os.makedirs(output_folder, exist_ok=True)
#     result_path_all = os.path.join(output_folder, output_filename)
#     result_path_best = os.path.join(output_folder, 'best_params_'+output_filename)
#     model_filename = os.path.join(output_folder, 'model_'+output_filename)


#     # Check if the parameters are already saved
#     if skip_existing_files:
#          if os.path.exists(result_path_best) and os.path.exists(model_filename) and os.path.exists(result_path_all):
#             print(f"{result_path_best}: already DONE")
#             with open(result_path_best, "r") as file:
#                 best_params = json.load(file)
#             best_model = joblib.load(model_filename)
#             return best_model, best_params

#     train_x, test_x, train_y, test_y, train_index, test_index = gen_ml(pd.DataFrame(data['all'].weekcase),1, 1, lagged_amount, [1], resid = False)#gen_ml(pd.DataFrame(data['all'].weekcase), 1, 5, resid=False)

#     # Unpack the data dictionary
#     #train_X, train_y = data_dict["train_X"], data_dict["train_y"]
#     #test_X, test_y = data_dict["test_X"], data_dict["test_y"]

#     # Apply any custom preprocessing or steps
#     if custom_steps:
#         custom_steps(model, train_x, train_y)

#     # Adjust parameters for models that need a random state
#     if use_random_state is not None and hasattr(model, "random_state"):
#         model.random_state = 42 if use_random_state else None

#     # Perform grid search
#     grid_search = GridSearchCV(
#         estimator=model,
#         param_grid=param_grid,
#         scoring=scoring,
#         cv=cv,#5,  # Change as per your requirement
#         n_jobs=-1,  # Use all available processors
#         error_score='raise',
#         verbose=2
#     )
#     grid_search.fit(train_x, train_y)
#     grid_search_cv_results = pd.DataFrame(grid_search.cv_results_)#.to_csv('grid_search_results.csv', index=False)


#     # Print JSON string for inspection
#     #json_results = grid_search_cv_results.to_json(orient="records", indent=4)  # Pretty-print JSON
#     #print(json_results)

#     #print(grid_search_cv_results)
#     #grid_search_best_params = pd.DataFrame(grid_search.best_params_)
#     #grid_search_best_params['key']=key
#     #print(grid_search_best_params)

#     #print(grid_search_cv_results)

#     # Save results to a file

#     grid_search_cv_results.to_json(result_path_all, orient="records", indent=4)  # Saves as a JSON file

#     #grid_search_cv_results.to_csv(result_path_all+'.csv', index=False)
#     #grid_search_best_params.to_csv(result_path+'_best_params.csv', index=False)
#     best_params = grid_search.best_params_
#     with open(result_path_best, "w") as file:
#         json.dump(best_params, file)
#         print('grid_search_best_params write done.', result_path_best)
#     #with open(result_path, "w") as file:
#     #    json.dump(grid_search.cv_results_, file)


#     print(f"Grid search results saved to: {result_path_all}")

#     # Extract and return the best model and hyperparameters
#     best_model = grid_search.best_estimator_
#     joblib.dump(best_model, model_filename)
#     print(f"Best model saved to: {model_filename}")

#     return best_model, best_params


import os
import json
import joblib
import pandas as pd
from sklearn.model_selection import GridSearchCV

def run_grid_search(
    model,
    cv,
    param_grid,
    lagged_amount,
    key,
    data,
    output_folder,
    output_filename,
    scoring='neg_mean_absolute_percentage_error',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files=True
):
    """
    Generalized function for grid search on any model.

    Parameters:
        model: Machine learning model instance.
        cv: Cross-validation object.
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        key: Unique identifier for the current search.
        data: Dictionary containing training and testing data splits.
        output_folder: Folder to save results.
        output_filename: Filename for saving grid search results.
        scoring: Scoring metric for grid search.
        use_random_state: Boolean; if True, sets model.random_state = 42.
        custom_steps: Optional preprocessing function.
        skip_existing_files: If True, skip if output already exists.

    Returns:
        best_model: Trained model with the best parameters from the grid search.
        best_params: Dictionary of the best hyperparameters.
    """
    os.makedirs(output_folder, exist_ok=True)
    result_path_all = os.path.join(output_folder, output_filename)
    result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')
    model_filename = os.path.join(output_folder, f'model_{output_filename}')

    # Check if already done
    if skip_existing_files:
        if os.path.exists(result_path_best) and os.path.exists(model_filename) and os.path.exists(result_path_all):
            print(f"{result_path_best}: already DONE")
            with open(result_path_best, "r") as file:
              best_params = json.load(file)
              if "input_params" in best_params and "output_params" in best_params:
                  best_info = json.load(file)
                  best_params = best_info["output_params"]
            best_model = joblib.load(model_filename)
            return best_model, best_params

    #print(data['all'].shape)

    # Generate train/test
    #changed 'all' to 'train'
    train_x, test_x, train_y, test_y, train_index, test_index = gen_ml(
        pd.DataFrame(data['train'].weekcase), 1, 1, lagged_amount, [1], resid=False
    )#data['train'] korte hobe

    if custom_steps:
        custom_steps(model, train_x, train_y)

    if use_random_state is not None and hasattr(model, "random_state"):
        model.random_state = 42 if use_random_state else None

    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring,
        cv=cv,
        n_jobs=-1,
        error_score='raise',
        verbose=2
    )
    grid_search.fit(train_x, train_y)

    # Save all CV results
    pd.DataFrame(grid_search.cv_results_).to_json(result_path_all, orient="records", indent=4)

    # Save best parameters with input param_grid as JSON
    best_params = grid_search.best_params_
    best_param_info = {
        "input_params": param_grid,
        "output_params": best_params
    }
    with open(result_path_best, "w") as file:
        json.dump(best_param_info, file, indent=4)
        print(f"Best parameters saved to: {result_path_best}")

    # Save model
    best_model = grid_search.best_estimator_
    joblib.dump(best_model, model_filename)
    print(f"Best model saved to: {model_filename}")

    return best_model, best_params


def run_grid_search_for_each_key(
    model_class,
    model_name,
    cv,
    param_grid,
    lagged_amount,
    data_dict,
    output_folder,
    tuned_on_filename,
    output_filename_template = '{key}_{model_name}_results_{tuned_on_filename}.json',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files = True
):
    """
    Generalized function to perform grid search for each key in a data dictionary.

    Parameters:
        model_class: Class of the machine learning model (e.g., XGBClassifier, RandomForestClassifier).
        cv: Cross-validation object (e.g., TimeSeriesSplit).
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        data_dict: Dictionary containing data for each key.
        output_folder: Folder to save the grid search results.
        tuned_on_filename: Filename for the data the models are tuned on.
        output_filename_template: Template for the output filename, should include a placeholder for `key`.
        use_random_state: Optional; Boolean to control whether random state should be used (for specific models).
        custom_steps: Optional; Function for any custom preprocessing or steps specific to the model.

    Returns:
        best_models: Dictionary containing the best model for each key.
        best_params: Dictionary containing the best parameters for each key.
    """
    best_models = {}
    best_params = {}

    for key in data_dict.keys():
        print(f"Running grid search for key: {key}")

        best_model, best_params_for_key = run_grid_search(
            model=model_class(),
            cv = cv,
            param_grid=param_grid,
            lagged_amount = lagged_amount,
            key = key,
            data=data_dict[key],  # Process one key at a time
            output_folder=output_folder,
            output_filename=output_filename_template.format(key=key, model_name = model_name, tuned_on_filename = tuned_on_filename),
            use_random_state=use_random_state,
            custom_steps=custom_steps,
            skip_existing_files = skip_existing_files
        )

        best_models[key] = best_model
        best_params[key] = best_params_for_key
        print(f"Key: {key} - Best Model:", best_model)
        print(f"Key: {key} - Best Parameters:", best_params_for_key)

    return best_models, best_params