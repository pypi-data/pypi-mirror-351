{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475d292c",
   "metadata": {},
   "source": [
    "# Special Case: Impact of Model Complexity on FGC\n",
    "\n",
    "When training a Random Forest model, we usually tune our models wrt. hyperparameters by optimizing a specified scoring function, e.g. R^2 or accuracy. When only optimizing for a metric, we might end up with a highly complex Random Forest model, which has deeply grown trees to better fit the data at hand. When the model gets too complex, it can start to learn irrelevant information (“noise”) within the dataset and we run into the problem of overfitting. When this happens, the algorithm unfortunately cannot perform accurately on unseen data, defeating its purpose. This problem also propagates into the generalization of the explanations we retrieve from FGC. FGC allows us to uncover the stable patterns in the data using the structure of a Random Forest model. However, if the model becomes too complex, e.g. has deeply grown trees, it starts learning patterns that are specific to certain instances in the training set, rather than learning generalizeable patterns. \n",
    "\n",
    "**Note:** for installation description and a general introduction to FGC please have a look at [Read the Docs - Installation](https://forest-guided-clustering.readthedocs.io/en/latest/_getting_started/installation.html) and [Introduction Notebook](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/introduction_to_FGC_use_cases.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the Forest-Guided Clustering package\n",
    "from fgclustering import FgClustering\n",
    "\n",
    "## Imports for datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "## Additional imports for use-cases\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase how model complexity impacts FGC, we will again use the California Housing dataset (for dataset description, please see *Use Case 3*). We will use the first 1000 samples of the dataset as training data to train a Random Forest Regressor, where we tune the *max_depth* with 5-fold corss-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3159e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Grid: [{'max_depth': 2}, {'max_depth': 5}, {'max_depth': 10}, {'max_depth': 20}, {'max_depth': 30}]\n",
      "Test R^2 score: [0.21068248 0.43973832 0.510726   0.51409796 0.5153817 ]\n",
      "Parameters of best prediction model: {'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "data_housing = fetch_california_housing(as_frame=True).frame\n",
    "\n",
    "data_housing_train = data_housing.iloc[:1000]\n",
    "X_housing_train = data_housing_train.loc[:, data_housing_train.columns != 'MedHouseVal']\n",
    "y_housing_train = data_housing_train.MedHouseVal\n",
    "\n",
    "regressor = RandomForestRegressor(max_features='log2', max_samples=0.8, bootstrap=True, oob_score=True, random_state=42)\n",
    "\n",
    "grid = {'max_depth':[2, 5, 10, 20, 30]}\n",
    "grid_regressor = GridSearchCV(regressor, grid, cv=5)\n",
    "grid_regressor.fit(X_housing_train, y_housing_train)\n",
    "rf_housing = grid_regressor.best_estimator_\n",
    "\n",
    "params = grid_regressor.cv_results_['params']\n",
    "score = grid_regressor.cv_results_['mean_test_score']\n",
    "print(f'Parameter Grid: {params}')\n",
    "print(f'Test R^2 score: {score}')\n",
    "\n",
    "print(f'Parameters of best prediction model: {grid_regressor.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e3c57",
   "metadata": {},
   "source": [
    "The results above show that optimizing only for metrics results in a highly complex model with maximum tree depth of 30, although the performance metric does not change from *max_depth=10* upwards. We now apply Forest-Guided Clustering on the trained model and dataset, to see if we retrieve any stable pattern from this Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b268470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:05,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 2 the mean Jaccard Index across clusters is 0.4677641754581318\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:04<00:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 3 the mean Jaccard Index across clusters is 0.288179587085049\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 4 the mean Jaccard Index across clusters is 0.22673861539429546\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 5 the mean Jaccard Index across clusters is 0.1905951724855571\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/lisasousa/Desktop/fg-clustering/fgclustering/forest_guided_clustering.py:149: UserWarning: No stable clusters were found!\n",
      "  warnings.warn(\"No stable clusters were found!\")\n"
     ]
    }
   ],
   "source": [
    "fgc = FgClustering(model=rf_housing, data=data_housing_train, target_column='MedHouseVal')\n",
    "fgc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5643c",
   "metadata": {},
   "source": [
    "As we can see from the results above, FGC does not find any stable clustering, which means that we do not find any generalizeable pattern in the data. But how is that possible, given that in Use Case 3 we use the same data (to train the model / run FGC) and find clear and stable patterns using FGC? The reason is that we optimized our model only in terms of metric performance and not in terms of explainability! We saw above that optimizing our model in terms of R^2 score, lead to a highly complex model with a maximum tree depth of 30, while the performance is not significantly better than a Random Forest model with a maximum tree depth of 10. A high tree depth leads to trees with many leaves containing only few samples. However, the deeper we go in the tree, the higher the chances that the separation is only based on properties specific to the training samples, i.e. we start fitting the \"noise\" in our training data. Let's now see what happens if we apply FGC to the same Random Forest model trained with *max_depth=10*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffb7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 2 the mean Jaccard Index across clusters is 0.7092479153369771\n",
      "The stability of each cluster is:\n",
      "  Cluster 1: Stability 0.70093\n",
      "  Cluster 2: Stability 0.71757\n",
      "For number of cluster 2 the score is 777.2148166602167\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:05<00:05,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 3 the mean Jaccard Index across clusters is 0.5587828975008259\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:11<00:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 4 the mean Jaccard Index across clusters is 0.5753863521755713\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For number of cluster 5 the mean Jaccard Index across clusters is 0.6543169605341483\n",
      "Clustering is instable, no score computed!\n",
      "\n",
      "\n",
      "Optimal number of cluster is: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(max_depth=10, max_features='log2', max_samples=0.8, bootstrap=True, oob_score=True, random_state=42)\n",
    "regressor.fit(X_housing_train, y_housing_train)\n",
    "\n",
    "fgc = FgClustering(model=regressor, data=data_housing_train, target_column='MedHouseVal')\n",
    "fgc.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b297125",
   "metadata": {},
   "source": [
    "Using a Random Forest model with *max_depth=10*, which achieves equally good performance results as a Random Forest model with *max_depth=30*, indeed finds a stable clustering with *k=2*. This shows that the performance metric should not be the only optimization aim when we train a Random Forest model that we want to interpret!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
