import datetime
from fireworks.control_plane.generated.protos.gateway import BaseModelDetails, BaseModelDetailsCheckpointFormat, Code, ConversationConfig, DeployedModelRef, DeployedModelState, DeploymentPrecision, Model, ModelKind, ModelState, PeftDetails, SkuInfo, Status
from fireworks.control_plane.generated.protos.google.type import Money

models = [
Model(name='accounts/fireworks/models/chronos-hermes-13b-v2', display_name='Chronos Hermes 13B v2', create_time=datetime.datetime(2024, 2, 29, 5, 52, 38, 739683, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='(chronos-13b-v2 + Nous-Hermes-Llama2-13b) 75/25 merge. This offers the imaginative writing style of chronos while still retaining coherency and being capable. Outputs are long and utilize exceptional prose.', hugging_face_url='https://huggingface.co/Austism/chronos-hermes-13b-v2', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), conversation_config=ConversationConfig(style='alpaca'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 0.8999999761581421, 'top_k': 50.0, 'top_p': 0.6000000238418579}),
Model(name='accounts/fireworks/models/codegemma-2b', display_name='CodeGemma 2B', create_time=datetime.datetime(2024, 6, 19, 23, 20, 47, 969349, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models are text-to-text and text-to-code decoder-only models and are available as a 7 billion pretrained variant that specializes in code completion and code generation tasks, a 7 billion parameter instruction-tuned variant for code chat and instruction following and a 2 billion parameter pretrained variant for fast code completion.', hugging_face_url='https://huggingface.co/google/codegemma-2b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'codegemma_nl_benchmarks.png', 'config.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=3030460416, tunable=True, model_type='gemma'), context_length=8192, created_by='xiaoyifan@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/codegemma-7b', display_name='CodeGemma 7B', create_time=datetime.datetime(2024, 6, 20, 5, 57, 22, 203451, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='CodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models are text-to-text and text-to-code decoder-only models and are available as a 7 billion pretrained variant that specializes in code completion and code generation tasks, a 7 billion parameter instruction-tuned variant for code chat and instruction following and a 2 billion parameter pretrained variant for fast code completion.', hugging_face_url='https://huggingface.co/google/codegemma-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'codegemma_nl_benchmarks.png', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=9324112896, tunable=True, model_type='gemma'), context_length=8192, created_by='xiaoyifan@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-13b', display_name='Code Llama 13B', create_time=datetime.datetime(2024, 3, 1, 4, 36, 24, 780704, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the base 13B version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-13b-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), conversation_config=ConversationConfig(style='llama-infill'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-13b-instruct', display_name='Code Llama 13B Instruct', create_time=datetime.datetime(2024, 3, 1, 4, 57, 22, 747291, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 13B instruction-tuned version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), conversation_config=ConversationConfig(style='llama-chat'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-13b-python', display_name='Code Llama 13B Python', create_time=datetime.datetime(2024, 3, 1, 4, 43, 18, 546919, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 13B Python specialist version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-13b-Python-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-34b', display_name='Code Llama 34B', create_time=datetime.datetime(2024, 3, 1, 5, 51, 12, 380040, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 34B base version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-34b-hf', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00007.safetensors', 'model-00002-of-00007.safetensors', 'model-00003-of-00007.safetensors', 'model-00004-of-00007.safetensors', 'model-00005-of-00007.safetensors', 'model-00006-of-00007.safetensors', 'model-00007-of-00007.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000, model_type='llama'), conversation_config=ConversationConfig(style='llama-infill'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/code-llama-34b-instruct', display_name='Code Llama 34B Instruct', create_time=datetime.datetime(2024, 3, 1, 17, 1, 41, 983465, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 34B instruction-tuned version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00007.safetensors', 'model-00002-of-00007.safetensors', 'model-00003-of-00007.safetensors', 'model-00004-of-00007.safetensors', 'model-00005-of-00007.safetensors', 'model-00006-of-00007.safetensors', 'model-00007-of-00007.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000, model_type='llama'), conversation_config=ConversationConfig(style='llama-chat'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-34b-python', display_name='Code Llama 34B Python', create_time=datetime.datetime(2024, 3, 1, 5, 48, 28, 989748, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 34B Python specialist version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-34b-Python-hf', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'CODE_OF_CONDUCT.md', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00007.safetensors', 'model-00002-of-00007.safetensors', 'model-00003-of-00007.safetensors', 'model-00004-of-00007.safetensors', 'model-00005-of-00007.safetensors', 'model-00006-of-00007.safetensors', 'model-00007-of-00007.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/code-llama-70b', display_name='Code Llama 70B', create_time=datetime.datetime(2024, 3, 1, 16, 42, 36, 428402, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 70B Base version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-70b-hf', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000), context_length=16384, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-70b-instruct', display_name='Code Llama 70B Instruct', create_time=datetime.datetime(2024, 3, 1, 19, 45, 24, 496630, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 70B instruction-tuned version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000), conversation_config=ConversationConfig(style='codellama-70b-instruct'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-70b-python', display_name='Code Llama 70B Python', create_time=datetime.datetime(2024, 3, 1, 21, 0, 51, 169055, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 70B Python specialist version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-70b-Python-hf', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-7b', display_name='Code Llama 7B', create_time=datetime.datetime(2024, 3, 1, 4, 10, 16, 878352, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the base 7B version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-7b-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), conversation_config=ConversationConfig(style='llama-infill'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-7b-instruct', display_name='Code Llama 7B Instruct', create_time=datetime.datetime(2024, 3, 1, 4, 15, 22, 317896, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 7B instruction-tuned version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'pytorch_model-00002-of-00003.bin', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), conversation_config=ConversationConfig(style='llama-chat'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/code-llama-7b-python', display_name='Code Llama 7B Python', create_time=datetime.datetime(2024, 3, 1, 4, 6, 45, 245036, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Code Llama is a collection of pretrained and fine-tuned Large Language Models ranging in scale from 7 billion to 70 billion parameters, specializing in using both code and natural language prompts to generate code and natural language about code. This is the 7B Python specialist version.', github_url='https://github.com/facebookresearch/codellama', hugging_face_url='https://huggingface.co/codellama/CodeLlama-13b-Python-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/code-qwen-1p5-7b', display_name='CodeQwen 1.5 7B', create_time=datetime.datetime(2024, 6, 19, 0, 22, 9, 885143, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='CodeQwen1.5 is based on Qwen1.5, a language model series including decoder language models of different model sizes. It is trained on 3 trillion tokens of data of codes, and it includes group query attention (GQA) for efficient inference.', hugging_face_url='https://huggingface.co/Qwen/CodeQwen1.5-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7250284544, tunable=True, model_type='qwen2'), context_length=65536, created_by='xiaoyifan@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 24, 991502, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/cogito-v1-preview-llama-3b', display_name='Cogito v1 Preview Llama 3B', create_time=datetime.datetime(2025, 4, 7, 20, 3, 24, 809572, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Cogito LLMs are instruction tuned generative models that is also a hybrid reasoning model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=3606752256, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n{#- Set the system message. If enable_thinking is true, add the "Enable deep thinking subroutine." #}\n{%- if enable_thinking %}\n    {%- if system_message != "" %}\n        {%- set system_message = "Enable deep thinking subroutine.\n\n" ~ system_message %}\n    {%- else %}\n        {%- set system_message = "Enable deep thinking subroutine." %}\n    {%- endif %}\n{%- endif %}\n{#- Set the system message. In case there are tools present, add them to the system message. #}\n{%- if tools is not none or system_message != \'\' %}\n    {{- "<|start_header_id|>system<|end_header_id|>\n\n" }}\n    {{- system_message }}\n    {%- if tools is not none %}\n        {%- if system_message != "" %}\n            {{- "\n\n" }}\n        {%- endif %}\n        {{- "Available Tools:\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\n\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- "<|eot_id|>" }}\n{%- endif %}\n\n{#- Rest of the messages #}\n{%- for message in messages %}\n    {#- The special cases are when the message is from a tool (via role ipython/tool/tool_results) or when the message is from the assistant, but has "tool_calls". If not, we add the message directly as usual. #}\n    {#- Case 1 - Usual, non tool related message. #}\n    {%- if not (message.role == "ipython" or message.role == "tool" or message.role == "tool_results" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\n\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|eot_id|>\' }}\n    \n    {#- Case 2 - the response is from the assistant, but has a tool call returned. The assistant may also have returned some content along with the tool call. #}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- "<|start_header_id|>assistant<|end_header_id|>\n\n" }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                    {%- if item.text | trim != "" %}\n                        {{- "\n\n" }}\n                    {%- endif %}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- "[" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {%- if not tool_call.id is defined %}\n                {{- out }}\n            {%- else %}\n                {{- out[:-1] }}\n                {{- \', "id": "\' + tool_call.id + \'"}\' }}\n            {%- endif %}\n            {%- if not loop.last %}\n                {{- ", " }}\n            {%- else %}\n                {{- "]<|eot_id|>" }}\n            {%- endif %}\n        {%- endfor %}\n    \n    {#- Case 3 - the response is from a tool call. The tool call may have an id associated with it as well. If it does, we add it to the prompt. #}\n    {%- elif message.role == "ipython" or message["role"] == "tool_results" or message["role"] == "tool" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}\n        {%- if message.tool_call_id is defined and message.tool_call_id != \'\' %}\n            {{- \'{"content": \' + (message.content | tojson) + \', "call_id": "\' + message.tool_call_id + \'"}\' }}\n        {%- else %}\n            {{- \'{"content": \' + (message.content | tojson) + \'}\' }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\n\n\' }}\n{%- endif %}'), context_length=131072, supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 25, 697156, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/cogito-v1-preview-llama-70b', display_name='Cogito v1 Preview Llama 70B', create_time=datetime.datetime(2025, 4, 7, 21, 55, 23, 448429, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70553706496, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n{#- Set the system message. If enable_thinking is true, add the "Enable deep thinking subroutine." #}\n{%- if enable_thinking %}\n    {%- if system_message != "" %}\n        {%- set system_message = "Enable deep thinking subroutine.\n\n" ~ system_message %}\n    {%- else %}\n        {%- set system_message = "Enable deep thinking subroutine." %}\n    {%- endif %}\n{%- endif %}\n{#- Set the system message. In case there are tools present, add them to the system message. #}\n{%- if tools is not none or system_message != \'\' %}\n    {{- "<|start_header_id|>system<|end_header_id|>\n\n" }}\n    {{- system_message }}\n    {%- if tools is not none %}\n        {%- if system_message != "" %}\n            {{- "\n\n" }}\n        {%- endif %}\n        {{- "Available Tools:\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\n\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- "<|eot_id|>" }}\n{%- endif %}\n\n{#- Rest of the messages #}\n{%- for message in messages %}\n    {#- The special cases are when the message is from a tool (via role ipython/tool/tool_results) or when the message is from the assistant, but has "tool_calls". If not, we add the message directly as usual. #}\n    {#- Case 1 - Usual, non tool related message. #}\n    {%- if not (message.role == "ipython" or message.role == "tool" or message.role == "tool_results" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\n\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|eot_id|>\' }}\n    \n    {#- Case 2 - the response is from the assistant, but has a tool call returned. The assistant may also have returned some content along with the tool call. #}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- "<|start_header_id|>assistant<|end_header_id|>\n\n" }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                    {%- if item.text | trim != "" %}\n                        {{- "\n\n" }}\n                    {%- endif %}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- "[" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {%- if not tool_call.id is defined %}\n                {{- out }}\n            {%- else %}\n                {{- out[:-1] }}\n                {{- \', "id": "\' + tool_call.id + \'"}\' }}\n            {%- endif %}\n            {%- if not loop.last %}\n                {{- ", " }}\n            {%- else %}\n                {{- "]<|eot_id|>" }}\n            {%- endif %}\n        {%- endfor %}\n    \n    {#- Case 3 - the response is from a tool call. The tool call may have an id associated with it as well. If it does, we add it to the prompt. #}\n    {%- elif message.role == "ipython" or message["role"] == "tool_results" or message["role"] == "tool" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}\n        {%- if message.tool_call_id is defined and message.tool_call_id != \'\' %}\n            {{- \'{"content": \' + (message.content | tojson) + \', "call_id": "\' + message.tool_call_id + \'"}\' }}\n        {%- else %}\n            {{- \'{"content": \' + (message.content | tojson) + \'}\' }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\n\n\' }}\n{%- endif %}'), context_length=131072, supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 26, 243275, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/cogito-v1-preview-llama-8b', display_name='Cogito v1 Preview Llama 8B', create_time=datetime.datetime(2025, 4, 7, 20, 5, 6, 822531, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Cogito LLMs are instruction tuned generative models that is also a hybrid reasoning model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=8030261248, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n{#- Set the system message. If enable_thinking is true, add the "Enable deep thinking subroutine." #}\n{%- if enable_thinking %}\n    {%- if system_message != "" %}\n        {%- set system_message = "Enable deep thinking subroutine.\n\n" ~ system_message %}\n    {%- else %}\n        {%- set system_message = "Enable deep thinking subroutine." %}\n    {%- endif %}\n{%- endif %}\n{#- Set the system message. In case there are tools present, add them to the system message. #}\n{%- if tools is not none or system_message != \'\' %}\n    {{- "<|start_header_id|>system<|end_header_id|>\n\n" }}\n    {{- system_message }}\n    {%- if tools is not none %}\n        {%- if system_message != "" %}\n            {{- "\n\n" }}\n        {%- endif %}\n        {{- "Available Tools:\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\n\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- "<|eot_id|>" }}\n{%- endif %}\n\n{#- Rest of the messages #}\n{%- for message in messages %}\n    {#- The special cases are when the message is from a tool (via role ipython/tool/tool_results) or when the message is from the assistant, but has "tool_calls". If not, we add the message directly as usual. #}\n    {#- Case 1 - Usual, non tool related message. #}\n    {%- if not (message.role == "ipython" or message.role == "tool" or message.role == "tool_results" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\n\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|eot_id|>\' }}\n    \n    {#- Case 2 - the response is from the assistant, but has a tool call returned. The assistant may also have returned some content along with the tool call. #}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- "<|start_header_id|>assistant<|end_header_id|>\n\n" }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] | trim }}\n        {%- else %}\n            {%- for item in message[\'content\'] %}\n                {%- if item.type == \'text\' %}\n                    {{- item.text | trim }}\n                    {%- if item.text | trim != "" %}\n                        {{- "\n\n" }}\n                    {%- endif %}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- "[" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {%- if not tool_call.id is defined %}\n                {{- out }}\n            {%- else %}\n                {{- out[:-1] }}\n                {{- \', "id": "\' + tool_call.id + \'"}\' }}\n            {%- endif %}\n            {%- if not loop.last %}\n                {{- ", " }}\n            {%- else %}\n                {{- "]<|eot_id|>" }}\n            {%- endif %}\n        {%- endfor %}\n    \n    {#- Case 3 - the response is from a tool call. The tool call may have an id associated with it as well. If it does, we add it to the prompt. #}\n    {%- elif message.role == "ipython" or message["role"] == "tool_results" or message["role"] == "tool" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}\n        {%- if message.tool_call_id is defined and message.tool_call_id != \'\' %}\n            {{- \'{"content": \' + (message.content | tojson) + \', "call_id": "\' + message.tool_call_id + \'"}\' }}\n        {%- else %}\n            {{- \'{"content": \' + (message.content | tojson) + \'}\' }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\n\n\' }}\n{%- endif %}'), context_length=131072, supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 26, 777435, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/cogito-v1-preview-qwen-14b', display_name='Cogito v1 Preview Qwen 14B', create_time=datetime.datetime(2025, 4, 7, 20, 7, 42, 486886, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'merges.txt', 'model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14765947904, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{%- if tools %}\n    {{- "<|im_start|>system\n" }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {%- if enable_thinking %}\n            {{- "Enable deep thinking subroutine.\n\n" + messages[0][\'content\'] + "\n\n" }}\n        {%- else %}\n            {{- messages[0][\'content\'] + "\n\n" }}\n        {%- endif %}\n    {%- elif enable_thinking %}\n        {{- "Enable deep thinking subroutine.\n\n" }}\n    {%- endif %}\n    {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{"name": <function-name>, "arguments": <args-json-object>}\n</tool_call><|im_end|>\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {%- if enable_thinking %}\n            {{- "<|im_start|>system\n" + "Enable deep thinking subroutine.\n\n" + messages[0][\'content\'] + "<|im_end|>\n" }}\n        {%- else %}\n            {{- "<|im_start|>system\n" + messages[0][\'content\'] + "<|im_end|>\n" }}\n        {%- endif %}\n    {%- elif enable_thinking %}\n        {{- "<|im_start|>system\n" + "Enable deep thinking subroutine." + "<|im_end|>\n" }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\n\' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- elif message.content is iterable and message.content is not string %}\n            {%- for content_item in message.content %}\n                {%- if content_item.type == \'text\' %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\' + \'\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\n\' }}\n            {%- if message.content is string %}\n                {{- message.content }}\n            {%- elif message.content is iterable and message.content is not string %}\n                {%- for content_item in message.content %}\n                    {%- if content_item.type == \'text\' %}\n                        {{- content_item.text }}\n                    {%- endif %}\n                {%- endfor %}\n            {%- endif %}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\n<tool_call>\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\n<tool_response>\n\' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- elif message.content is iterable and message.content is not string %}\n            {%- for content_item in message.content %}\n                {%- if content_item.type == \'text\' %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\n\' }}\n{%- endif %}'), context_length=131072, supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 27, 330731, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/cogito-v1-preview-qwen-32b', display_name='Cogito v1 Preview Qwen 32B', create_time=datetime.datetime(2025, 4, 7, 20, 13, 57, 394972, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Cogito LLMs are instruction tuned generative models that are also hybrid reasoning models', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'merges.txt', 'model-00001-of-00014.safetensors', 'model-00002-of-00014.safetensors', 'model-00003-of-00014.safetensors', 'model-00004-of-00014.safetensors', 'model-00005-of-00014.safetensors', 'model-00006-of-00014.safetensors', 'model-00007-of-00014.safetensors', 'model-00008-of-00014.safetensors', 'model-00009-of-00014.safetensors', 'model-00010-of-00014.safetensors', 'model-00011-of-00014.safetensors', 'model-00012-of-00014.safetensors', 'model-00013-of-00014.safetensors', 'model-00014-of-00014.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32759790592, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{%- if tools %}\n    {{- "<|im_start|>system\n" }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {%- if enable_thinking %}\n            {{- "Enable deep thinking subroutine.\n\n" + messages[0][\'content\'] + "\n\n" }}\n        {%- else %}\n            {{- messages[0][\'content\'] + "\n\n" }}\n        {%- endif %}\n    {%- elif enable_thinking %}\n        {{- "Enable deep thinking subroutine.\n\n" }}\n    {%- endif %}\n    {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{"name": <function-name>, "arguments": <args-json-object>}\n</tool_call><|im_end|>\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {%- if enable_thinking %}\n            {{- "<|im_start|>system\n" + "Enable deep thinking subroutine.\n\n" + messages[0][\'content\'] + "<|im_end|>\n" }}\n        {%- else %}\n            {{- "<|im_start|>system\n" + messages[0][\'content\'] + "<|im_end|>\n" }}\n        {%- endif %}\n    {%- elif enable_thinking %}\n        {{- "<|im_start|>system\n" + "Enable deep thinking subroutine." + "<|im_end|>\n" }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\n\' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- elif message.content is iterable and message.content is not string %}\n            {%- for content_item in message.content %}\n                {%- if content_item.type == \'text\' %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\' + \'\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\n\' }}\n            {%- if message.content is string %}\n                {{- message.content }}\n            {%- elif message.content is iterable and message.content is not string %}\n                {%- for content_item in message.content %}\n                    {%- if content_item.type == \'text\' %}\n                        {{- content_item.text }}\n                    {%- endif %}\n                {%- endfor %}\n            {%- endif %}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\n<tool_call>\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\n<tool_response>\n\' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- elif message.content is iterable and message.content is not string %}\n            {%- for content_item in message.content %}\n                {%- if content_item.type == \'text\' %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\n\' }}\n{%- endif %}'), context_length=131072, supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 27, 868297, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/dbrx-instruct', display_name='DBRX Instruct', create_time=datetime.datetime(2024, 3, 29, 6, 21, 12, 397925, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DBRX Instruct is a 132B parameter mixture-of-experts (MoE) large language model developed by Databricks. Specializing in few-turn interactions, it is an instruction fine-tuned version of DBRX Base. The transformer-based, decoder-only model is trained on 12 trillion tokens of text and code data. Utilizing a fine-grained MoE architecture, it activates 36B parameters per input, enhancing model quality. It supports a context length of up to 32K tokens and incorporates advanced techniques like rotary position encodings, gated linear units, and grouped query attention.', hugging_face_url='https://huggingface.co/databricks/dbrx-instruct', base_model_details=BaseModelDetails(world_size=8, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=132000000000, moe=True, model_type='dbrx'), conversation_config=ConversationConfig(style='chatml'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens')], default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-coder-1b-base', display_name='DeepSeek Coder 1.3B Base', create_time=datetime.datetime(2024, 6, 18, 21, 19, 8, 877103, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. deepseek-coder-1.3b-base is a 1.3B parameter model with Multi-Head Attention trained on 1 trillion tokens.', hugging_face_url='https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1346471936, tunable=True, model_type='llama'), context_length=16384, created_by='xiaoyifan@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/deepseek-coder-33b-instruct', display_name='DeepSeek Coder 33B Instruct', create_time=datetime.datetime(2024, 2, 27, 17, 40, 52, 540831, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. deepseek-coder-33b-instruct is a 33B parameter model initialized from deepseek-coder-33b-base and fine-tuned on 2B tokens of instruction data.', hugging_face_url='https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00007.bin', 'pytorch_model-00002-of-00007.bin', 'pytorch_model-00003-of-00007.bin', 'pytorch_model-00004-of-00007.bin', 'pytorch_model-00005-of-00007.bin', 'pytorch_model-00006-of-00007.bin', 'pytorch_model-00007-of-00007.bin', 'pytorch_model.bin.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=33000000000, model_type='llama'), context_length=16384, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 28, 403343, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-coder-7b-base', display_name='DeepSeek Coder 7B Base', create_time=datetime.datetime(2024, 3, 15, 23, 15, 34, 454256, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. Deepseek Coder 6.7B Base is a 6.7B parameter model with Multi-Head Attention trained on 2 trillion tokens by employing a window size of 16K and an extra fill-in-the-blank task', hugging_face_url='https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=7000000000, tunable=True, model_type='llama'), context_length=16384, created_by='yingwg@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 28, 959756, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-coder-7b-base-v1p5', display_name='DeepSeek Coder 7B Base v1.5', create_time=datetime.datetime(2024, 2, 29, 5, 43, 40, 39247, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Deepseek Coder 7B Base v1.5 LLM is pre-trained from Deepseek 7B on 2T tokens by employing a window size of 4K and next token prediction objective.', github_url='https://github.com/deepseek-ai/deepseek-coder', hugging_face_url='https://huggingface.co/deepseek-ai/deepseek-coder-7b-base-v1.5', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=7000000000, model_type='llama'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 29, 491270, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5', display_name='DeepSeek Coder 7B Instruct v1.5', create_time=datetime.datetime(2024, 2, 27, 22, 19, 12, 156622, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Deepseek-Coder-7B-Instruct-v1.5 is pre-trained from Deepseek-LLM 7B on 2T tokens by employing a window size of 4K and next token prediction objective, and then fine-tuned on 2B tokens of instruction data.', hugging_face_url='https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=7000000000, model_type='llama'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 30, 24244, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/deepseek-coder-v2-instruct', display_name='DeepSeek Coder V2 Instruct', create_time=datetime.datetime(2024, 7, 11, 10, 56, 2, 889604, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek Coder V2 Instruct is a 236-billion-parameter open-source Mixture-of-Experts (MoE) code language model with 21 billion active parameters, developed by DeepSeek AI. Fine-tuned for instruction following, it achieves performance comparable to GPT4-Turbo on code-specific tasks. Pre-trained on an additional 6 trillion tokens, it enhances coding and mathematical reasoning capabilities, supports 338 programming languages, and extends context length from 16K to 128K while maintaining strong general language performance.', hugging_face_url='https://huggingface.co/DeepSeek/deepseek-coder-v2-instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=236000000000, model_type='deepseek_v2'), conversation_config=ConversationConfig(style='jinja'), context_length=131072, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, default_sampling_params={'top_k': 50.0, 'top_p': 0.949999988079071, 'temperature': 0.30000001192092896}),
Model(name='accounts/fireworks/models/deepseek-coder-v2-lite-base', display_name='DeepSeek Coder V2 Lite Base', create_time=datetime.datetime(2024, 7, 6, 16, 4, 29, 471119, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description=' an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.', hugging_face_url='https://huggingface.co/DeepSeek/deepseek-coder-v2-lite-base', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'configuration_deepseek.py', 'generation_config.json', 'model-00001-of-000004.safetensors', 'model-00002-of-000004.safetensors', 'model-00003-of-000004.safetensors', 'model-00004-of-000004.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenization_deepseek_fast.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=15706484224, tunable=True, model_type='deepseek_v2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}"), context_length=163840, created_by='xiaoyifan@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], calibrated=True, tunable=True, update_time=datetime.datetime(2025, 5, 2, 23, 50, 29, 988689, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.30000001192092896, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-coder-v2-lite-instruct', display_name='DeepSeek Coder V2 Lite Instruct', create_time=datetime.datetime(2024, 7, 4, 18, 20, 6, 208574, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek Coder V2 Lite Instruct is a 16-billion-parameter open-source Mixture-of-Experts (MoE) code language model with 2.4 billion active parameters, developed by DeepSeek AI. Fine-tuned for instruction following, it achieves performance comparable to GPT4-Turbo on code-specific tasks. Pre-trained on an additional 6 trillion tokens, it enhances coding and mathematical reasoning capabilities, supports 338 programming languages, and extends context length from 16K to 128K while maintaining strong general language performance.', hugging_face_url='https://huggingface.co/DeepSeek/deepseek-coder-v2-lite-instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=15700000000, tunable=True, model_type='deepseek_v2'), conversation_config=ConversationConfig(style='jinja', template='\n {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\n {{ bos_token }}\n {% set _mode = mode | default(\'generate\', true) %}\n {% for message in messages %}\n     {% if message[\'role\'] == \'user\' %}\n         {{ \'User: \' + message[\'content\'] + \'\\n\\n\' }}\n     {% elif message[\'role\'] == \'assistant\' %}\n         {{ \'Assistant: \' + ("" if _mode == \'generate\' else unk_token) + message[\'content\'] + (eos_token if _mode == \'generate\' else eos_token + unk_token) }}\n     {% elif message[\'role\'] == \'system\' %}\n         {{ message[\'content\'] + \'\\n\\n\' }}\n     {% endif %}\n {% endfor %}\n {% if add_generation_prompt %}\n     {{ \'Assistant:\' }}\n {% endif %}\n'), context_length=163840, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], calibrated=True, tunable=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 30, 568479, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.30000001192092896, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-prover-v2', display_name='DeepSeek Prover V2', create_time=datetime.datetime(2025, 5, 1, 3, 26, 45, 587294, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3.', hugging_face_url='https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'configuration_deepseek.py', 'model-00001-of-000163.safetensors', 'model-00002-of-000163.safetensors', 'model-00003-of-000163.safetensors', 'model-00004-of-000163.safetensors', 'model-00005-of-000163.safetensors', 'model-00006-of-000163.safetensors', 'model-00007-of-000163.safetensors', 'model-00008-of-000163.safetensors', 'model-00009-of-000163.safetensors', 'model-00010-of-000163.safetensors', 'model-00011-of-000163.safetensors', 'model-00012-of-000163.safetensors', 'model-00013-of-000163.safetensors', 'model-00014-of-000163.safetensors', 'model-00015-of-000163.safetensors', 'model-00016-of-000163.safetensors', 'model-00017-of-000163.safetensors', 'model-00018-of-000163.safetensors', 'model-00019-of-000163.safetensors', 'model-00020-of-000163.safetensors', 'model-00021-of-000163.safetensors', 'model-00022-of-000163.safetensors', 'model-00023-of-000163.safetensors', 'model-00024-of-000163.safetensors', 'model-00025-of-000163.safetensors', 'model-00026-of-000163.safetensors', 'model-00027-of-000163.safetensors', 'model-00028-of-000163.safetensors', 'model-00029-of-000163.safetensors', 'model-00030-of-000163.safetensors', 'model-00031-of-000163.safetensors', 'model-00032-of-000163.safetensors', 'model-00033-of-000163.safetensors', 'model-00034-of-000163.safetensors', 'model-00035-of-000163.safetensors', 'model-00036-of-000163.safetensors', 'model-00037-of-000163.safetensors', 'model-00038-of-000163.safetensors', 'model-00039-of-000163.safetensors', 'model-00040-of-000163.safetensors', 'model-00041-of-000163.safetensors', 'model-00042-of-000163.safetensors', 'model-00043-of-000163.safetensors', 'model-00044-of-000163.safetensors', 'model-00045-of-000163.safetensors', 'model-00046-of-000163.safetensors', 'model-00047-of-000163.safetensors', 'model-00048-of-000163.safetensors', 'model-00049-of-000163.safetensors', 'model-00050-of-000163.safetensors', 'model-00051-of-000163.safetensors', 'model-00052-of-000163.safetensors', 'model-00053-of-000163.safetensors', 'model-00054-of-000163.safetensors', 'model-00055-of-000163.safetensors', 'model-00056-of-000163.safetensors', 'model-00057-of-000163.safetensors', 'model-00058-of-000163.safetensors', 'model-00059-of-000163.safetensors', 'model-00060-of-000163.safetensors', 'model-00061-of-000163.safetensors', 'model-00062-of-000163.safetensors', 'model-00063-of-000163.safetensors', 'model-00064-of-000163.safetensors', 'model-00065-of-000163.safetensors', 'model-00066-of-000163.safetensors', 'model-00067-of-000163.safetensors', 'model-00068-of-000163.safetensors', 'model-00069-of-000163.safetensors', 'model-00070-of-000163.safetensors', 'model-00071-of-000163.safetensors', 'model-00072-of-000163.safetensors', 'model-00073-of-000163.safetensors', 'model-00074-of-000163.safetensors', 'model-00075-of-000163.safetensors', 'model-00076-of-000163.safetensors', 'model-00077-of-000163.safetensors', 'model-00078-of-000163.safetensors', 'model-00079-of-000163.safetensors', 'model-00080-of-000163.safetensors', 'model-00081-of-000163.safetensors', 'model-00082-of-000163.safetensors', 'model-00083-of-000163.safetensors', 'model-00084-of-000163.safetensors', 'model-00085-of-000163.safetensors', 'model-00086-of-000163.safetensors', 'model-00087-of-000163.safetensors', 'model-00088-of-000163.safetensors', 'model-00089-of-000163.safetensors', 'model-00090-of-000163.safetensors', 'model-00091-of-000163.safetensors', 'model-00092-of-000163.safetensors', 'model-00093-of-000163.safetensors', 'model-00094-of-000163.safetensors', 'model-00095-of-000163.safetensors', 'model-00096-of-000163.safetensors', 'model-00097-of-000163.safetensors', 'model-00098-of-000163.safetensors', 'model-00099-of-000163.safetensors', 'model-00100-of-000163.safetensors', 'model-00101-of-000163.safetensors', 'model-00102-of-000163.safetensors', 'model-00103-of-000163.safetensors', 'model-00104-of-000163.safetensors', 'model-00105-of-000163.safetensors', 'model-00106-of-000163.safetensors', 'model-00107-of-000163.safetensors', 'model-00108-of-000163.safetensors', 'model-00109-of-000163.safetensors', 'model-00110-of-000163.safetensors', 'model-00111-of-000163.safetensors', 'model-00112-of-000163.safetensors', 'model-00113-of-000163.safetensors', 'model-00114-of-000163.safetensors', 'model-00115-of-000163.safetensors', 'model-00116-of-000163.safetensors', 'model-00117-of-000163.safetensors', 'model-00118-of-000163.safetensors', 'model-00119-of-000163.safetensors', 'model-00120-of-000163.safetensors', 'model-00121-of-000163.safetensors', 'model-00122-of-000163.safetensors', 'model-00123-of-000163.safetensors', 'model-00124-of-000163.safetensors', 'model-00125-of-000163.safetensors', 'model-00126-of-000163.safetensors', 'model-00127-of-000163.safetensors', 'model-00128-of-000163.safetensors', 'model-00129-of-000163.safetensors', 'model-00130-of-000163.safetensors', 'model-00131-of-000163.safetensors', 'model-00132-of-000163.safetensors', 'model-00133-of-000163.safetensors', 'model-00134-of-000163.safetensors', 'model-00135-of-000163.safetensors', 'model-00136-of-000163.safetensors', 'model-00137-of-000163.safetensors', 'model-00138-of-000163.safetensors', 'model-00139-of-000163.safetensors', 'model-00140-of-000163.safetensors', 'model-00141-of-000163.safetensors', 'model-00142-of-000163.safetensors', 'model-00143-of-000163.safetensors', 'model-00144-of-000163.safetensors', 'model-00145-of-000163.safetensors', 'model-00146-of-000163.safetensors', 'model-00147-of-000163.safetensors', 'model-00148-of-000163.safetensors', 'model-00149-of-000163.safetensors', 'model-00150-of-000163.safetensors', 'model-00151-of-000163.safetensors', 'model-00152-of-000163.safetensors', 'model-00153-of-000163.safetensors', 'model-00154-of-000163.safetensors', 'model-00155-of-000163.safetensors', 'model-00156-of-000163.safetensors', 'model-00157-of-000163.safetensors', 'model-00158-of-000163.safetensors', 'model-00159-of-000163.safetensors', 'model-00160-of-000163.safetensors', 'model-00161-of-000163.safetensors', 'model-00162-of-000163.safetensors', 'model-00163-of-000163.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=671067257432, moe=True, model_type='deepseek_v3'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<User>' + message['content'] + '<Assistant>'}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<tooloutputsend>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- else %}{{message['content'] + '<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- endif %}{%- endfor %}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=163840, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens')], calibrated=True, tunable=True, use_hf_apply_chat_template=True, extra_deployment_args=['--stream-buffer=0', '--conversation-style=deepseek', '--num-draft-tokens=2', '--draft-model={"type":"mtp"}', '--moe-sharding=ep', '--attention-sharding=tensor_parallel_head_kv_seq_parallel'], update_time=datetime.datetime(2025, 5, 8, 0, 49, 31, 86181, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/deepseek-r1', display_name='DeepSeek R1 (Fast)', create_time=datetime.datetime(2025, 1, 20, 18, 27, 53, 770705, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek R1 (Fast) is the speed-optimized serverless deployment of DeepSeek-R1. Compared to the DeepSeek R1 (Basic) endpoint, R1 (Fast) provides faster speeds with higher per-token prices, see https://fireworks.ai/pricing for details. Identical models are served on the two endpoints, so there are no quality or quantization differences. DeepSeek-R1 is a state-of-the-art large language model optimized with reinforcement learning and cold-start data for exceptional reasoning, math, and code performance. The model is identical to the one uploaded by DeepSeek on HuggingFace. Note that fine-tuning for this model is only available through contacting fireworks at https://fireworks.ai/company/contact-us.', hugging_face_url='https://huggingface.co/deepseek-ai/DeepSeek-R1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=685000000000, model_type='deepseek_v3'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- else %}{{'<Assistant>' + message['content'] + '<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- endif %}{%- endfor %}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=163840, featured_priority=10, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=8), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-bal3jpuk', deployment='accounts/pyroworks/deployments/yyer8xj2', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-r1-v7rkt1l9', deployment='accounts/fireworks/deployments/azatror1', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-w2e8jmfq', deployment='accounts/pyroworks/deployments/ufv2obet', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/perplexity/deployedModels/deepseek-r1-bfcb38ea', deployment='accounts/perplexity/deployments/6521cbe6', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/uber/deployedModels/deepseek-r1-7da6151b', deployment='accounts/uber/deployments/72059fa3', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-r1-27dd1cb5', deployment='accounts/fireworks/deployments/937ccc86', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/quora/deployedModels/deepseek-r1-c4evhazu', deployment='accounts/quora/deployments/odoe87sg', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-r1-6642420f', deployment='accounts/fireworks/deployments/5f6e7ae8', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/mark-1099fc/deployedModels/deepseek-r1-agzq4x89', deployment='accounts/mark-1099fc/deployments/d1cad87d', state=DeployedModelState.DEPLOYED)], calibrated=True, tunable=True, extra_deployment_args=['--stream-buffer=0', '--num-draft-tokens=2', '--draft-model={"type":"mtp"}', '--moe-sharding=ep', '--attention-sharding=tensor_parallel_head_kv_seq_parallel', '--end-thinking-token=</think>'], update_time=datetime.datetime(2025, 5, 8, 0, 49, 31, 641049, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-r1-basic', display_name='DeepSeek R1 (Basic)', create_time=datetime.datetime(2025, 3, 18, 14, 5, 46, 238727, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek R1 (Basic) is the cost-optimized serverless deployment of DeepSeek-R1. Compared to the DeepSeek R1 (Fast) endpoint, R1 (Basic) provides lower per-token prices with slower speeds, see https://fireworks.ai/pricing for details. Identical models are served on the two endpoints, so there are no quality or quantization differences. DeepSeek-R1 is a state-of-the-art large language model optimized with reinforcement learning and cold-start data for exceptional reasoning, math, and code performance. The model is identical to the one uploaded by DeepSeek on HuggingFace. Note that fine-tuning for this model is only available through contacting fireworks at https://fireworks.ai/company/contact-us.', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'configuration_deepseek.py', 'model-00001-of-000163.safetensors', 'model-00002-of-000163.safetensors', 'model-00003-of-000163.safetensors', 'model-00004-of-000163.safetensors', 'model-00005-of-000163.safetensors', 'model-00006-of-000163.safetensors', 'model-00007-of-000163.safetensors', 'model-00008-of-000163.safetensors', 'model-00009-of-000163.safetensors', 'model-00010-of-000163.safetensors', 'model-00011-of-000163.safetensors', 'model-00012-of-000163.safetensors', 'model-00013-of-000163.safetensors', 'model-00014-of-000163.safetensors', 'model-00015-of-000163.safetensors', 'model-00016-of-000163.safetensors', 'model-00017-of-000163.safetensors', 'model-00018-of-000163.safetensors', 'model-00019-of-000163.safetensors', 'model-00020-of-000163.safetensors', 'model-00021-of-000163.safetensors', 'model-00022-of-000163.safetensors', 'model-00023-of-000163.safetensors', 'model-00024-of-000163.safetensors', 'model-00025-of-000163.safetensors', 'model-00026-of-000163.safetensors', 'model-00027-of-000163.safetensors', 'model-00028-of-000163.safetensors', 'model-00029-of-000163.safetensors', 'model-00030-of-000163.safetensors', 'model-00031-of-000163.safetensors', 'model-00032-of-000163.safetensors', 'model-00033-of-000163.safetensors', 'model-00034-of-000163.safetensors', 'model-00035-of-000163.safetensors', 'model-00036-of-000163.safetensors', 'model-00037-of-000163.safetensors', 'model-00038-of-000163.safetensors', 'model-00039-of-000163.safetensors', 'model-00040-of-000163.safetensors', 'model-00041-of-000163.safetensors', 'model-00042-of-000163.safetensors', 'model-00043-of-000163.safetensors', 'model-00044-of-000163.safetensors', 'model-00045-of-000163.safetensors', 'model-00046-of-000163.safetensors', 'model-00047-of-000163.safetensors', 'model-00048-of-000163.safetensors', 'model-00049-of-000163.safetensors', 'model-00050-of-000163.safetensors', 'model-00051-of-000163.safetensors', 'model-00052-of-000163.safetensors', 'model-00053-of-000163.safetensors', 'model-00054-of-000163.safetensors', 'model-00055-of-000163.safetensors', 'model-00056-of-000163.safetensors', 'model-00057-of-000163.safetensors', 'model-00058-of-000163.safetensors', 'model-00059-of-000163.safetensors', 'model-00060-of-000163.safetensors', 'model-00061-of-000163.safetensors', 'model-00062-of-000163.safetensors', 'model-00063-of-000163.safetensors', 'model-00064-of-000163.safetensors', 'model-00065-of-000163.safetensors', 'model-00066-of-000163.safetensors', 'model-00067-of-000163.safetensors', 'model-00068-of-000163.safetensors', 'model-00069-of-000163.safetensors', 'model-00070-of-000163.safetensors', 'model-00071-of-000163.safetensors', 'model-00072-of-000163.safetensors', 'model-00073-of-000163.safetensors', 'model-00074-of-000163.safetensors', 'model-00075-of-000163.safetensors', 'model-00076-of-000163.safetensors', 'model-00077-of-000163.safetensors', 'model-00078-of-000163.safetensors', 'model-00079-of-000163.safetensors', 'model-00080-of-000163.safetensors', 'model-00081-of-000163.safetensors', 'model-00082-of-000163.safetensors', 'model-00083-of-000163.safetensors', 'model-00084-of-000163.safetensors', 'model-00085-of-000163.safetensors', 'model-00086-of-000163.safetensors', 'model-00087-of-000163.safetensors', 'model-00088-of-000163.safetensors', 'model-00089-of-000163.safetensors', 'model-00090-of-000163.safetensors', 'model-00091-of-000163.safetensors', 'model-00092-of-000163.safetensors', 'model-00093-of-000163.safetensors', 'model-00094-of-000163.safetensors', 'model-00095-of-000163.safetensors', 'model-00096-of-000163.safetensors', 'model-00097-of-000163.safetensors', 'model-00098-of-000163.safetensors', 'model-00099-of-000163.safetensors', 'model-00100-of-000163.safetensors', 'model-00101-of-000163.safetensors', 'model-00102-of-000163.safetensors', 'model-00103-of-000163.safetensors', 'model-00104-of-000163.safetensors', 'model-00105-of-000163.safetensors', 'model-00106-of-000163.safetensors', 'model-00107-of-000163.safetensors', 'model-00108-of-000163.safetensors', 'model-00109-of-000163.safetensors', 'model-00110-of-000163.safetensors', 'model-00111-of-000163.safetensors', 'model-00112-of-000163.safetensors', 'model-00113-of-000163.safetensors', 'model-00114-of-000163.safetensors', 'model-00115-of-000163.safetensors', 'model-00116-of-000163.safetensors', 'model-00117-of-000163.safetensors', 'model-00118-of-000163.safetensors', 'model-00119-of-000163.safetensors', 'model-00120-of-000163.safetensors', 'model-00121-of-000163.safetensors', 'model-00122-of-000163.safetensors', 'model-00123-of-000163.safetensors', 'model-00124-of-000163.safetensors', 'model-00125-of-000163.safetensors', 'model-00126-of-000163.safetensors', 'model-00127-of-000163.safetensors', 'model-00128-of-000163.safetensors', 'model-00129-of-000163.safetensors', 'model-00130-of-000163.safetensors', 'model-00131-of-000163.safetensors', 'model-00132-of-000163.safetensors', 'model-00133-of-000163.safetensors', 'model-00134-of-000163.safetensors', 'model-00135-of-000163.safetensors', 'model-00136-of-000163.safetensors', 'model-00137-of-000163.safetensors', 'model-00138-of-000163.safetensors', 'model-00139-of-000163.safetensors', 'model-00140-of-000163.safetensors', 'model-00141-of-000163.safetensors', 'model-00142-of-000163.safetensors', 'model-00143-of-000163.safetensors', 'model-00144-of-000163.safetensors', 'model-00145-of-000163.safetensors', 'model-00146-of-000163.safetensors', 'model-00147-of-000163.safetensors', 'model-00148-of-000163.safetensors', 'model-00149-of-000163.safetensors', 'model-00150-of-000163.safetensors', 'model-00151-of-000163.safetensors', 'model-00152-of-000163.safetensors', 'model-00153-of-000163.safetensors', 'model-00154-of-000163.safetensors', 'model-00155-of-000163.safetensors', 'model-00156-of-000163.safetensors', 'model-00157-of-000163.safetensors', 'model-00158-of-000163.safetensors', 'model-00159-of-000163.safetensors', 'model-00160-of-000163.safetensors', 'model-00161-of-000163.safetensors', 'model-00162-of-000163.safetensors', 'model-00163-of-000163.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=671067257432, moe=True, model_type='deepseek_v3'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- else %}{{'<Assistant>' + message['content'] + '<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- endif %}{%- endfor %}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=163840, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=550000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=2, nanos=190000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-basic-zgs1nm5d', deployment='accounts/pyroworks/deployments/doky9937', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-r1-throughput-f59efaf1', deployment='accounts/fireworks/deployments/1e8f275e', state=DeployedModelState.DEPLOYED, default=True, public=True)], calibrated=True, tunable=True, extra_deployment_args=['--attention-sharding=tensor_parallel_head_kv_seq_parallel', '--moe-sharding=ep', '--conversation-style=deepseek', '--num-draft-tokens=3', '--draft-model={"type":"mtp"}'], update_time=datetime.datetime(2025, 5, 8, 0, 49, 32, 197172, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-llama-70b', display_name='DeepSeek R1 Distill Llama 70B', create_time=datetime.datetime(2025, 1, 21, 22, 44, 20, 585605, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-000017.safetensors', 'model-00002-of-000017.safetensors', 'model-00003-of-000017.safetensors', 'model-00004-of-000017.safetensors', 'model-00005-of-000017.safetensors', 'model-00006-of-000017.safetensors', 'model-00007-of-000017.safetensors', 'model-00008-of-000017.safetensors', 'model-00009-of-000017.safetensors', 'model-00010-of-000017.safetensors', 'model-00011-of-000017.safetensors', 'model-00012-of-000017.safetensors', 'model-00013-of-000017.safetensors', 'model-00014-of-000017.safetensors', 'model-00015-of-000017.safetensors', 'model-00016-of-000017.safetensors', 'model-00017-of-000017.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70553706496, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, featured_priority=5, created_by='shaunak@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/vineeth-vajipey-7e1414/deployedModels/deepseek-r1-distill-llama-70b-k5dztzl8', deployment='accounts/vineeth-vajipey-7e1414/deployments/c6zyjkmo', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/yi-ee2d48/deployedModels/deepseek-r1-distill-llama-70b-bq1ybehg', deployment='accounts/yi-ee2d48/deployments/y4zx34qi', state=DeployedModelState.DEPLOYED)], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 32, 783006, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 0.949999988079071, 'temperature': 0.6000000238418579}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-llama-8b', display_name='DeepSeek R1 Distill Llama 8B', create_time=datetime.datetime(2025, 1, 22, 17, 31, 57, 166736, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama 8B distilled with reasoning from Deepseek R1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=8030261248, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, featured_priority=5, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 33, 334207, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-qwen-14b', display_name='DeepSeek R1 Distill Qwen 14B', create_time=datetime.datetime(2025, 1, 22, 17, 34, 10, 16215, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen 14B distilled with reasoning from Deepseek R1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=14770033664, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, featured_priority=5, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/vineeth-vajipey-7e1414/deployedModels/deepseek-r1-distill-qwen-14b-xx4lazz0', deployment='accounts/vineeth-vajipey-7e1414/deployments/m2b6w12g', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 33, 896331, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 0.949999988079071, 'temperature': 0.6000000238418579}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b', display_name='DeepSeek R1 Distill Qwen 1.5B', create_time=datetime.datetime(2025, 1, 22, 17, 16, 22, 406629, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen 1.5B distilled with reasoning from Deepseek R1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1777088000, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, featured_priority=5, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/moamenmoustafa04-bd8fbe/deployedModels/deepseek-r1-distill-qwen-1p5b-vb234spr', deployment='accounts/moamenmoustafa04-bd8fbe/deployments/iknwxdd4', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-y16fw6hr', deployment='accounts/pyroworks/deployments/hqaf40iw', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-ji0jshtq', deployment='accounts/pyroworks/deployments/pe8ykjkl', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-sidu3dtc', deployment='accounts/pyroworks/deployments/edbzlqx9', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-mup1ycri', deployment='accounts/pyroworks/deployments/w6my08x7', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-plothrkr', deployment='accounts/pyroworks/deployments/vds6k79u', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/upwork-dev/deployedModels/deepseek-r1-distill-qwen-1p5b-r10wana2', deployment='accounts/upwork-dev/deployments/bw7bahbl', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-r1-distill-qwen-1p5b-inyb1syg', deployment='accounts/pyroworks/deployments/fnerzpy5', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 34, 449869, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-qwen-32b', display_name='DeepSeek R1 Distill Qwen 32B', create_time=datetime.datetime(2025, 1, 22, 0, 11, 1, 910030, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-000008.safetensors', 'model-00002-of-000008.safetensors', 'model-00003-of-000008.safetensors', 'model-00004-of-000008.safetensors', 'model-00005-of-000008.safetensors', 'model-00006-of-000008.safetensors', 'model-00007-of-000008.safetensors', 'model-00008-of-000008.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, created_by='tianyi@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 34, 980381, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 0.949999988079071, 'temperature': 0.6000000238418579}),
Model(name='accounts/fireworks/models/deepseek-r1-distill-qwen-7b', display_name='DeepSeek R1 Distill Qwen 7B', create_time=datetime.datetime(2025, 1, 22, 17, 19, 11, 992654, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen 7B distilled with reasoning from Deepseek R1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-000002.safetensors', 'model-00002-of-000002.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<User>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<Assistant>' + content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=131072, featured_priority=5, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/elaskyethan-7c8152/deployedModels/deepseek-r1-distill-qwen-7b-pv8s2tiq', deployment='accounts/elaskyethan-7c8152/deployments/gl3dsxas', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/vineeth-vajipey-7e1414/deployedModels/deepseek-r1-distill-qwen-7b-f7h4q60r', deployment='accounts/vineeth-vajipey-7e1414/deployments/uyo4ynta', state=DeployedModelState.DEPLOYED)], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 35, 538613, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-v2-lite-chat', display_name='DeepSeek V2 Lite Chat', create_time=datetime.datetime(2024, 10, 22, 15, 45, 20, 273721, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation.', github_url='https://github.com/deepseek-ai/DeepSeek-V2', hugging_face_url='https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'configuration_deepseek.py', 'generation_config.json', 'model-00001-of-000004.safetensors', 'model-00002-of-000004.safetensors', 'model-00003-of-000004.safetensors', 'model-00004-of-000004.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenization_deepseek_fast.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1311632896, model_type='deepseek_v2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}"), context_length=163840, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, default_sampling_params={'temperature': 0.30000001192092896, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-v2p5', display_name='DeepSeek V2.5', create_time=datetime.datetime(2024, 9, 6, 18, 23, 29, 344313, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.', hugging_face_url='https://huggingface.co/DeepSeek/deepseek-v2p5', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'configuration_deepseek.py', 'generation_config.json', 'model-00001-of-000055.safetensors', 'model-00002-of-000055.safetensors', 'model-00003-of-000055.safetensors', 'model-00004-of-000055.safetensors', 'model-00005-of-000055.safetensors', 'model-00006-of-000055.safetensors', 'model-00007-of-000055.safetensors', 'model-00008-of-000055.safetensors', 'model-00009-of-000055.safetensors', 'model-00010-of-000055.safetensors', 'model-00011-of-000055.safetensors', 'model-00012-of-000055.safetensors', 'model-00013-of-000055.safetensors', 'model-00014-of-000055.safetensors', 'model-00015-of-000055.safetensors', 'model-00016-of-000055.safetensors', 'model-00017-of-000055.safetensors', 'model-00018-of-000055.safetensors', 'model-00019-of-000055.safetensors', 'model-00020-of-000055.safetensors', 'model-00021-of-000055.safetensors', 'model-00022-of-000055.safetensors', 'model-00023-of-000055.safetensors', 'model-00024-of-000055.safetensors', 'model-00025-of-000055.safetensors', 'model-00026-of-000055.safetensors', 'model-00027-of-000055.safetensors', 'model-00028-of-000055.safetensors', 'model-00029-of-000055.safetensors', 'model-00030-of-000055.safetensors', 'model-00031-of-000055.safetensors', 'model-00032-of-000055.safetensors', 'model-00033-of-000055.safetensors', 'model-00034-of-000055.safetensors', 'model-00035-of-000055.safetensors', 'model-00036-of-000055.safetensors', 'model-00037-of-000055.safetensors', 'model-00038-of-000055.safetensors', 'model-00039-of-000055.safetensors', 'model-00040-of-000055.safetensors', 'model-00041-of-000055.safetensors', 'model-00042-of-000055.safetensors', 'model-00043-of-000055.safetensors', 'model-00044-of-000055.safetensors', 'model-00045-of-000055.safetensors', 'model-00046-of-000055.safetensors', 'model-00047-of-000055.safetensors', 'model-00048-of-000055.safetensors', 'model-00049-of-000055.safetensors', 'model-00050-of-000055.safetensors', 'model-00051-of-000055.safetensors', 'model-00052-of-000055.safetensors', 'model-00053-of-000055.safetensors', 'model-00054-of-000055.safetensors', 'model-00055-of-000055.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=13023892480, model_type='deepseek_v2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}    {%- if message['role'] == 'system' %}        {% set ns.system_prompt = message['content'] %}    {%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}    {%- if message['role'] == 'user' %}    {%- set ns.is_tool = false -%}{{'<User>' + message['content']}}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is none %}        {%- set ns.is_tool = false -%}        {%- for tool in message['tool_calls']%}            {%- if not ns.is_first %}{{'<Assistant><toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}            {%- set ns.is_first = true -%}            {%- else %}{{'\\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<toolcallend>'}}{{'<toolcallsend><endofsentence>'}}                   {%- endif %}        {%- endfor %}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is not none %}        {%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}        {%- set ns.is_tool = false -%}        {%- else %}{{'<Assistant>' + message['content'] + '<endofsentence>'}}        {%- endif %}    {%- endif %}    {%- if message['role'] == 'tool' %}        {%- set ns.is_tool = true -%}        {%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}        {%- set ns.is_output_first = false %}        {%- else %}{{'\\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}        {%- endif %}    {%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=163840, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, default_sampling_params={'temperature': 0.30000001192092896, 'top_k': 50.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/deepseek-v3', display_name='DeepSeek V3', create_time=datetime.datetime(2024, 12, 30, 16, 37, 48, 553193, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='A a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token from Deepseek. Note that fine-tuning for this model is only available through contacting fireworks at https://fireworks.ai/company/contact-us.', hugging_face_url='https://huggingface.co/deepseek-ai/DeepSeek-V3', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=685000000000, model_type='deepseek_v3'), conversation_config=ConversationConfig(style='deepseekv3', template="\n{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(\n    initial_system_message_handled = false,\n    last_assistant_index_for_eos = -1,\n    messages = messages\n) -%}\n\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' + message|tojson) }}\n    {%- endif -%}\n\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' + message|tojson) }}\n    {%- endif -%}\n\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ message['content'] }}\n    {%- endif -%}\n\n    {%- if message['role'] | upper == 'USER' -%}\n        {{ '<User>' + message['content'] }}\n    {%- endif -%}\n\n    {%- if message['role'] | upper == 'ASSISTANT' and message['content'] is not none -%}\n        {%- if _mode == 'train' -%}\n            {{ '<Assistant>' + unk_token + message['content'] + '<endofsentence>' + unk_token }}\n        {%- else -%}\n            {{ '<Assistant>' + message['content'] + '<endofsentence>' }}\n        {%- endif -%}\n    {%- endif -%}\n\n    {%- if message['role'] | upper in ['USER', 'ASSISTANT'] -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<Assistant>' }}\n{%- endif -%}\n"), context_length=131072, supports_tools=True, tokens_per_second=33, featured_priority=9, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/teo-0b3810/deployedModels/deepseek-v3-hu9gg0li', deployment='accounts/teo-0b3810/deployments/wvlodmjs', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-v3-76613082', deployment='accounts/fireworks/deployments/62a64a79', state=DeployedModelState.DEPLOYED, default=True, public=True)], calibrated=True, tunable=True, extra_deployment_args=['--stream-buffer=0', '--num-draft-tokens=2', '--draft-model={"type":"mtp"}', '--moe-sharding=ep', '--attention-sharding=tensor_parallel_head_kv_seq_parallel', '--conversation-style=deepseek'], update_time=datetime.datetime(2025, 4, 14, 1, 12, 9, 423825, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/deepseek-v3-0324', display_name='Deepseek V3 03-24', create_time=datetime.datetime(2025, 3, 24, 14, 40, 20, 594012, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token from Deepseek. Updated checkpoint. Note that fine-tuning for this model is only available upon request through contacting fireworks at https://fireworks.ai/company/contact-us.', hugging_face_url='https://huggingface.co/deepseek-ai/DeepSeek-V3-0324', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'configuration_deepseek.py', 'model-00001-of-000163.safetensors', 'model-00002-of-000163.safetensors', 'model-00003-of-000163.safetensors', 'model-00004-of-000163.safetensors', 'model-00005-of-000163.safetensors', 'model-00006-of-000163.safetensors', 'model-00007-of-000163.safetensors', 'model-00008-of-000163.safetensors', 'model-00009-of-000163.safetensors', 'model-00010-of-000163.safetensors', 'model-00011-of-000163.safetensors', 'model-00012-of-000163.safetensors', 'model-00013-of-000163.safetensors', 'model-00014-of-000163.safetensors', 'model-00015-of-000163.safetensors', 'model-00016-of-000163.safetensors', 'model-00017-of-000163.safetensors', 'model-00018-of-000163.safetensors', 'model-00019-of-000163.safetensors', 'model-00020-of-000163.safetensors', 'model-00021-of-000163.safetensors', 'model-00022-of-000163.safetensors', 'model-00023-of-000163.safetensors', 'model-00024-of-000163.safetensors', 'model-00025-of-000163.safetensors', 'model-00026-of-000163.safetensors', 'model-00027-of-000163.safetensors', 'model-00028-of-000163.safetensors', 'model-00029-of-000163.safetensors', 'model-00030-of-000163.safetensors', 'model-00031-of-000163.safetensors', 'model-00032-of-000163.safetensors', 'model-00033-of-000163.safetensors', 'model-00034-of-000163.safetensors', 'model-00035-of-000163.safetensors', 'model-00036-of-000163.safetensors', 'model-00037-of-000163.safetensors', 'model-00038-of-000163.safetensors', 'model-00039-of-000163.safetensors', 'model-00040-of-000163.safetensors', 'model-00041-of-000163.safetensors', 'model-00042-of-000163.safetensors', 'model-00043-of-000163.safetensors', 'model-00044-of-000163.safetensors', 'model-00045-of-000163.safetensors', 'model-00046-of-000163.safetensors', 'model-00047-of-000163.safetensors', 'model-00048-of-000163.safetensors', 'model-00049-of-000163.safetensors', 'model-00050-of-000163.safetensors', 'model-00051-of-000163.safetensors', 'model-00052-of-000163.safetensors', 'model-00053-of-000163.safetensors', 'model-00054-of-000163.safetensors', 'model-00055-of-000163.safetensors', 'model-00056-of-000163.safetensors', 'model-00057-of-000163.safetensors', 'model-00058-of-000163.safetensors', 'model-00059-of-000163.safetensors', 'model-00060-of-000163.safetensors', 'model-00061-of-000163.safetensors', 'model-00062-of-000163.safetensors', 'model-00063-of-000163.safetensors', 'model-00064-of-000163.safetensors', 'model-00065-of-000163.safetensors', 'model-00066-of-000163.safetensors', 'model-00067-of-000163.safetensors', 'model-00068-of-000163.safetensors', 'model-00069-of-000163.safetensors', 'model-00070-of-000163.safetensors', 'model-00071-of-000163.safetensors', 'model-00072-of-000163.safetensors', 'model-00073-of-000163.safetensors', 'model-00074-of-000163.safetensors', 'model-00075-of-000163.safetensors', 'model-00076-of-000163.safetensors', 'model-00077-of-000163.safetensors', 'model-00078-of-000163.safetensors', 'model-00079-of-000163.safetensors', 'model-00080-of-000163.safetensors', 'model-00081-of-000163.safetensors', 'model-00082-of-000163.safetensors', 'model-00083-of-000163.safetensors', 'model-00084-of-000163.safetensors', 'model-00085-of-000163.safetensors', 'model-00086-of-000163.safetensors', 'model-00087-of-000163.safetensors', 'model-00088-of-000163.safetensors', 'model-00089-of-000163.safetensors', 'model-00090-of-000163.safetensors', 'model-00091-of-000163.safetensors', 'model-00092-of-000163.safetensors', 'model-00093-of-000163.safetensors', 'model-00094-of-000163.safetensors', 'model-00095-of-000163.safetensors', 'model-00096-of-000163.safetensors', 'model-00097-of-000163.safetensors', 'model-00098-of-000163.safetensors', 'model-00099-of-000163.safetensors', 'model-00100-of-000163.safetensors', 'model-00101-of-000163.safetensors', 'model-00102-of-000163.safetensors', 'model-00103-of-000163.safetensors', 'model-00104-of-000163.safetensors', 'model-00105-of-000163.safetensors', 'model-00106-of-000163.safetensors', 'model-00107-of-000163.safetensors', 'model-00108-of-000163.safetensors', 'model-00109-of-000163.safetensors', 'model-00110-of-000163.safetensors', 'model-00111-of-000163.safetensors', 'model-00112-of-000163.safetensors', 'model-00113-of-000163.safetensors', 'model-00114-of-000163.safetensors', 'model-00115-of-000163.safetensors', 'model-00116-of-000163.safetensors', 'model-00117-of-000163.safetensors', 'model-00118-of-000163.safetensors', 'model-00119-of-000163.safetensors', 'model-00120-of-000163.safetensors', 'model-00121-of-000163.safetensors', 'model-00122-of-000163.safetensors', 'model-00123-of-000163.safetensors', 'model-00124-of-000163.safetensors', 'model-00125-of-000163.safetensors', 'model-00126-of-000163.safetensors', 'model-00127-of-000163.safetensors', 'model-00128-of-000163.safetensors', 'model-00129-of-000163.safetensors', 'model-00130-of-000163.safetensors', 'model-00131-of-000163.safetensors', 'model-00132-of-000163.safetensors', 'model-00133-of-000163.safetensors', 'model-00134-of-000163.safetensors', 'model-00135-of-000163.safetensors', 'model-00136-of-000163.safetensors', 'model-00137-of-000163.safetensors', 'model-00138-of-000163.safetensors', 'model-00139-of-000163.safetensors', 'model-00140-of-000163.safetensors', 'model-00141-of-000163.safetensors', 'model-00142-of-000163.safetensors', 'model-00143-of-000163.safetensors', 'model-00144-of-000163.safetensors', 'model-00145-of-000163.safetensors', 'model-00146-of-000163.safetensors', 'model-00147-of-000163.safetensors', 'model-00148-of-000163.safetensors', 'model-00149-of-000163.safetensors', 'model-00150-of-000163.safetensors', 'model-00151-of-000163.safetensors', 'model-00152-of-000163.safetensors', 'model-00153-of-000163.safetensors', 'model-00154-of-000163.safetensors', 'model-00155-of-000163.safetensors', 'model-00156-of-000163.safetensors', 'model-00157-of-000163.safetensors', 'model-00158-of-000163.safetensors', 'model-00159-of-000163.safetensors', 'model-00160-of-000163.safetensors', 'model-00161-of-000163.safetensors', 'model-00162-of-000163.safetensors', 'model-00163-of-000163.safetensors', 'model.safetensors.index.json', 'modeling_deepseek.py', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=671067257432, moe=True, model_type='deepseek_v3'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<User>' + message['content'] + '<Assistant>'}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<tooloutputsend>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- else %}{{message['content'] + '<toolcallsbegin><toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<toolcallbegin>' + tool['type'] + '<toolsep>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<toolcallend>'}}{%- endif %}{%- endfor %}{{'<toolcallsend><endofsentence>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<tooloutputsend>' + message['content'] + '<endofsentence>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{content + '<endofsentence>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<tooloutputsbegin><tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<tooloutputbegin>' + message['content'] + '<tooloutputend>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<tooloutputsend>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<Assistant>'}}{% endif %}"), context_length=163840, supports_tools=True, featured_priority=8, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-v3-0324-wn6kix4j', deployment='accounts/pyroworks/deployments/pwcfvbty', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-v3-0324-nr0yia1x', deployment='accounts/fireworks/deployments/jil95g6v', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-v3-0324-c8u01at4', deployment='accounts/pyroworks/deployments/p136ak3o', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-v3-0324-ezo8uype', deployment='accounts/pyroworks/deployments/xjtxwm6b', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-v3-0324-x9ug13v4', deployment='accounts/pyroworks/deployments/ta0n79cx', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/deepseek-v3-0324-tl85sfsd', deployment='accounts/pyroworks/deployments/xzhft032', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/deepseek-v3-0324-4cc44231', deployment='accounts/fireworks/deployments/fa78b78c', state=DeployedModelState.DEPLOYED, default=True, public=True)], calibrated=True, tunable=True, extra_deployment_args=['--stream-buffer=0', '--conversation-style=deepseek', '--num-draft-tokens=2', '--draft-model={"type":"mtp"}', '--moe-sharding=ep', '--attention-sharding=tensor_parallel_head_kv_seq_parallel'], update_time=datetime.datetime(2025, 4, 14, 1, 11, 10, 152585, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/dolphin-2-9-2-qwen2-72b', display_name='Dolphin 2.9.2 Qwen2 72B', create_time=datetime.datetime(2024, 6, 7, 20, 43, 6, 217944, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Dolphin 2.9.2 Qwen2 72B is a fine-tuned version of the Qwen2 72B Large Language Model with a variety of instruction, conversational, and coding skills. Also supports function calling.', hugging_face_url='https://huggingface.co/cognitivecomputations/dolphin-2.9.2-qwen2-72b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00031.safetensors', 'model-00002-of-00031.safetensors', 'model-00003-of-00031.safetensors', 'model-00004-of-00031.safetensors', 'model-00005-of-00031.safetensors', 'model-00006-of-00031.safetensors', 'model-00007-of-00031.safetensors', 'model-00008-of-00031.safetensors', 'model-00009-of-00031.safetensors', 'model-00010-of-00031.safetensors', 'model-00011-of-00031.safetensors', 'model-00012-of-00031.safetensors', 'model-00013-of-00031.safetensors', 'model-00014-of-00031.safetensors', 'model-00015-of-00031.safetensors', 'model-00016-of-00031.safetensors', 'model-00017-of-00031.safetensors', 'model-00018-of-00031.safetensors', 'model-00019-of-00031.safetensors', 'model-00020-of-00031.safetensors', 'model-00021-of-00031.safetensors', 'model-00022-of-00031.safetensors', 'model-00023-of-00031.safetensors', 'model-00024-of-00031.safetensors', 'model-00025-of-00031.safetensors', 'model-00026-of-00031.safetensors', 'model-00027-of-00031.safetensors', 'model-00028-of-00031.safetensors', 'model-00029-of-00031.safetensors', 'model-00030-of-00031.safetensors', 'model-00031-of-00031.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=72706203648, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"), context_length=32768, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/dolphin-2p6-mixtral-8x7b', display_name='Dolphin 2.6 Mixtral 8x7b', create_time=datetime.datetime(2024, 2, 29, 17, 56, 19, 590503, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Dolphin 2.6 Mixtral 8x7b is a fine-tuned version of the Mixtral-8x7b Large Language Model specializing in coding.', hugging_face_url='https://huggingface.co/cognitivecomputations/dolphin-2.6-mixtral-8x7b', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=46000000000, moe=True), conversation_config=ConversationConfig(style='chatml'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/eagle-llama-v3-8b-instruct-v1', display_name='EAGLE Llama 3 8B V1', create_time=datetime.datetime(2025, 4, 28, 23, 8, 8, 616318, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, description='EAGLE draft model for Llama 3.x 3B instruct models', base_model_details=BaseModelDetails(), created_by='divchenko@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 36, 77139, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/eagle-llama-v3-8b-instruct-v2', display_name='EAGLE Llama 3 8B V2', create_time=datetime.datetime(2025, 4, 28, 23, 8, 30, 915606, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, description='EAGLE draft model for Llama 3.x 3B instruct models', base_model_details=BaseModelDetails(), created_by='divchenko@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 36, 634885, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/eagle-qwen-v2p5-3b-instruct-v2', display_name='EAGLE Qwen 2.5 3B Instruct V2', create_time=datetime.datetime(2024, 12, 9, 23, 53, 41, 969984, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(), created_by='divchenko@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 37, 178869, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/firefunction-v1', display_name='FireFunction V1', create_time=datetime.datetime(2024, 2, 17, 0, 49, 26, 868123, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_TEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/mixtral-8x7b-instruct-hf', r=8, target_modules=['o_proj', 'gate', 'w1', 'w3', 'k_proj', 'w2', 'v_proj', 'q_proj'], base_model_type='mixtral'), public=True, description="Fireworks' open-source function calling model.", hugging_face_url='https://huggingface.co/fireworks-ai/firefunction-v1', base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), context_length=32768, supports_tools=True, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], update_time=datetime.datetime(2025, 5, 19, 22, 57, 5, 380001, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/firefunction-v2', display_name='FireFunction V2', create_time=datetime.datetime(2024, 8, 14, 7, 11, 49, 59752, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/llama-v3-70b-instruct-hf', r=4, target_modules=['o_proj', 'k_proj', 'q_proj', 'gate_proj', 'v_proj', 'down_proj', 'up_proj'], base_model_type='llama'), public=True, description="Fireworks' latest and most performant function-calling model. Firefunction-v2 is based on Llama-3 and trained to excel at function-calling as well as chat and instruction-following. See blog post for more details https://fireworks.ai/blog/firefunction-v2-launch-post", hugging_face_url='https://huggingface.co/fireworks-ai/llama-3-firefunction-v2', base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), supports_tools=True, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, update_time=datetime.datetime(2025, 5, 19, 23, 14, 50, 369316, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/firellava-13b', display_name='FireLLaVA 13B', create_time=datetime.datetime(2024, 1, 16, 19, 8, 41, 895776, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description="Fireworks' open-sourced LLaVA vision-language model trained on OSS LLM generated instruction following data. See more details on the blog post here: https://fireworks.ai/blog/firellava-the-first-commercially-permissive-oss-llava-model", hugging_face_url='https://huggingface.co/fireworks-ai/FireLLaVA-13b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), conversation_config=ConversationConfig(style='llava-chat'), context_length=4096, supports_image_input=True, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 37, 734755, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/firesearch-ocr-v6', display_name='Firesearch OCR V6', create_time=datetime.datetime(2024, 12, 11, 6, 11, 32, 710764, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json', 'configuration_phi3_v.py', 'generation_config.json', 'modeling_phi3_v.py', 'preprocessor_config.json', 'processing_phi3_v.py', 'processor_config.json', 'pytorch_model-00001-of-00004.bin', 'pytorch_model-00002-of-00004.bin', 'pytorch_model-00003-of-00004.bin', 'pytorch_model-00004-of-00004.bin', 'pytorch_model.bin.index.json', 'sample_inference.py', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=4448611328), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{{'<|' + message['role'] + '|>' + '\n' + message['content'] + '<|end|>\n' }}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{- '<|assistant|>\n' -}}{% endif %}"), context_length=131072, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/firesearch-ocr-v6-b939dfcf', deployment='accounts/fireworks/deployments/710892fc', state=DeployedModelState.DEPLOYED, default=True, public=True)], precisions=[DeploymentPrecision.FP8], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 38, 268890, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/flux-1-dev', display_name='FLUX.1 [dev]', create_time=datetime.datetime(2024, 10, 22, 3, 58, 32, 977120, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/black-forest-labs/FLUX.1-dev', base_model_details=BaseModelDetails(world_size=1), created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 5, 8, 0, 49, 38, 826408, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/flux-1-dev-controlnet-union', display_name='FLUX.1 [dev] ControlNet', create_time=datetime.datetime(2024, 10, 2, 17, 42, 8, 114780, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/flux-1-dev'), public=True, hugging_face_url='https://huggingface.co/black-forest-labs/FLUX.1-dev', base_model_details=BaseModelDetails(), created_by='james@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 39, 354652, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/flux-1-dev-fp8', display_name='FLUX.1 [dev] FP8', create_time=datetime.datetime(2024, 10, 21, 17, 48, 9, 99702, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/black-forest-labs/FLUX.1-dev', base_model_details=BaseModelDetails(world_size=1), featured_priority=5, created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/flux-1-dev-fp8-1fe2ee16', deployment='accounts/fireworks/deployments/fd7b206c', state=DeployedModelState.DEPLOYED, default=True, public=True)], update_time=datetime.datetime(2025, 5, 8, 0, 49, 39, 920278, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/flux-1-dev-midjourney-anime', create_time=datetime.datetime(2024, 10, 22, 1, 33, 11, 504320, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/flux-1-dev'), base_model_details=BaseModelDetails(), created_by='james@fireworks.ai', default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/flux-1-schnell', display_name='FLUX.1 [schnell]', create_time=datetime.datetime(2024, 10, 22, 4, 6, 35, 365683, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/black-forest-labs/FLUX.1-schnell', base_model_details=BaseModelDetails(world_size=1), created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 5, 8, 0, 49, 40, 455627, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/flux-1-schnell-fp8', display_name='FLUX.1 [schnell] FP8', create_time=datetime.datetime(2024, 10, 21, 18, 29, 36, 399374, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/black-forest-labs/FLUX.1-schnell', base_model_details=BaseModelDetails(world_size=1), featured_priority=3, created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/flux-1-schnell-fp8-086f0b25', deployment='accounts/fireworks/deployments/7618a08b', state=DeployedModelState.DEPLOYED, default=True, public=True)], update_time=datetime.datetime(2025, 5, 8, 0, 49, 41, 7764, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/fs-ocr-pawel-phi35-v5-1', create_time=datetime.datetime(2024, 12, 2, 2, 21, 15, 739769, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/phi-3-vision-128k-instruct', r=8, target_modules=['^(?!.*\\bvision_embed_tokens\\b).*(o_proj|qkv_proj|down_proj|gate_up_proj)$']), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), created_by='pawel@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], update_time=datetime.datetime(2025, 4, 25, 17, 47, 31, 627702, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/fs-ocr-webster-phi3p5-v1', create_time=datetime.datetime(2024, 12, 10, 20, 50, 59, 761419, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/phi-3-vision-128k-instruct', r=16, target_modules=['mlp.gate_up_proj', 'self_attn.o_proj', 'self_attn.qkv_proj', 'mlp.down_proj']), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], update_time=datetime.datetime(2025, 4, 25, 17, 46, 30, 212766, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/full-llama-v3p1-8b-instruct-8b-fp8', display_name='Llama 3.1 8B Instruct FP8 [Full]', create_time=datetime.datetime(2024, 7, 29, 4, 15, 1, 905581, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct', base_model_details=BaseModelDetails(), created_by='dzhulgakov@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 41, 551523, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/full-llama-v3p1-8b-instruct-8b-fp8-amd', display_name='Llama 3.1 8B Instruct FP8 AMD [Full]', create_time=datetime.datetime(2024, 8, 15, 0, 13, 59, 961131, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct', base_model_details=BaseModelDetails(), created_by='divchenko@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 42, 87649, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/gemma2-9b-it', display_name='Gemma 2 9B Instruct', create_time=datetime.datetime(2024, 6, 28, 0, 26, 24, 307550, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Gemma 2 9B Instruct is the instruction-tuned version of Gemma 2 9B and has the chat completions API enabled.', hugging_face_url='https://huggingface.co/google/gemma-2-9b-it', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=9000000000, model_type='gemma2'), conversation_config=ConversationConfig(style='jinja'), context_length=8192, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/gemma-2b-it', display_name='Gemma 2B Instruct', create_time=datetime.datetime(2024, 4, 30, 21, 41, 7, 192504, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.', hugging_face_url='https://huggingface.co/google/gemma-2b-it', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'gemma-2b-it.gguf', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=2000000000, model_type='gemma'), conversation_config=ConversationConfig(style='jinja', template="{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"), context_length=8192, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/gemma-3-27b-it', display_name='Gemma 3 27B Instruct', create_time=datetime.datetime(2025, 4, 15, 20, 19, 19, 459140, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Gemma 3 27B Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1498482688, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now("%d %b %Y") %}\n    {%- else %}\n        {%- set date_string = "26 Jul 2024" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n        {{- \'{"name": "\' + tool_call.name + \'", \' }}\n        {{- \'"parameters": \' }}\n        {{- tool_call.arguments | tojson }}\n        {{- "}" }}\n        {{- "<|eot_id|>" }}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/alex-b45656/deployedModels/gemma-3-27b-it-v8i1z2ok', deployment='accounts/alex-b45656/deployments/ciezgt9b', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/jsdelfino-14e8b4/deployedModels/gemma-3-27b-it-idjkw6o2', deployment='accounts/jsdelfino-14e8b4/deployments/kdyuar7t', state=DeployedModelState.DEPLOYED)], tunable=True, update_time=datetime.datetime(2025, 4, 17, 20, 32, 9, 768586, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 64.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/gemma-7b', display_name='Gemma 7B', create_time=datetime.datetime(2024, 3, 1, 21, 56, 10, 588013, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.', hugging_face_url='https://huggingface.co/google/gemma-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, model_type='gemma'), context_length=8192, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/gemma-7b-it', display_name='Gemma 7B Instruct', create_time=datetime.datetime(2024, 2, 22, 22, 46, 55, 394131, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.', hugging_face_url='https://huggingface.co/google/gemma-7b-it', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=7000000000, model_type='gemma'), conversation_config=ConversationConfig(style='jinja'), context_length=8192, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/gemma-7b-it-yo06phwt', deployment='accounts/pyroworks/deployments/z3o7560t', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/hermes-2-pro-mistral-7b', display_name='Hermes 2 Pro Mistral 7B', create_time=datetime.datetime(2024, 3, 13, 21, 50, 13, 173275, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Latest version of Nous Research's Hermes series of models, using an updated and cleaned version of the Hermes 2 dataset, and is now trained on a diverse and rich set of function calling and JSON mode samples", hugging_face_url='https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'fireworks.yaml', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, model_type='mistral'), conversation_config=ConversationConfig(style='jinja'), supports_tools=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama4-maverick-instruct', display_name='Llama 4 Maverick Instruct', create_time=datetime.datetime(2025, 4, 5, 17, 51, 6, 598889, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), description='The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Maverick is a 17 billion parameter model with 128 experts.', hugging_face_url='https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['chat_template.json', 'config.json', 'generation_config.json', 'model.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'processor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=400711848960, moe=True, model_type='llama4_text'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now("%d %b %Y") %}\n    {%- else %}\n        {%- set date_string = "26 Jul 2024" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = "" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- Find out if there are any images #}\n{% set image_ns = namespace(has_images=false) %}      \n{%- for message in messages %}\n    {%- for content in message[\'content\'] %}\n        {%- if content[\'type\'] == \'image\' %}\n            {%- set image_ns.has_images = true %}\n        {%- endif %}\n    {%- endfor %}\n{%- endfor %}\n\n{#- System message if the user supplied one #}\n{%- if user_supplied_system_message %}\n    {{- "<|header_start|>system<|header_end|>\\n\\n" }}\n    {%- if tools is not none %}\n        {{- "Environment: ipython\\n" }}\n    {%- endif %}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n        {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n        {{- "Do not use variables.\\n\\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\\n\\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- "<|eot|>" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|header_start|>user<|header_end|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n    {{- \'<|header_start|>\' + message[\'role\'] + \'<|header_end|>\\n\\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- elif \'tool_calls\' in message and message.tool_calls|length > 0 %}\n       {{- \'<|header_start|>assistant<|header_end|>\\n\\n\' -}}\n       {{- \'<|python_start|>\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n       {{- \'<|python_end|>\' }}\n        {%- for tool_call in message.tool_calls %}\n           {{- \'{"name": "\' + tool_call.function.name + \'", \' }}\n           {{- \'"parameters": \' }}\n           {{- tool_call.function.arguments | tojson }}\n           {{- "}" }}\n        {%- endfor %}\n       {{- "<|eot|>" }}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|header_start|>ipython<|header_end|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|header_start|>assistant<|header_end|>\n\n\' }}\n{%- endif %}\n'), context_length=1048576, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=2), unit='1M tokens')], calibrated=True, update_time=datetime.datetime(2025, 4, 5, 20, 47, 10, 790630, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama4-maverick-instruct-basic', display_name='Llama 4 Maverick Instruct (Basic)', create_time=datetime.datetime(2025, 4, 5, 18, 41, 35, 522738, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.', hugging_face_url='https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00055.safetensors', 'model-00002-of-00055.safetensors', 'model-00003-of-00055.safetensors', 'model-00004-of-00055.safetensors', 'model-00005-of-00055.safetensors', 'model-00006-of-00055.safetensors', 'model-00007-of-00055.safetensors', 'model-00008-of-00055.safetensors', 'model-00009-of-00055.safetensors', 'model-00010-of-00055.safetensors', 'model-00011-of-00055.safetensors', 'model-00012-of-00055.safetensors', 'model-00013-of-00055.safetensors', 'model-00014-of-00055.safetensors', 'model-00015-of-00055.safetensors', 'model-00016-of-00055.safetensors', 'model-00017-of-00055.safetensors', 'model-00018-of-00055.safetensors', 'model-00019-of-00055.safetensors', 'model-00020-of-00055.safetensors', 'model-00021-of-00055.safetensors', 'model-00022-of-00055.safetensors', 'model-00023-of-00055.safetensors', 'model-00024-of-00055.safetensors', 'model-00025-of-00055.safetensors', 'model-00026-of-00055.safetensors', 'model-00027-of-00055.safetensors', 'model-00028-of-00055.safetensors', 'model-00029-of-00055.safetensors', 'model-00030-of-00055.safetensors', 'model-00031-of-00055.safetensors', 'model-00032-of-00055.safetensors', 'model-00033-of-00055.safetensors', 'model-00034-of-00055.safetensors', 'model-00035-of-00055.safetensors', 'model-00036-of-00055.safetensors', 'model-00037-of-00055.safetensors', 'model-00038-of-00055.safetensors', 'model-00039-of-00055.safetensors', 'model-00040-of-00055.safetensors', 'model-00041-of-00055.safetensors', 'model-00042-of-00055.safetensors', 'model-00043-of-00055.safetensors', 'model-00044-of-00055.safetensors', 'model-00045-of-00055.safetensors', 'model-00046-of-00055.safetensors', 'model-00047-of-00055.safetensors', 'model-00048-of-00055.safetensors', 'model-00049-of-00055.safetensors', 'model-00050-of-00055.safetensors', 'model-00051-of-00055.safetensors', 'model-00052-of-00055.safetensors', 'model-00053-of-00055.safetensors', 'model-00054-of-00055.safetensors', 'model-00055-of-00055.safetensors', 'model.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'processor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=400711848960, model_type='llama4_text'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now("%d %b %Y") %}\n    {%- else %}\n        {%- set date_string = "26 Jul 2024" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = "" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- Find out if there are any images #}\n{% set image_ns = namespace(has_images=false) %}      \n{%- for message in messages %}\n    {%- for content in message[\'content\'] %}\n        {%- if content[\'type\'] == \'image\' %}\n            {%- set image_ns.has_images = true %}\n        {%- endif %}\n    {%- endfor %}\n{%- endfor %}\n\n{#- System message if the user supplied one #}\n{%- if user_supplied_system_message %}\n    {{- "<|header_start|>system<|header_end|>\\n\\n" }}\n    {%- if tools is not none %}\n        {{- "Environment: ipython\\n" }}\n    {%- endif %}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n        {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n        {{- "Do not use variables.\\n\\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\\n\\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- "<|eot|>" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|header_start|>user<|header_end|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n    {{- \'<|header_start|>\' + message[\'role\'] + \'<|header_end|>\\n\\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- elif \'tool_calls\' in message and message.tool_calls|length > 0 %}\n       {{- \'<|header_start|>assistant<|header_end|>\\n\\n\' -}}\n       {{- \'<|python_start|>\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n       {{- \'<|python_end|>\' }}\n        {%- for tool_call in message.tool_calls %}\n           {{- \'{"name": "\' + tool_call.function.name + \'", \' }}\n           {{- \'"parameters": \' }}\n           {{- tool_call.function.arguments | tojson }}\n           {{- "}" }}\n        {%- endfor %}\n       {{- "<|eot|>" }}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|header_start|>ipython<|header_end|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|header_start|>assistant<|header_end|>\n\n\' }}\n{%- endif %}\n'), context_length=1048576, supports_image_input=True, supports_tools=True, featured_priority=12, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=220000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=880000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama4-maverick-instruct-basic-708d92ba', deployment='accounts/fireworks/deployments/ee9723b6', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/fireworks/deployedModels/llama4-maverick-instruct-basic-mz6lrl91', deployment='accounts/fireworks/deployments/owxm4awa', state=DeployedModelState.DEPLOYED)], calibrated=True, use_hf_apply_chat_template=True, extra_deployment_args=['--moe-sharding=ep', '--max-seq-len=131072', '--conversation-style=llama4'], update_time=datetime.datetime(2025, 4, 27, 22, 59, 19, 284675, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama4-maverick-instruct-basic-eagle3-v0', display_name='llama4-maverick-instruct-basic-eagle3-v0', create_time=datetime.datetime(2025, 4, 13, 19, 59, 50, 576393, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), description='llama4-maverick-instruct-basic-eagle3-v0', base_model_details=BaseModelDetails(), created_by='beiyijie@fireworks.ai', update_time=datetime.datetime(2025, 4, 13, 20, 2, 34, 528419, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama4-scout-instruct', create_time=datetime.datetime(2025, 4, 5, 17, 50, 34, 668648, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), description='The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Scout is a 17 billion parameter model with 16 experts.', hugging_face_url='https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00050.safetensors', 'model-00002-of-00050.safetensors', 'model-00003-of-00050.safetensors', 'model-00004-of-00050.safetensors', 'model-00005-of-00050.safetensors', 'model-00006-of-00050.safetensors', 'model-00007-of-00050.safetensors', 'model-00008-of-00050.safetensors', 'model-00009-of-00050.safetensors', 'model-00010-of-00050.safetensors', 'model-00011-of-00050.safetensors', 'model-00012-of-00050.safetensors', 'model-00013-of-00050.safetensors', 'model-00014-of-00050.safetensors', 'model-00015-of-00050.safetensors', 'model-00016-of-00050.safetensors', 'model-00017-of-00050.safetensors', 'model-00018-of-00050.safetensors', 'model-00019-of-00050.safetensors', 'model-00020-of-00050.safetensors', 'model-00021-of-00050.safetensors', 'model-00022-of-00050.safetensors', 'model-00023-of-00050.safetensors', 'model-00024-of-00050.safetensors', 'model-00025-of-00050.safetensors', 'model-00026-of-00050.safetensors', 'model-00027-of-00050.safetensors', 'model-00028-of-00050.safetensors', 'model-00029-of-00050.safetensors', 'model-00030-of-00050.safetensors', 'model-00031-of-00050.safetensors', 'model-00032-of-00050.safetensors', 'model-00033-of-00050.safetensors', 'model-00034-of-00050.safetensors', 'model-00035-of-00050.safetensors', 'model-00036-of-00050.safetensors', 'model-00037-of-00050.safetensors', 'model-00038-of-00050.safetensors', 'model-00039-of-00050.safetensors', 'model-00040-of-00050.safetensors', 'model-00041-of-00050.safetensors', 'model-00042-of-00050.safetensors', 'model-00043-of-00050.safetensors', 'model-00044-of-00050.safetensors', 'model-00045-of-00050.safetensors', 'model-00046-of-00050.safetensors', 'model-00047-of-00050.safetensors', 'model-00048-of-00050.safetensors', 'model-00049-of-00050.safetensors', 'model-00050-of-00050.safetensors', 'model.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'processor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=107769861120, moe=True, model_type='llama4_text'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now("%d %b %Y") %}\n    {%- else %}\n        {%- set date_string = "26 Jul 2024" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}    \n    {%- if messages[0][\'content\'] is string %}\n        {%- set system_message = messages[0][\'content\']|trim %}\n    {%- else %}\n        {#- FIXME: The processor requires an array, always. #}\n        {%- set system_message = messages[0][\'content\'][0][\'text\']|trim %}\n    {%- endif %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = "" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- System message if the user supplied one #}\n{%- if user_supplied_system_message %}\n    {{- "<|header_start|>system<|header_end|>\n\n" }}\n    {%- if tools is not none %}\n        {{- "Environment: ipython\n" }}\n    {%- endif %}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n        {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n        {{- "Do not use variables.\n\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\n\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- "<|eot|>" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|header_start|>user<|header_end|>\n\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\n\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\n\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\n\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n    {{- \'<|header_start|>\' + message[\'role\'] + \'<|header_end|>\n\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- elif \'tool_calls\' in message and message.tool_calls|length > 0 %}\n       {{- \'<|header_start|>assistant<|header_end|>\n\n\' -}}\n       {{- \'<|python_start|>\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n       {{- \'<|python_end|>\' }}\n        {%- for tool_call in message.tool_calls %}\n           {{- \'{"name": "\' + tool_call.function.name + \'", \' }}\n           {{- \'"parameters": \' }}\n           {{- tool_call.function.arguments | tojson }}\n           {{- "}" }}\n        {%- endfor %}\n       {{- "<|eot|>" }}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|header_start|>ipython<|header_end|>\n\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|header_start|>assistant<|header_end|>\n\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=2), unit='1M tokens')], calibrated=True, extra_deployment_args=['--moe-sharding=ep'], update_time=datetime.datetime(2025, 4, 5, 20, 46, 54, 792254, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama4-scout-instruct-basic', display_name='Llama 4 Scout Instruct (Basic)', create_time=datetime.datetime(2025, 4, 5, 18, 37, 59, 158543, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.', hugging_face_url='https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00050.safetensors', 'model-00002-of-00050.safetensors', 'model-00003-of-00050.safetensors', 'model-00004-of-00050.safetensors', 'model-00005-of-00050.safetensors', 'model-00006-of-00050.safetensors', 'model-00007-of-00050.safetensors', 'model-00008-of-00050.safetensors', 'model-00009-of-00050.safetensors', 'model-00010-of-00050.safetensors', 'model-00011-of-00050.safetensors', 'model-00012-of-00050.safetensors', 'model-00013-of-00050.safetensors', 'model-00014-of-00050.safetensors', 'model-00015-of-00050.safetensors', 'model-00016-of-00050.safetensors', 'model-00017-of-00050.safetensors', 'model-00018-of-00050.safetensors', 'model-00019-of-00050.safetensors', 'model-00020-of-00050.safetensors', 'model-00021-of-00050.safetensors', 'model-00022-of-00050.safetensors', 'model-00023-of-00050.safetensors', 'model-00024-of-00050.safetensors', 'model-00025-of-00050.safetensors', 'model-00026-of-00050.safetensors', 'model-00027-of-00050.safetensors', 'model-00028-of-00050.safetensors', 'model-00029-of-00050.safetensors', 'model-00030-of-00050.safetensors', 'model-00031-of-00050.safetensors', 'model-00032-of-00050.safetensors', 'model-00033-of-00050.safetensors', 'model-00034-of-00050.safetensors', 'model-00035-of-00050.safetensors', 'model-00036-of-00050.safetensors', 'model-00037-of-00050.safetensors', 'model-00038-of-00050.safetensors', 'model-00039-of-00050.safetensors', 'model-00040-of-00050.safetensors', 'model-00041-of-00050.safetensors', 'model-00042-of-00050.safetensors', 'model-00043-of-00050.safetensors', 'model-00044-of-00050.safetensors', 'model-00045-of-00050.safetensors', 'model-00046-of-00050.safetensors', 'model-00047-of-00050.safetensors', 'model-00048-of-00050.safetensors', 'model-00049-of-00050.safetensors', 'model-00050-of-00050.safetensors', 'model.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'processor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=107769861120, model_type='llama4_text'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now("%d %b %Y") %}\n    {%- else %}\n        {%- set date_string = "26 Jul 2024" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}    \n    {%- if messages[0][\'content\'] is string %}\n        {%- set system_message = messages[0][\'content\']|trim %}\n    {%- else %}\n        {#- FIXME: The processor requires an array, always. #}\n        {%- set system_message = messages[0][\'content\'][0][\'text\']|trim %}\n    {%- endif %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = "" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- System message if the user supplied one #}\n{%- if user_supplied_system_message %}\n    {{- "<|header_start|>system<|header_end|>\n\n" }}\n    {%- if tools is not none %}\n        {{- "Environment: ipython\n" }}\n    {%- endif %}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n        {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n        {{- "Do not use variables.\n\n" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- "\n\n" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- "<|eot|>" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|header_start|>user<|header_end|>\n\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\n\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\n\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\n\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n    {{- \'<|header_start|>\' + message[\'role\'] + \'<|header_end|>\n\n\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- elif \'tool_calls\' in message and message.tool_calls|length > 0 %}\n       {{- \'<|header_start|>assistant<|header_end|>\n\n\' -}}\n       {{- \'<|python_start|>\' }}\n        {%- if message[\'content\'] is string %}\n            {{- message[\'content\'] }}\n        {%- else %}\n            {%- for content in message[\'content\'] %}\n                {%- if content[\'type\'] == \'image\' %}\n                    {{- \'<|image|>\' }}\n                {%- elif content[\'type\'] == \'text\' %}\n                    {{- content[\'text\'] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n       {{- \'<|python_end|>\' }}\n        {%- for tool_call in message.tool_calls %}\n           {{- \'{"name": "\' + tool_call.function.name + \'", \' }}\n           {{- \'"parameters": \' }}\n           {{- tool_call.function.arguments | tojson }}\n           {{- "}" }}\n        {%- endfor %}\n       {{- "<|eot|>" }}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|header_start|>ipython<|header_end|>\n\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|header_start|>assistant<|header_end|>\n\n\' }}\n{%- endif %}\n'), context_length=1048576, supports_image_input=True, supports_tools=True, featured_priority=11, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=150000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=600000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama4-scout-instruct-basic-0a2fe2a4', deployment='accounts/fireworks/deployments/97aca999', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/fireworks/deployedModels/llama4-scout-instruct-basic-qspycr68', deployment='accounts/fireworks/deployments/ftao44ue', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama4-scout-instruct-basic-twq4111j', deployment='accounts/pyroworks/deployments/wtgrq505', state=DeployedModelState.DEPLOYED)], calibrated=True, use_hf_apply_chat_template=True, update_time=datetime.datetime(2025, 4, 27, 22, 59, 27, 99228, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama4-scout-instruct-basic-eagle3-v0', display_name='llama4-scout-instruct-basic-eagle3-v0', create_time=datetime.datetime(2025, 4, 21, 20, 50, 38, 905507, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), description='llama4-scout-instruct-basic-eagle3-v0', base_model_details=BaseModelDetails(), created_by='beiyijie@fireworks.ai', update_time=datetime.datetime(2025, 4, 21, 20, 53, 22, 375776, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/llama-guard-2-8b', display_name='Llama Guard v2 8B', create_time=datetime.datetime(2024, 4, 18, 20, 52, 10, 239390, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Meta Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM  it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.', hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=8000000000, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if not message.get('content') -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}"), context_length=8192, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-guard-3-1b', display_name='Llama Guard v3 1B', create_time=datetime.datetime(2024, 9, 25, 16, 3, 59, 742240, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Llama-Guard-3-1B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1498482688, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{%- if messages|length % 2 == 0 -%}\n    {%- set role = \'Agent\' -%}\n{%-  else -%}\n    {%-  set role = \'User\' -%}\n{%-  endif -%}\n{%-  if categories is not defined -%}\n    {%-  \n        set categories = ({\n            "S1": "Violent Crimes.",\n            "S2": "Non-Violent Crimes.",\n            "S3": "Sex Crimes.",\n            "S4": "Child Exploitation.",\n            "S5": "Defamation.",\n            "S6": "Specialized Advice.",\n            "S7": "Privacy.",\n            "S8": "Intellectual Property.",\n            "S9": "Indiscriminate Weapons.",\n            "S10": "Hate.",\n            "S11": "Self-Harm.",\n            "S12": "Sexual Content.",\n            "S13": "Elections.",\n        })\n    -%}\n{%-  endif -%}\n\n{{- "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n" -}}\n{%- set ns = namespace(contains_image=false) -%}\n{%-  for message in messages -%}\n    {%- for content in message[\'content\'] | selectattr(\'type\', \'equalto\', \'image\') -%}\n        {%- set ns.contains_image = true -%}\n    {%- endfor -%}\n{%- endfor -%}\n{%- if ns.contains_image -%}\n{{- "<|image|> "-}}\n{%- endif -%}\n{{- "Task: Check if there is unsafe content in \'" + role + "\' messages in conversations according our safety policy with the below categories.\\n\\n" -}}\n{{- "<BEGIN UNSAFE CONTENT CATEGORIES>\\n"-}}\n\n{%-  for key in categories -%}\n    {%-  if key not in excluded_category_keys -%}\n        {{-key + ": " + categories[key] + "\\n" -}}\n    {%-  endif -%}\n{%-  endfor -%}\n\n{{- "<END UNSAFE CONTENT CATEGORIES>\\n\\n"-}}\n{{- "<BEGIN CONVERSATION>\\n\\n"-}}\n{%-  for message in messages -%}\n    {%-  if (message[\'role\'] == \'user\') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\'Conversation roles must alternate user/assistant/user/assistant/...\')-}}\n    {%-  endif -%}\n    {%-  set content = message[\'content\'] -%}\n    {%-  if message[\'role\'] == \'user\' -%}\n        {%-  set role = \'User\' -%}\n    {%-  elif message[\'role\'] == \'assistant\' -%}\n        {%-  set role = \'Agent\' -%}\n    {%-  endif -%}\n    {%-  for content in message[\'content\'] | selectattr(\'type\', \'equalto\', \'text\') -%}\n{{- role + \': \' + content[\'text\'] | trim + \'\\n\\n\'-}}\n    {%-  endfor -%}\n{%-  endfor -%}\n{{- "<END CONVERSATION>\\n\\n"-}}\n{{- "Provide your safety assessment for ONLY THE LAST " + role + " message in the above conversation:\\n"-}}\n{{- " - First line must read \'safe\' or \'unsafe\'.\\n"-}}\n{{- " - If unsafe, a second line must include a comma-separated list of violated categories. <|eot_id|><|start_header_id|>assistant<|end_header_id|>"-}}\n'), context_length=131072, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-guard-3-8b', display_name='Llama Guard 3 8B', create_time=datetime.datetime(2024, 11, 5, 21, 3, 47, 654102, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Llama-Guard-3-8B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'llama_guard_3_figure.png', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=8030261248, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{% if messages|length % 2 == 0 %}{% set role = \'Agent\' %}{% else %}{% set role = \'User\' %}{% endif %}{{ " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nTask: Check if there is unsafe content in \'" + role + "\' messages in conversations according our safety policy with the below categories.\n\n<BEGIN UNSAFE CONTENT CATEGORIES>\nS1: Violent Crimes.\nS2: Non-Violent Crimes.\nS3: Sex Crimes.\nS4: Child Exploitation.\nS5: Defamation.\nS6: Specialized Advice.\nS7: Privacy.\nS8: Intellectual Property.\nS9: Indiscriminate Weapons.\nS10: Hate.\nS11: Self-Harm.\nS12: Sexual Content.\nS13: Elections.\nS14: Code Interpreter Abuse.\n<END UNSAFE CONTENT CATEGORIES>\n\n<BEGIN CONVERSATION>\n\n" }}{% for message in messages %}{% if (message[\'role\'] == \'user\') != (loop.index0 % 2 == 0) %}{{ raise_exception(\'Conversation roles must alternate user/assistant/user/assistant/...\') }}{% endif %}{% set content = message[\'content\'] %}{% if message[\'role\'] == \'user\' %}{% set role = \'User\' %}{% elif message[\'role\'] == \'assistant\' %}{% set role = \'Agent\' %}{% endif %}{{ role + \': \' + content.strip() + \'\n\n\' }}{% endfor %}{{ "<END CONVERSATION>\n\nProvide your safety assessment for ONLY THE LAST " + role + " message in the above conversation:\n - First line must read \'safe\' or \'unsafe\'.\n - If unsafe, a second line must include a comma-separated list of violated categories.<|eot_id|><|start_header_id|>assistant<|end_header_id|>" }}'), context_length=131072, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-guard-3-8b-9c624847', deployment='accounts/fireworks/deployments/f913b246', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-guard-3-8b-wrwovg31', deployment='accounts/pyroworks/deployments/noi4mnq8', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-guard-3-8b-wawt6foe', deployment='accounts/pyroworks/deployments/b77mbsg5', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 42, 613016, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/llamaguard-7b', display_name='Llama Guard 7B', create_time=datetime.datetime(2023, 12, 12, 0, 40, 4, 429008, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM: it generates text in its output that indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories.', github_url='https://github.com/facebookresearch/PurpleLlama', hugging_face_url='https://huggingface.co/meta-llama/LlamaGuard-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'config.json', 'consolidate_params.json', 'consolidated.00-001.pth', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'params.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v2-13b', display_name='Llama 2 13B', create_time=datetime.datetime(2023, 8, 12, 19, 40, 47, 970674, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), peft_details=PeftDetails(), public=True, description='Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the 13B pretrained model.', github_url='https://github.com/facebookresearch/llama', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-13b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=13000000000, tunable=True, model_type='llama'), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama-v2-13b-chat', display_name='Llama 2 13B Chat', create_time=datetime.datetime(2023, 8, 12, 19, 34, 10, 764671, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), peft_details=PeftDetails(), public=True, description='Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the 13B fine-tuned model, optimized for dialogue use cases.', github_url='https://github.com/facebookresearch/llama', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-13b-chat', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=13000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + '<<SYS>>\\n' + ns.system_prompt + '\\n<</SYS>>\\n\\n' + message['content'] + ' [/INST] ' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST] ' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v2-70b', display_name='Llama 2 70B', create_time=datetime.datetime(2024, 1, 3, 2, 20, 58, 761242, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-70b', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=1, tunable=True, model_type='llama'), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama-v2-70b-chat', display_name='Llama 2 70B Chat', create_time=datetime.datetime(2023, 8, 12, 19, 43, 27, 787882, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), peft_details=PeftDetails(), public=True, description='Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the 70B fine-tuned model, optimized for dialogue use cases.', github_url='https://github.com/facebookresearch/llama', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-70b-chat', base_model_details=BaseModelDetails(world_size=8, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + '<<SYS>>\\n' + ns.system_prompt + '\\n<</SYS>>\\n\\n' + message['content'] + ' [/INST] ' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST] ' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.8999999761581421, 'top_k': 50.0, 'top_p': 0.6000000238418579}),
Model(name='accounts/fireworks/models/llama-v2-7b', display_name='Llama 2 7B', create_time=datetime.datetime(2023, 8, 12, 19, 45, 25, 752908, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), peft_details=PeftDetails(), public=True, description="Meta's Llama 2 model family is a collection of pretrained and fine-tuned generative text models trained on 40% more data than Llama 1 with double the context length. This is the 7B Base version trained on 2 trillion tokens with a context length of 4096.", github_url='https://github.com/facebookresearch/llama', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=7000000000, tunable=True, model_type='llama'), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v2-7b-chat', display_name='Llama 2 7B Chat', create_time=datetime.datetime(2023, 8, 12, 19, 44, 38, 553832, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), peft_details=PeftDetails(), public=True, description='Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the 7B fine-tuned model, optimized for dialogue use cases.', github_url='https://github.com/facebookresearch/llama', hugging_face_url='https://huggingface.co/meta-llama/Llama-2-7b-chat', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=7000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + '<<SYS>>\\n' + ns.system_prompt + '\\n<</SYS>>\\n\\n' + message['content'] + ' [/INST] ' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST] ' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pablo-c6721c/deployedModels/llama-v2-7b-chat-h52xjtqg', deployment='accounts/pablo-c6721c/deployments/fl8sj7t9', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3-70b-instruct', display_name='Llama 3 70B Instruct', create_time=datetime.datetime(2024, 4, 18, 17, 43, 40, 631630, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.', github_url='https://github.com/meta-llama/llama3', hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000, model_type='llama'), conversation_config=ConversationConfig(style='jinja'), context_length=8192, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], default_draft_model='accounts/fireworks/models/llama-v3-70b-instruct-v2', default_draft_token_count=2, precisions=[DeploymentPrecision.FP8], calibrated=True, tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3-70b-instruct-hf', display_name='Llama 3 70B Instruct (HF version)', create_time=datetime.datetime(2024, 4, 21, 3, 33, 52, 297689, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Metas Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Llama 3 70B Instruct (HF Version) is the original, FP16 version of Llama 3 70B Instruct whose results should be consistent with the official Hugging Face implementation.', hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=8192, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], default_draft_model='accounts/fireworks/models/llama-v3-70b-instruct-v2', default_draft_token_count=1, precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3-70b-instruct-v2', display_name='Llama v3 70B Instruct V2 Draft Model', create_time=datetime.datetime(2024, 5, 2, 20, 51, 55, 595053, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(), created_by='beiyijie@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 50, 10, 201345, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3-8b', display_name='Llama 3 8B', create_time=datetime.datetime(2024, 11, 21, 17, 57, 41, 587530, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.', github_url='https://github.com/meta-llama/llama3', hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-8B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=8030261248, model_type='llama'), context_length=8192, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama-v3-8b-instruct', display_name='Llama 3 8B Instruct', create_time=datetime.datetime(2024, 4, 18, 20, 29, 41, 898013, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.', github_url='https://github.com/meta-llama/llama3', hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=8000000000, model_type='llama'), conversation_config=ConversationConfig(style='jinja'), context_length=8192, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama-v3-8b-instruct-hf', display_name='Llama 3 8B Instruct (HF version)', create_time=datetime.datetime(2024, 4, 18, 21, 1, 43, 181110, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Meta's Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Llama 3 8B Instruct (HF Version) is the original, FP16 version of Llama 3 8B Instruct whose results should be consistent with the official Hugging Face implementation.", hugging_face_url='https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=8000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=8192, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], default_draft_model='accounts/fireworks/models/llama-v3-8b-instruct-v0', default_draft_token_count=2, precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/llama-v3-8b-instruct-v0', display_name='Llama v3 8B Instruct V0 Draft Model', create_time=datetime.datetime(2024, 4, 25, 17, 53, 11, 147906, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(), created_by='beiyijie@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 50, 10, 834985, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-405b-instruct', display_name='Llama 3.1 405B Instruct', create_time=datetime.datetime(2024, 7, 19, 22, 33, 6, 251447, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes. The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. 405B model is the most capable from the Llama 3.1 family. This model is served in FP8 closely matching reference implementation.', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00191.safetensors', 'model-00002-of-00191.safetensors', 'model-00003-of-00191.safetensors', 'model-00004-of-00191.safetensors', 'model-00005-of-00191.safetensors', 'model-00006-of-00191.safetensors', 'model-00007-of-00191.safetensors', 'model-00008-of-00191.safetensors', 'model-00009-of-00191.safetensors', 'model-00010-of-00191.safetensors', 'model-00011-of-00191.safetensors', 'model-00012-of-00191.safetensors', 'model-00013-of-00191.safetensors', 'model-00014-of-00191.safetensors', 'model-00015-of-00191.safetensors', 'model-00016-of-00191.safetensors', 'model-00017-of-00191.safetensors', 'model-00018-of-00191.safetensors', 'model-00019-of-00191.safetensors', 'model-00020-of-00191.safetensors', 'model-00021-of-00191.safetensors', 'model-00022-of-00191.safetensors', 'model-00023-of-00191.safetensors', 'model-00024-of-00191.safetensors', 'model-00025-of-00191.safetensors', 'model-00026-of-00191.safetensors', 'model-00027-of-00191.safetensors', 'model-00028-of-00191.safetensors', 'model-00029-of-00191.safetensors', 'model-00030-of-00191.safetensors', 'model-00031-of-00191.safetensors', 'model-00032-of-00191.safetensors', 'model-00033-of-00191.safetensors', 'model-00034-of-00191.safetensors', 'model-00035-of-00191.safetensors', 'model-00036-of-00191.safetensors', 'model-00037-of-00191.safetensors', 'model-00038-of-00191.safetensors', 'model-00039-of-00191.safetensors', 'model-00040-of-00191.safetensors', 'model-00041-of-00191.safetensors', 'model-00042-of-00191.safetensors', 'model-00043-of-00191.safetensors', 'model-00044-of-00191.safetensors', 'model-00045-of-00191.safetensors', 'model-00046-of-00191.safetensors', 'model-00047-of-00191.safetensors', 'model-00048-of-00191.safetensors', 'model-00049-of-00191.safetensors', 'model-00050-of-00191.safetensors', 'model-00051-of-00191.safetensors', 'model-00052-of-00191.safetensors', 'model-00053-of-00191.safetensors', 'model-00054-of-00191.safetensors', 'model-00055-of-00191.safetensors', 'model-00056-of-00191.safetensors', 'model-00057-of-00191.safetensors', 'model-00058-of-00191.safetensors', 'model-00059-of-00191.safetensors', 'model-00060-of-00191.safetensors', 'model-00061-of-00191.safetensors', 'model-00062-of-00191.safetensors', 'model-00063-of-00191.safetensors', 'model-00064-of-00191.safetensors', 'model-00065-of-00191.safetensors', 'model-00066-of-00191.safetensors', 'model-00067-of-00191.safetensors', 'model-00068-of-00191.safetensors', 'model-00069-of-00191.safetensors', 'model-00070-of-00191.safetensors', 'model-00071-of-00191.safetensors', 'model-00072-of-00191.safetensors', 'model-00073-of-00191.safetensors', 'model-00074-of-00191.safetensors', 'model-00075-of-00191.safetensors', 'model-00076-of-00191.safetensors', 'model-00077-of-00191.safetensors', 'model-00078-of-00191.safetensors', 'model-00079-of-00191.safetensors', 'model-00080-of-00191.safetensors', 'model-00081-of-00191.safetensors', 'model-00082-of-00191.safetensors', 'model-00083-of-00191.safetensors', 'model-00084-of-00191.safetensors', 'model-00085-of-00191.safetensors', 'model-00086-of-00191.safetensors', 'model-00087-of-00191.safetensors', 'model-00088-of-00191.safetensors', 'model-00089-of-00191.safetensors', 'model-00090-of-00191.safetensors', 'model-00091-of-00191.safetensors', 'model-00092-of-00191.safetensors', 'model-00093-of-00191.safetensors', 'model-00094-of-00191.safetensors', 'model-00095-of-00191.safetensors', 'model-00096-of-00191.safetensors', 'model-00097-of-00191.safetensors', 'model-00098-of-00191.safetensors', 'model-00099-of-00191.safetensors', 'model-00100-of-00191.safetensors', 'model-00101-of-00191.safetensors', 'model-00102-of-00191.safetensors', 'model-00103-of-00191.safetensors', 'model-00104-of-00191.safetensors', 'model-00105-of-00191.safetensors', 'model-00106-of-00191.safetensors', 'model-00107-of-00191.safetensors', 'model-00108-of-00191.safetensors', 'model-00109-of-00191.safetensors', 'model-00110-of-00191.safetensors', 'model-00111-of-00191.safetensors', 'model-00112-of-00191.safetensors', 'model-00113-of-00191.safetensors', 'model-00114-of-00191.safetensors', 'model-00115-of-00191.safetensors', 'model-00116-of-00191.safetensors', 'model-00117-of-00191.safetensors', 'model-00118-of-00191.safetensors', 'model-00119-of-00191.safetensors', 'model-00120-of-00191.safetensors', 'model-00121-of-00191.safetensors', 'model-00122-of-00191.safetensors', 'model-00123-of-00191.safetensors', 'model-00124-of-00191.safetensors', 'model-00125-of-00191.safetensors', 'model-00126-of-00191.safetensors', 'model-00127-of-00191.safetensors', 'model-00128-of-00191.safetensors', 'model-00129-of-00191.safetensors', 'model-00130-of-00191.safetensors', 'model-00131-of-00191.safetensors', 'model-00132-of-00191.safetensors', 'model-00133-of-00191.safetensors', 'model-00134-of-00191.safetensors', 'model-00135-of-00191.safetensors', 'model-00136-of-00191.safetensors', 'model-00137-of-00191.safetensors', 'model-00138-of-00191.safetensors', 'model-00139-of-00191.safetensors', 'model-00140-of-00191.safetensors', 'model-00141-of-00191.safetensors', 'model-00142-of-00191.safetensors', 'model-00143-of-00191.safetensors', 'model-00144-of-00191.safetensors', 'model-00145-of-00191.safetensors', 'model-00146-of-00191.safetensors', 'model-00147-of-00191.safetensors', 'model-00148-of-00191.safetensors', 'model-00149-of-00191.safetensors', 'model-00150-of-00191.safetensors', 'model-00151-of-00191.safetensors', 'model-00152-of-00191.safetensors', 'model-00153-of-00191.safetensors', 'model-00154-of-00191.safetensors', 'model-00155-of-00191.safetensors', 'model-00156-of-00191.safetensors', 'model-00157-of-00191.safetensors', 'model-00158-of-00191.safetensors', 'model-00159-of-00191.safetensors', 'model-00160-of-00191.safetensors', 'model-00161-of-00191.safetensors', 'model-00162-of-00191.safetensors', 'model-00163-of-00191.safetensors', 'model-00164-of-00191.safetensors', 'model-00165-of-00191.safetensors', 'model-00166-of-00191.safetensors', 'model-00167-of-00191.safetensors', 'model-00168-of-00191.safetensors', 'model-00169-of-00191.safetensors', 'model-00170-of-00191.safetensors', 'model-00171-of-00191.safetensors', 'model-00172-of-00191.safetensors', 'model-00173-of-00191.safetensors', 'model-00174-of-00191.safetensors', 'model-00175-of-00191.safetensors', 'model-00176-of-00191.safetensors', 'model-00177-of-00191.safetensors', 'model-00178-of-00191.safetensors', 'model-00179-of-00191.safetensors', 'model-00180-of-00191.safetensors', 'model-00181-of-00191.safetensors', 'model-00182-of-00191.safetensors', 'model-00183-of-00191.safetensors', 'model-00184-of-00191.safetensors', 'model-00185-of-00191.safetensors', 'model-00186-of-00191.safetensors', 'model-00187-of-00191.safetensors', 'model-00188-of-00191.safetensors', 'model-00189-of-00191.safetensors', 'model-00190-of-00191.safetensors', 'model-00191-of-00191.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=410081247232, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"), context_length=131072, supports_tools=True, tokens_per_second=75, featured_priority=10, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens')], default_draft_model='accounts/fireworks/models/full-llama-v3p1-8b-instruct-8b-fp8', default_draft_token_count=3, deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-405b-instruct-c5a1bae9', deployment='accounts/fireworks/deployments/aefe5940', state=DeployedModelState.DEPLOYED, default=True, public=True)], precisions=[DeploymentPrecision.FP8_V2], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-405b-instruct-fp16', create_time=datetime.datetime(2024, 7, 23, 4, 34, 57, 981711, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00191.safetensors', 'model-00002-of-00191.safetensors', 'model-00003-of-00191.safetensors', 'model-00004-of-00191.safetensors', 'model-00005-of-00191.safetensors', 'model-00006-of-00191.safetensors', 'model-00007-of-00191.safetensors', 'model-00008-of-00191.safetensors', 'model-00009-of-00191.safetensors', 'model-00010-of-00191.safetensors', 'model-00011-of-00191.safetensors', 'model-00012-of-00191.safetensors', 'model-00013-of-00191.safetensors', 'model-00014-of-00191.safetensors', 'model-00015-of-00191.safetensors', 'model-00016-of-00191.safetensors', 'model-00017-of-00191.safetensors', 'model-00018-of-00191.safetensors', 'model-00019-of-00191.safetensors', 'model-00020-of-00191.safetensors', 'model-00021-of-00191.safetensors', 'model-00022-of-00191.safetensors', 'model-00023-of-00191.safetensors', 'model-00024-of-00191.safetensors', 'model-00025-of-00191.safetensors', 'model-00026-of-00191.safetensors', 'model-00027-of-00191.safetensors', 'model-00028-of-00191.safetensors', 'model-00029-of-00191.safetensors', 'model-00030-of-00191.safetensors', 'model-00031-of-00191.safetensors', 'model-00032-of-00191.safetensors', 'model-00033-of-00191.safetensors', 'model-00034-of-00191.safetensors', 'model-00035-of-00191.safetensors', 'model-00036-of-00191.safetensors', 'model-00037-of-00191.safetensors', 'model-00038-of-00191.safetensors', 'model-00039-of-00191.safetensors', 'model-00040-of-00191.safetensors', 'model-00041-of-00191.safetensors', 'model-00042-of-00191.safetensors', 'model-00043-of-00191.safetensors', 'model-00044-of-00191.safetensors', 'model-00045-of-00191.safetensors', 'model-00046-of-00191.safetensors', 'model-00047-of-00191.safetensors', 'model-00048-of-00191.safetensors', 'model-00049-of-00191.safetensors', 'model-00050-of-00191.safetensors', 'model-00051-of-00191.safetensors', 'model-00052-of-00191.safetensors', 'model-00053-of-00191.safetensors', 'model-00054-of-00191.safetensors', 'model-00055-of-00191.safetensors', 'model-00056-of-00191.safetensors', 'model-00057-of-00191.safetensors', 'model-00058-of-00191.safetensors', 'model-00059-of-00191.safetensors', 'model-00060-of-00191.safetensors', 'model-00061-of-00191.safetensors', 'model-00062-of-00191.safetensors', 'model-00063-of-00191.safetensors', 'model-00064-of-00191.safetensors', 'model-00065-of-00191.safetensors', 'model-00066-of-00191.safetensors', 'model-00067-of-00191.safetensors', 'model-00068-of-00191.safetensors', 'model-00069-of-00191.safetensors', 'model-00070-of-00191.safetensors', 'model-00071-of-00191.safetensors', 'model-00072-of-00191.safetensors', 'model-00073-of-00191.safetensors', 'model-00074-of-00191.safetensors', 'model-00075-of-00191.safetensors', 'model-00076-of-00191.safetensors', 'model-00077-of-00191.safetensors', 'model-00078-of-00191.safetensors', 'model-00079-of-00191.safetensors', 'model-00080-of-00191.safetensors', 'model-00081-of-00191.safetensors', 'model-00082-of-00191.safetensors', 'model-00083-of-00191.safetensors', 'model-00084-of-00191.safetensors', 'model-00085-of-00191.safetensors', 'model-00086-of-00191.safetensors', 'model-00087-of-00191.safetensors', 'model-00088-of-00191.safetensors', 'model-00089-of-00191.safetensors', 'model-00090-of-00191.safetensors', 'model-00091-of-00191.safetensors', 'model-00092-of-00191.safetensors', 'model-00093-of-00191.safetensors', 'model-00094-of-00191.safetensors', 'model-00095-of-00191.safetensors', 'model-00096-of-00191.safetensors', 'model-00097-of-00191.safetensors', 'model-00098-of-00191.safetensors', 'model-00099-of-00191.safetensors', 'model-00100-of-00191.safetensors', 'model-00101-of-00191.safetensors', 'model-00102-of-00191.safetensors', 'model-00103-of-00191.safetensors', 'model-00104-of-00191.safetensors', 'model-00105-of-00191.safetensors', 'model-00106-of-00191.safetensors', 'model-00107-of-00191.safetensors', 'model-00108-of-00191.safetensors', 'model-00109-of-00191.safetensors', 'model-00110-of-00191.safetensors', 'model-00111-of-00191.safetensors', 'model-00112-of-00191.safetensors', 'model-00113-of-00191.safetensors', 'model-00114-of-00191.safetensors', 'model-00115-of-00191.safetensors', 'model-00116-of-00191.safetensors', 'model-00117-of-00191.safetensors', 'model-00118-of-00191.safetensors', 'model-00119-of-00191.safetensors', 'model-00120-of-00191.safetensors', 'model-00121-of-00191.safetensors', 'model-00122-of-00191.safetensors', 'model-00123-of-00191.safetensors', 'model-00124-of-00191.safetensors', 'model-00125-of-00191.safetensors', 'model-00126-of-00191.safetensors', 'model-00127-of-00191.safetensors', 'model-00128-of-00191.safetensors', 'model-00129-of-00191.safetensors', 'model-00130-of-00191.safetensors', 'model-00131-of-00191.safetensors', 'model-00132-of-00191.safetensors', 'model-00133-of-00191.safetensors', 'model-00134-of-00191.safetensors', 'model-00135-of-00191.safetensors', 'model-00136-of-00191.safetensors', 'model-00137-of-00191.safetensors', 'model-00138-of-00191.safetensors', 'model-00139-of-00191.safetensors', 'model-00140-of-00191.safetensors', 'model-00141-of-00191.safetensors', 'model-00142-of-00191.safetensors', 'model-00143-of-00191.safetensors', 'model-00144-of-00191.safetensors', 'model-00145-of-00191.safetensors', 'model-00146-of-00191.safetensors', 'model-00147-of-00191.safetensors', 'model-00148-of-00191.safetensors', 'model-00149-of-00191.safetensors', 'model-00150-of-00191.safetensors', 'model-00151-of-00191.safetensors', 'model-00152-of-00191.safetensors', 'model-00153-of-00191.safetensors', 'model-00154-of-00191.safetensors', 'model-00155-of-00191.safetensors', 'model-00156-of-00191.safetensors', 'model-00157-of-00191.safetensors', 'model-00158-of-00191.safetensors', 'model-00159-of-00191.safetensors', 'model-00160-of-00191.safetensors', 'model-00161-of-00191.safetensors', 'model-00162-of-00191.safetensors', 'model-00163-of-00191.safetensors', 'model-00164-of-00191.safetensors', 'model-00165-of-00191.safetensors', 'model-00166-of-00191.safetensors', 'model-00167-of-00191.safetensors', 'model-00168-of-00191.safetensors', 'model-00169-of-00191.safetensors', 'model-00170-of-00191.safetensors', 'model-00171-of-00191.safetensors', 'model-00172-of-00191.safetensors', 'model-00173-of-00191.safetensors', 'model-00174-of-00191.safetensors', 'model-00175-of-00191.safetensors', 'model-00176-of-00191.safetensors', 'model-00177-of-00191.safetensors', 'model-00178-of-00191.safetensors', 'model-00179-of-00191.safetensors', 'model-00180-of-00191.safetensors', 'model-00181-of-00191.safetensors', 'model-00182-of-00191.safetensors', 'model-00183-of-00191.safetensors', 'model-00184-of-00191.safetensors', 'model-00185-of-00191.safetensors', 'model-00186-of-00191.safetensors', 'model-00187-of-00191.safetensors', 'model-00188-of-00191.safetensors', 'model-00189-of-00191.safetensors', 'model-00190-of-00191.safetensors', 'model-00191-of-00191.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=410081247232, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"), context_length=131072, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-405b-instruct-long', display_name='Llama 3.1 405B Instruct Long', create_time=datetime.datetime(2024, 8, 14, 23, 44, 25, 794805, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=1000), conversation_config=ConversationConfig(style='jinja'), created_by='divchenko@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=3), unit='1M tokens')], default_draft_model='accounts/fireworks/models/full-llama-v3p1-8b-instruct-8b-fp8-amd', default_draft_token_count=3, deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-405b-instruct-long-e908c3f7', deployment='accounts/fireworks/deployments/217a3539', state=DeployedModelState.DEPLOYED, default=True, public=True)], precisions=[DeploymentPrecision.FP8], update_time=datetime.datetime(2025, 5, 8, 0, 49, 43, 159304, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-70b-instruct', display_name='Llama 3.1 70B Instruct', create_time=datetime.datetime(2024, 7, 18, 7, 22, 37, 13293, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes. The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="\n{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=131072, supports_tools=True, tokens_per_second=100, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], default_draft_model='accounts/fireworks/models/llama-v3p1-70b-instruct-1b', default_draft_token_count=4, deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-70b-instruct-effe64ba', deployment='accounts/fireworks/deployments/3c7a68b0', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-70b-instruct-e117abe5', deployment='accounts/fireworks/deployments/5bb3209d', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8, DeploymentPrecision.FP8_MM_KV_ATTN_V2], calibrated=True, tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p1-70b-instruct-1b', display_name='Llama 3.1 70B Instruct 1B', create_time=datetime.datetime(2024, 10, 2, 23, 15, 0, 453429, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(), created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 5, 8, 0, 49, 43, 710691, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-70b-instruct-2024-12', display_name='Llama 3.1 70B Instruct 2024-12', create_time=datetime.datetime(2024, 12, 4, 0, 43, 0, 437819, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), description='Llama 3.1 70B Instruct 2024-12 is the December update of Llama 3.1 70B. The model improves upon the first version of Llama 3.1 70B (released July 2024) with advances in tool calling, multilingual text support, math and coding. The model achieves industry leading results in reasoning, math and instruction following and provides similar performance as 3.1 405B but with significant speed and cost improvements', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70553706496), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8, DeploymentPrecision.FP8_MM_V2], supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p1-8b-instruct', display_name='Llama 3.1 8B Instruct', create_time=datetime.datetime(2024, 7, 23, 0, 0, 8, 369396, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: job fireworks-llama-v3p1-8b-instruct-phmsdusm failed with reason BackoffLimitExceeded'), public=True, description='The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes. The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'patch.diff', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=8000000000, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="\n{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=131072, tokens_per_second=300, featured_priority=8, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-8b-instruct-8ed9c5d9', deployment='accounts/fireworks/deployments/ee744c5f', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/mukund-0f85ff/deployedModels/llama-v3p1-8b-instruct-ea5c5c39', deployment='accounts/mukund-0f85ff/deployments/7d09219a', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/cdoglair/deployedModels/llama-v3p1-8b-instruct-n2o4hfs1', deployment='accounts/cdoglair/deployments/xa6ft7ru', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/instacart/deployedModels/llama-v3p1-8b-instruct-n4czbr1c', deployment='accounts/instacart/deployments/p2mnr634', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p1-8b-instruct-acxk4wok', deployment='accounts/pyroworks/deployments/mfu1aynx', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/openevidence/deployedModels/llama-v3p1-8b-instruct-fvq3hb86', deployment='accounts/openevidence/deployments/gc2ptf6f', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/liner/deployedModels/llama-v3p1-8b-instruct-9ff266b8', deployment='accounts/liner/deployments/7c95a0b5', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/talentneuron/deployedModels/llama-v3p1-8b-instruct-3678d57f', deployment='accounts/talentneuron/deployments/cfcbe95a', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p1-8b-instruct-ey9k1h39', deployment='accounts/pyroworks/deployments/ij27x96f', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-8b-instruct-68281b2f', deployment='accounts/fireworks/deployments/936bccef', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/joppe-c3bf90/deployedModels/llama-v3p1-8b-instruct-rb7nuw4k', deployment='accounts/joppe-c3bf90/deployments/of3o4tnb', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/scott-woodruff44-149d09/deployedModels/llama-v3p1-8b-instruct-mf2dmgl9', deployment='accounts/scott-woodruff44-149d09/deployments/eo6evvxp', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/openevidence/deployedModels/llama-v3p1-8b-instruct-m6mx6l9l', deployment='accounts/openevidence/deployments/yvaodvd8', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/mdaepp/deployedModels/llama-v3p1-8b-instruct-icr9k4pg', deployment='accounts/mdaepp/deployments/nlrhcy02', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p1-8b-instruct-squmsoq5', deployment='accounts/pyroworks/deployments/w6a2k5gp', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/prorata/deployedModels/llama-v3p1-8b-instruct-mc1msitm', deployment='accounts/prorata/deployments/cs0mbzm9', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p1-8b-instruct-w9hafo7t', deployment='accounts/pyroworks/deployments/w05hf5ef', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-8b-instruct-ab41092d', deployment='accounts/fireworks/deployments/ee744c5f', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/prorata/deployedModels/llama-v3p1-8b-instruct-qkevp6m3', deployment='accounts/prorata/deployments/uegoa0c7', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/tobriant-fc21ec/deployedModels/llama-v3p1-8b-instruct-f14512e1', deployment='accounts/tobriant-fc21ec/deployments/fb6fcfab', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/gabriel-sandu-cf01a0/deployedModels/llama-v3p1-8b-instruct-ea31b9aa', deployment='accounts/gabriel-sandu-cf01a0/deployments/d9635ae6', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p1-8b-instruct-reasoning-fcs', create_time=datetime.datetime(2025, 1, 31, 23, 5, 34, 168305, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/llama-v3p1-8b-instruct', r=4, target_modules=['v_proj', 'q_proj', 'gate_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj'], base_model_type='llama'), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), created_by='tianyi@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p1-8b-instruct-reasoning-fcs-765d0bed', deployment='accounts/fireworks/deployments/ee744c5f', state=DeployedModelState.DEPLOYED, default=True)], tunable=True, update_time=datetime.datetime(2025, 4, 25, 17, 46, 30, 228875, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-8b-instruct-reasoning-fcs-plus', create_time=datetime.datetime(2025, 2, 1, 23, 39, 27, 31832, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/llama-v3p1-8b-instruct', r=4, target_modules=['down_proj', 'gate_proj', 'up_proj', 'o_proj', 'q_proj', 'v_proj', 'k_proj'], base_model_type='llama'), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), created_by='tianyi@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, update_time=datetime.datetime(2025, 4, 25, 17, 46, 30, 60001, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-8b-instruct-reasoning-fcs-train', create_time=datetime.datetime(2025, 2, 3, 16, 0, 56, 715168, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_PEFT_ADDON, state=ModelState.READY, status=Status(), peft_details=PeftDetails(base_model='accounts/fireworks/models/llama-v3p1-8b-instruct', r=4, target_modules=['k_proj', 'down_proj', 'v_proj', 'q_proj', 'up_proj', 'gate_proj', 'o_proj'], base_model_type='llama'), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(style='jinja'), created_by='tianyi@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, update_time=datetime.datetime(2025, 4, 25, 17, 46, 30, 173324, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct', display_name='Llama 3.1 Nemotron 70B', create_time=datetime.datetime(2024, 10, 16, 15, 51, 59, 623355, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model was trained using RLHF on a Llama-3.1-70B-Instruct model. As of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.', hugging_face_url='https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70553706496, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/zoom/deployedModels/llama-v3p1-nemotron-70b-instruct-jd33aefz', deployment='accounts/zoom/deployments/r7uijm35', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/zoom/deployedModels/llama-v3p1-nemotron-70b-instruct-s4ysl4lr', deployment='accounts/zoom/deployments/gz5x59gw', state=DeployedModelState.DEPLOYED)], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 7, 21, 30, 39, 682863, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p2-11b-vision-instruct', display_name='Llama 3.2 11B Vision Instruct', create_time=datetime.datetime(2024, 9, 24, 23, 53, 39, 792984, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Instruction-tuned image reasoning model from Meta with 11B parameters. Optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The model can understand visual data, such as charts and graphs and also bridge the gap between vision and language by generating text to describe images details', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=10668170256, model_type='mllama'), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"), context_length=131072, supports_image_input=True, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/kiran-deep-378597/deployedModels/llama-v3p2-11b-vision-instruct-bjfuu3gw', deployment='accounts/kiran-deep-378597/deployments/cnfy0dxi', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p2-1b', display_name='Llama 3.2 1B', create_time=datetime.datetime(2024, 9, 25, 15, 59, 15, 568480, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-1B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1498482688, model_type='llama'), context_length=131072, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p2-1b-instruct', display_name='Llama 3.2 1B Instruct', create_time=datetime.datetime(2024, 9, 18, 21, 23, 21, 952789, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama 3.2 1B instruct is a lightweight, multilingual model from Meta. The model is designed for efficiency and offers substantial latency and cost improvements compared to larger models. Example use cases for the model include retrieval and summarization', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=1498482688, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="\n{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=131072, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-1b-instruct-fdx4ruig', deployment='accounts/pyroworks/deployments/vkgx9gj9', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-1b-instruct-he90c3ct', deployment='accounts/pyroworks/deployments/z9me4xf2', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-1b-instruct-feolmjwe', deployment='accounts/pyroworks/deployments/m19zw4dg', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/lights0320/deployedModels/llama-v3p2-1b-instruct-nldf972h', deployment='accounts/lights0320/deployments/huby8hjl', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p2-3b', display_name='Llama 3.2 3B', create_time=datetime.datetime(2024, 9, 25, 16, 0, 20, 111763, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8_V2'), public=True, hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-3B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=3606752256, model_type='llama'), context_length=131072, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llama-v3p2-3b-instruct', display_name='Llama 3.2 3B Instruct', create_time=datetime.datetime(2024, 9, 18, 21, 24, 41, 300478, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Llama 3.2 3B instruct is a lightweight, multilingual model from Meta. The model is designed for efficiency and offers substantial latency and cost improvements compared to larger models. Example use cases for the model include query and prompt rewriting and writing assistance', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=3606752256, tunable=True, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template="\n{%- set _mode = mode | default('generate', true) -%}\n{%- set stop_token = '<|eot_id|>' -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(initial_system_message_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if 'content' not in message -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and not ns.initial_system_message_handled -%}\n        {%- set ns.initial_system_message_handled = true -%}\n        {{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.initial_system_message_handled else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' -%}\n            {{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + stop_token }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + unk_token + message['content'] + stop_token + unk_token }}\n            {%- else -%}\n                {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + (stop_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' and ns.last_assistant_index_for_eos == -1 -%}\n    {{ '<|start_header_id|>assistant<|end_header_id|>' }}\n{%- endif -%}\n"), context_length=131072, featured_priority=5, created_by='dzhulgakov@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-3b-instruct-zai4vc4h', deployment='accounts/pyroworks/deployments/iosom46t', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/doordash-nv/deployedModels/llama-v3p2-3b-instruct-h4aox4pg', deployment='accounts/doordash-nv/deployments/udm9y05r', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-3b-instruct-fxr24yob', deployment='accounts/pyroworks/deployments/x4n6wif6', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p2-3b-instruct-jis3xlso', deployment='accounts/pyroworks/deployments/wt749w85', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/lights0320/deployedModels/llama-v3p2-3b-instruct-kpnpd1eb', deployment='accounts/lights0320/deployments/ahg8sptj', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/299cliff/deployedModels/llama-v3p2-3b-instruct-e39b3174', deployment='accounts/299cliff/deployments/f6c79637', state=DeployedModelState.DEPLOYING)], tunable=True, supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 0.8999999761581421, 'temperature': 0.6000000238418579}),
Model(name='accounts/fireworks/models/llama-v3p2-90b-vision-instruct', display_name='Llama 3.2 90B Vision Instruct', create_time=datetime.datetime(2024, 9, 23, 23, 8, 48, 966402, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Instruction-tuned image reasoning model with 90B parameters from Meta. Optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The model can understand visual data, such as charts and graphs and also bridge the gap between vision and language by generating text to describe images details \n\nNote: This mode is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE.txt', 'README.md', 'USE_POLICY.md', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00037.safetensors', 'model-00002-of-00037.safetensors', 'model-00003-of-00037.safetensors', 'model-00004-of-00037.safetensors', 'model-00005-of-00037.safetensors', 'model-00006-of-00037.safetensors', 'model-00007-of-00037.safetensors', 'model-00008-of-00037.safetensors', 'model-00009-of-00037.safetensors', 'model-00010-of-00037.safetensors', 'model-00011-of-00037.safetensors', 'model-00012-of-00037.safetensors', 'model-00013-of-00037.safetensors', 'model-00014-of-00037.safetensors', 'model-00015-of-00037.safetensors', 'model-00016-of-00037.safetensors', 'model-00017-of-00037.safetensors', 'model-00018-of-00037.safetensors', 'model-00019-of-00037.safetensors', 'model-00020-of-00037.safetensors', 'model-00021-of-00037.safetensors', 'model-00022-of-00037.safetensors', 'model-00023-of-00037.safetensors', 'model-00024-of-00037.safetensors', 'model-00025-of-00037.safetensors', 'model-00026-of-00037.safetensors', 'model-00027-of-00037.safetensors', 'model-00028-of-00037.safetensors', 'model-00029-of-00037.safetensors', 'model-00030-of-00037.safetensors', 'model-00031-of-00037.safetensors', 'model-00032-of-00037.safetensors', 'model-00033-of-00037.safetensors', 'model-00034-of-00037.safetensors', 'model-00035-of-00037.safetensors', 'model-00036-of-00037.safetensors', 'model-00037-of-00037.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=90000000000, model_type='mllama'), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"), context_length=131072, supports_image_input=True, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/invisibletech/deployedModels/llama-v3p2-90b-vision-instruct-8064ce16', deployment='accounts/invisibletech/deployments/c5b15576', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llama-v3p3-70b-instruct', display_name='Llama 3.3 70B Instruct', create_time=datetime.datetime(2024, 12, 5, 23, 41, 43, 295895, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(message='Preparing'), public=True, description='Llama 3.3 70B Instruct is the December update of Llama 3.1 70B. The model improves upon Llama 3.1 70B (released July 2024) with advances in tool calling, multilingual text support, math and coding. The model achieves industry leading results in reasoning, math and instruction following and provides similar performance as 3.1 405B but with significant speed and cost improvements.', hugging_face_url='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=70000000000, model_type='llama'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n'), context_length=131072, featured_priority=5, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/atomicwork-poc/deployedModels/llama-v3p3-70b-instruct-lbk9vkh5', deployment='accounts/atomicwork-poc/deployments/pjd94vlr', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/atomicwork-poc/deployedModels/llama-v3p3-70b-instruct-lvca7kjg', deployment='accounts/atomicwork-poc/deployments/el8aecoi', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/coreysanders/deployedModels/llama-v3p3-70b-instruct-z7di8qmu', deployment='accounts/coreysanders/deployments/c4qe5pdy', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/harshvardhan-660c0e/deployedModels/llama-v3p3-70b-instruct-d3vptleu', deployment='accounts/harshvardhan-660c0e/deployments/tabeg5nv', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/mazirhx2-ae3f68/deployedModels/llama-v3p3-70b-instruct-yk9zxnzd', deployment='accounts/mazirhx2-ae3f68/deployments/wijsapkg', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/jsdelfino-14e8b4/deployedModels/llama-v3p3-70b-instruct-dgm50woi', deployment='accounts/jsdelfino-14e8b4/deployments/fj0bvdk7', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/vivek-e49575/deployedModels/llama-v3p3-70b-instruct-x09fvtub', deployment='accounts/vivek-e49575/deployments/l87znzpy', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/fireworks/deployedModels/llama-v3p3-70b-instruct-72a75b9a', deployment='accounts/fireworks/deployments/13735fce', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/pyroworks/deployedModels/llama-v3p3-70b-instruct-xivreeyd', deployment='accounts/pyroworks/deployments/afj1tjd5', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/notion/deployedModels/llama-v3p3-70b-instruct-80986cac', deployment='accounts/notion/deployments/a4fac5d2', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/sentientfoundation/deployedModels/llama-v3p3-70b-instruct-b8cabe3e', deployment='accounts/sentientfoundation/deployments/59a214e6', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/sentientfoundation/deployedModels/llama-v3p3-70b-instruct-n4qhf62e', deployment='accounts/sentientfoundation/deployments/m74ganea', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8, DeploymentPrecision.FP8_MM_V2, DeploymentPrecision.FP8_V2], calibrated=True, tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.8999999761581421}),
Model(name='accounts/fireworks/models/llava-v1p6-mistral-7b', display_name='LLaVA 1.6 Mistral 7B', create_time=datetime.datetime(2024, 8, 29, 21, 37, 10, 573860, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), description='The LLaVA-NeXT model was proposed in LLaVA-NeXT: Improved reasoning, OCR, and world knowledge by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon LLaVa-1.5 by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning.', hugging_face_url='https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['README.md', 'added_tokens.json', 'chat_template.json', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7566747648, model_type='llava_mistral'), conversation_config=ConversationConfig(style='jinja', template="{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"), context_length=32768, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/llava-yi-34b', display_name='LLaVA V1.6 Yi 34B', create_time=datetime.datetime(2024, 2, 27, 18, 38, 49, 639658, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='LLaVA is an open-source chatbot trained by fine-tuning LLMs on multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. LlaVA V1.6 Yi 34B is the fine-tuned version of the Yi 34B LLM.', hugging_face_url='https://huggingface.co/llava-hf/llava-v1.6-34b-hf', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=34000000000), conversation_config=ConversationConfig(style='chatml'), context_length=4096, supports_image_input=True, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-7b', display_name='Mistral 7B', create_time=datetime.datetime(2023, 10, 6, 17, 4, 57, 28445, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters utilizing Grouped-query attention (GQA) for faster inference and Sliding window attention (SWA) to handle longer sequences at a lower cost.', github_url='https://github.com/mistralai/mistral-src', hugging_face_url='https://huggingface.co/mistralai/Mistral-7B-v0.1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=7000000000, tunable=True, model_type='mistral'), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-1d5e6ced', deployment='accounts/cresta-ai/deployments/97e378a8', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/totomail-tp-3b012c/deployedModels/mistral-7b-zte5td8m', deployment='accounts/totomail-tp-3b012c/deployments/hbk04yc3', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/mistral-7b-instruct-4k', display_name='Mistal 7B Instruct V0.1', create_time=datetime.datetime(2023, 9, 27, 22, 32, 35, 931389, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.', github_url='https://github.com/mistralai/mistral-src', hugging_face_url='https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=7000000000, model_type='mistral'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if not message.get('content') -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + ns.system_prompt + ' ' + message['content'] + ' [/INST]' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST]' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 44, 251506, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/mistral-7b-instruct-v0p2', display_name='Mistral 7B Instruct v0.2', create_time=datetime.datetime(2024, 2, 29, 6, 30, 48, 686391, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mistral 7B Instruct v0.2 is an instruction fine-tuned version of the Mistral 7B v0.2 language model. This update introduces key improvements over v0.1, including an expanded context window of 32k tokens (up from 8k), a Rope-theta value of 1e6, and the removal of Sliding-Window Attention. Designed to excel in generating coherent and contextually rich responses, it is suitable for a wide range of natural language processing tasks.', github_url='https://github.com/mistralai/mistral-src', hugging_face_url='https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, tunable=True, model_type='mistral'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if not message.get('content') -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + ns.system_prompt + ' ' + message['content'] + ' [/INST]' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST]' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-instruct-v0p2-8a05fb36', deployment='accounts/cresta-ai/deployments/a11b39cc', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-instruct-v0p2-7f036b39', deployment='accounts/cresta-ai/deployments/b45c911c', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-instruct-v0p2-a16ce735', deployment='accounts/cresta-ai/deployments/52d79bf8', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-instruct-v0p2-wxd302ww', deployment='accounts/cresta-ai/deployments/saiykq02', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-7b-instruct-v0p2-z3be1m8a', deployment='accounts/cresta-ai/deployments/syjb29to', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-7b-instruct-v3', display_name='Mistral 7B Instruct v0.3', create_time=datetime.datetime(2024, 5, 29, 22, 37, 38, 987558, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mistral 7B Instruct v0.3 is an instruction fine-tuned version of the Mistral 7B v0.3 language model. It features an extended vocabulary of 32,768 tokens, supports the v3 tokenizer, and includes function calling capabilities. Optimized for instruction-following tasks, it excels in generating responses for chat applications, code generation, and understanding structured prompts. The model is designed to work efficiently with the Mistral inference library, enabling developers to deploy it across various natural language processing applications.', hugging_face_url='https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'consolidated.safetensors', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'params.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer.model.v3', 'tokenizer_config.json'], parameter_count=7248023552, model_type='mistral'), conversation_config=ConversationConfig(style='jinja', template="{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"), context_length=32768, supports_tools=True, created_by='peters@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/agenor-thomas-gilles-396507/deployedModels/mistral-7b-instruct-v3-wwjpe1h4', deployment='accounts/agenor-thomas-gilles-396507/deployments/psrtyml0', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-7b-v0p2', display_name='Mistral 7B v0.2', create_time=datetime.datetime(2024, 3, 25, 15, 23, 33, 390395, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mistral-7B-v0.2 Large Language Model (LLM) is the successor to the Mistral-7B-v0.1 LLM featuring an increased context window for improved accuracy and performance. It was used to train the Mistral-7B-Instruct-v0.2 model', github_url='https://github.com/mistralai/mistral-src', hugging_face_url='https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, model_type='mistral'), context_length=32768, created_by='kevin@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-nemo-base-2407', display_name='Mistral Nemo Base 2407', create_time=datetime.datetime(2024, 7, 18, 22, 13, 55, 514927, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mistral-Nemo-Base-2407 Large Language Model (LLM) is a pretrained generative text model of 12B parameters trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.', github_url='https://github.com/mistralai/mistral-inference', hugging_face_url='https://huggingface.co/mistralai/Mistral-Nemo-Base-2407', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'consolidated.safetensors', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'params.json', 'special_tokens_map.json', 'tekken.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=12247782400, model_type='mistral'), context_length=128000, created_by='dmaliuga@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-nemo-instruct-2407', display_name='Mistral Nemo Instruct 2407', create_time=datetime.datetime(2024, 7, 22, 0, 7, 7, 194367, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is the instruction-tuned version of Mistral-Nemo-Base-2407 and has the chat completions API enabled. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.', github_url='https://github.com/mistralai/mistral-inference', hugging_face_url='https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'consolidated.safetensors', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'params.json', 'special_tokens_map.json', 'tekken.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=12247782400, model_type='mistral'), conversation_config=ConversationConfig(style='jinja', template='{%- if messages[0]["role"] == "system" %}\n    {%- set system_message = messages[0]["content"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr("role", "equalto", "user") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == "tool" or message.role == "tool_results" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message["role"] == "user") != (ns.index % 2 == 0) %}\n            {{- raise_exception("After the optional system message, conversation roles must alternate user/assistant/user/assistant/...") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n{%- if _mode == \'generate\' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in loop_messages %}\n    {%- if message["role"] == "user" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- "[AVAILABLE_TOOLS][" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- \'{"type": "function", "function": {\' }}\n                {%- for key, val in tool.items() if key != "return" %}\n                    {%- if val is string %}\n                        {{- \'"\' + key + \'": "\' + val + \'"\' }}\n                    {%- else %}\n                        {{- \'"\' + key + \'": \' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- ", " }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- "}}" }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- else %}\n                    {{- "]" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- "[/AVAILABLE_TOOLS]" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- "[INST]" + system_message + "\\n\\n" + message["content"] + "[/INST]" }}\n        {%- else %}\n            {{- "[INST]" + message["content"] + "[/INST]" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- "[TOOL_CALLS][" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception("Tool call IDs should be alphanumeric strings with length 9!") }}\n            {%- endif %}\n            {{- \', "id": "\' + tool_call.id + \'"}\' }}\n            {%- if not loop.last %}\n                {{- ", " }}\n            {%- else %}\n                {{- "]" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\'role\'] | lower == "assistant" %}\n        {%- if _mode == \'train\' -%}\n            {{ unk_token + message[\'content\'] + eos_token + unk_token }}\n        {%- else -%}\n            {{- message[\'content\'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else \'\') }}\n       {%- endif -%}\n        {{- message[\'content\'] + eos_token}}\n    {%- elif message[\'role\'] | lower == "tool_results" or message["role"] == "tool" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- \'[TOOL_RESULTS]{"content": \' + content|string + ", " }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception("Tool call IDs should be alphanumeric strings with length 9!") }}\n        {%- endif %}\n        {{- \'"call_id": "\' + message.tool_call_id + \'"}[/TOOL_RESULTS]\' }}\n    {%- else %}\n        {{- raise_exception("Only user and assistant roles are supported, with the exception of an initial optional system message!") }}\n    {%- endif %}\n{%- endfor %}'), context_length=128000, created_by='dmaliuga@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/cresta-ai/deployedModels/mistral-nemo-instruct-2407-f3e7d494', deployment='accounts/cresta-ai/deployments/781a7681', state=DeployedModelState.DEPLOYED)], calibrated=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mistral-small-24b-instruct-2501', display_name='Mistral Small 24B Instruct 2501', create_time=datetime.datetime(2025, 1, 30, 12, 59, 4, 586649, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='24B parameters and achieving state-of-the-art capabilities comparable to larger models', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'consolidated.safetensors', 'generation_config.json', 'model-00001-of-00010.safetensors', 'model-00002-of-00010.safetensors', 'model-00003-of-00010.safetensors', 'model-00004-of-00010.safetensors', 'model-00005-of-00010.safetensors', 'model-00006-of-00010.safetensors', 'model-00007-of-00010.safetensors', 'model-00008-of-00010.safetensors', 'model-00009-of-00010.safetensors', 'model-00010-of-00010.safetensors', 'model.safetensors.index.json', 'params.json', 'special_tokens_map.json', 'tekken.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=23572403200, model_type='mistral'), conversation_config=ConversationConfig(style='jinja', template="{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST]' + message['content'] + '[/INST]' }}{% elif message['role'] == 'system' %}{{ '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% else %}{{ raise_exception('Only user, system and assistant roles are supported!') }}{% endif %}{% endfor %}"), context_length=32768, featured_priority=2, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 44, 777311, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mixtral-8x22b', display_name='Mixtral Moe 8x22B', create_time=datetime.datetime(2024, 4, 10, 15, 1, 9, 723419, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Mixtral MoE 8x22B v0.1 Large Language Model (LLM) is a pretrained generative sparse Mixture-of-Experts model fluent in English, French, Italian, German, and Spanish, with a focus on mathematics and coding tasks.', hugging_face_url='https://huggingface.co/mistralai/Mixtral-8x22B-v0.1', base_model_details=BaseModelDetails(world_size=8, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=176000000000, moe=True, model_type='mixtral'), context_length=65536, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/mixtral-8x22b-instruct', display_name='Mixtral MoE 8x22B Instruct', create_time=datetime.datetime(2024, 4, 17, 17, 38, 28, 592439, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mixtral MoE 8x22B Instruct v0.1 is the instruction-tuned version of Mixtral MoE 8x22B v0.1 and has the chat completions API enabled.', hugging_face_url='https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1', base_model_details=BaseModelDetails(world_size=8, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=176000000000, moe=True, model_type='mixtral'), conversation_config=ConversationConfig(style='jinja'), context_length=65536, supports_tools=True, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', units=1, nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/mixtral-8x22b-instruct-aa4f04cc', deployment='accounts/fireworks/deployments/feab4fdc', state=DeployedModelState.DEPLOYED, default=True, public=True)], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/mixtral-8x7b', display_name='Mixtral 8x7B v0.1', create_time=datetime.datetime(2023, 12, 9, 0, 55, 59, 355256, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mixtral 8x7B v0.1 is a sparse mixture-of-experts (SMoE) large language model developed by Mistral AI. With 46.7 billion total parameters and 12.9 billion active parameters per token, it outperforms Llama 2 70B and matches GPT-3.5 on many benchmarks while offering efficient inference. The model handles context lengths up to 32k tokens, supports multiple languages including English, French, Italian, German, and Spanish, and excels in code generation tasks. Licensed under Apache 2.0, Mixtral provides a powerful and efficient solution for diverse NLP applications.', hugging_face_url='https://huggingface.co/mistralai/Mixtral-8x7B-v0.1', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=46000000000, moe=True, tunable=True, model_type='mixtral'), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/mixtral-8x7b-instruct', display_name='Mixtral MoE 8x7B Instruct', create_time=datetime.datetime(2023, 12, 11, 20, 33, 11, 352472, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mixtral MoE 8x7B Instruct is the instruction-tuned version of Mixtral MoE 8x7B and has the chat completions API enabled.', hugging_face_url='https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=46000000000, moe=True, model_type='mixtral'), conversation_config=ConversationConfig(style='mistral-chat'), context_length=32768, tokens_per_second=300, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/mixtral-8x7b-instruct-hf', display_name='Mixtral MoE 8x7B Instruct (HF version)', create_time=datetime.datetime(2024, 2, 6, 1, 40, 59, 47247, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Mixtral MoE 8x7B Instruct (HF Version) is the original, FP16 version of Mixtral MoE 8x7B Instruct whose results should be consistent with the official Hugging Face implementation.', hugging_face_url='https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=46000000000, moe=True, tunable=True, model_type='mixtral'), conversation_config=ConversationConfig(style='jinja', template="{%- set _mode = mode | default('generate', true) -%}\n{%- set message_roles = ['SYSTEM', 'USER', 'ASSISTANT'] -%}\n{%- set ns = namespace(system_prompt='', system_prompt_handled=false, last_assistant_index_for_eos=-1, messages=messages) -%}\n{%- for message in ns.messages -%}\n    {%- if not message.get('role') -%}\n        {{ raise_exception('Key [role] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if message['role'] | upper not in message_roles -%}\n        {{ raise_exception('Invalid role ' + message['role']|tojson + '. Only ' + message_roles|tojson + ' are supported.') }}\n    {%- endif -%}\n    {%- if not message.get('content') -%}\n        {{ raise_exception('Key [content] is missing. Original input: ' +  message|tojson) }}\n    {%- endif -%}\n    {%- if loop.last and message['role'] | upper == 'ASSISTANT' -%}\n        {%- set ns.last_assistant_index_for_eos = loop.index0 -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if _mode == 'generate' -%}\n    {{ bos_token }}\n{%- endif -%}\n{%- for message in ns.messages -%}\n    {%- if message['role'] | upper == 'SYSTEM' and ns.system_prompt == '' -%}\n        {%- set ns.system_prompt = message['content'] -%}\n    {%- elif message['role'] | upper != 'SYSTEM' -%}\n        {%- if _mode == 'train' and (message['role'] | upper == 'USER') != ((loop.index0 - (1 if ns.system_prompt != '' else 0)) % 2 == 0) -%}\n            {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif -%}\n        {%- if message['role'] | upper == 'USER' and ns.system_prompt != '' and not ns.system_prompt_handled -%}\n            {{- '[INST] ' + ns.system_prompt + ' ' + message['content'] + ' [/INST]' }}\n            {%- set ns.system_prompt_handled = true -%}\n        {%- elif message['role'] | upper == 'USER' -%}\n            {{- '[INST] ' + message['content'] + ' [/INST]' }}\n        {%- elif message['role'] | upper == 'ASSISTANT' -%}\n            {%- if _mode == 'train' -%}\n                {{ unk_token + message['content'] + eos_token + unk_token }}\n            {%- else -%}\n                {{- message['content'] + (eos_token if loop.index0 != ns.last_assistant_index_for_eos else '') }}\n            {%- endif -%}\n        {%- else -%}\n            {{ raise_exception('Unexpected role found') }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n"), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], default_draft_model='accounts/fireworks/models/mixtral-8x7b-instruct-v0-oss', default_draft_token_count=2, supports_lora=True, default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/mixtral-8x7b-instruct-v0-oss', display_name='Mixtral 8x7b Instruct V0 Draft Model', create_time=datetime.datetime(2024, 4, 16, 21, 40, 37, 350347, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(parameter_count=46000000000, moe=True), created_by='beiyijie@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 50, 11, 381551, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/mythomax-l2-13b', display_name='MythoMax L2 13B', create_time=datetime.datetime(2024, 2, 13, 21, 43, 52, 765973, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='An improved, potentially even perfected variant of MythoMix, a MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. Proficient at both storytelling and roleplaying due to its unique nature.', hugging_face_url='https://huggingface.co/Gryphe/MythoMax-L2-13b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=13000000000), conversation_config=ConversationConfig(style='jinja'), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 45, 360398, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 0.6000000238418579, 'temperature': 0.8999999761581421}),
Model(name='accounts/fireworks/models/nous-capybara-7b-v1p9', display_name='Nous Capybara 7B V1.9', create_time=datetime.datetime(2024, 3, 1, 21, 13, 43, 958433, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Nous-Capybara 7B V1.9 is a new model trained for multiple epochs on a dataset of roughly 20,000 carefully curated conversational examples, most of which are comprised of entirely new in-house synthesized tokens.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Capybara-7B-V1.9', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo', display_name='Nouse Hermes 2 Mixtral 8x7B DPO', create_time=datetime.datetime(2024, 3, 2, 2, 56, 50, 567118, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the Mixtral 8x7B MoE LLM. The model was trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=46000000000, moe=True), conversation_config=ConversationConfig(style='chatml'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=500000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 45, 954676, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/nous-hermes-2-yi-34b', display_name='Nous Hermes 2 Yi 34B', create_time=datetime.datetime(2024, 3, 1, 21, 41, 33, 580805, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Nous Research's fine-tuned version of the Yi 34B Large Language Model trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape.", hugging_face_url='https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00015.safetensors', 'model-00002-of-00015.safetensors', 'model-00003-of-00015.safetensors', 'model-00004-of-00015.safetensors', 'model-00005-of-00015.safetensors', 'model-00006-of-00015.safetensors', 'model-00007-of-00015.safetensors', 'model-00008-of-00015.safetensors', 'model-00009-of-00015.safetensors', 'model-00010-of-00015.safetensors', 'model-00011-of-00015.safetensors', 'model-00012-of-00015.safetensors', 'model-00013-of-00015.safetensors', 'model-00014-of-00015.safetensors', 'model-00015-of-00015.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000), conversation_config=ConversationConfig(style='chatml'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 46, 517498, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/nous-hermes-llama2-13b', display_name='Nous Hermes Llama2 13B', create_time=datetime.datetime(2024, 2, 29, 6, 0, 10, 294867, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=13000000000), conversation_config=ConversationConfig(style='alpaca'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 0.8999999761581421, 'top_k': 50.0, 'top_p': 0.6000000238418579}),
Model(name='accounts/fireworks/models/nous-hermes-llama2-70b', display_name='Nous Hermes Llama2 70B', create_time=datetime.datetime(2024, 3, 1, 23, 36, 53, 792054, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms in the synthetic training data.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=70000000000), conversation_config=ConversationConfig(style='alpaca'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/nous-hermes-llama2-7b', display_name='Nous Hermes Llama2 7B', create_time=datetime.datetime(2024, 2, 29, 17, 23, 59, 527250, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'model.safetensors', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), conversation_config=ConversationConfig(style='alpaca'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 0.8999999761581421, 'top_k': 50.0, 'top_p': 0.6000000238418579}),
Model(name='accounts/fireworks/models/openchat-3p5-0106-7b', display_name='OpenChat 3.5 0106', create_time=datetime.datetime(2024, 3, 3, 4, 42, 35, 410295, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning.', github_url='https://github.com/imoneoi/openchat', hugging_face_url='https://huggingface.co/openchat/openchat-3.5-0106', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'openchat.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), conversation_config=ConversationConfig(style='jinja'), context_length=8192, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 0.5, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/openhermes-2-mistral-7b', display_name='OpenHermes 2 Mistral 7B', create_time=datetime.datetime(2024, 3, 1, 0, 41, 36, 751999, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='OpenHermes 2 Mistral 7B is a state of the art Mistral Fine-tune. OpenHermes was trained on 900,000 entries of primarily GPT-4 generated data, from open datasets across the AI landscape.', hugging_face_url='https://huggingface.co/teknium/OpenHermes-2-Mistral-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, model_type='mistral'), conversation_config=ConversationConfig(style='chatml'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 47, 60589, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/openhermes-2p5-mistral-7b', display_name='OpenHermes 2.5 Mistral 7B', create_time=datetime.datetime(2024, 3, 1, 0, 45, 13, 904788, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, trained on 1,000,000 entries of primarily GPT-4 generated data as well as other high quality data from open datasets across the AI landscape. OpenHermes 2.5 achieved higher humaneval and various benchmark scores compared to OpenHermes 2.', hugging_face_url='https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json', 'transformers_inference.py'], parameter_count=7000000000, model_type='mistral'), conversation_config=ConversationConfig(style='chatml'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 47, 595098, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/openorca-7b', display_name='Mistral 7B OpenOrca', create_time=datetime.datetime(2024, 2, 27, 0, 50, 21, 494068, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="A fine-tuned version of Mistral-7B trained on the OpenOrca dataset, based on the dataset generated for Microsoft Research's Orca Paper. Developed using OpenChat packing, and trained using Axolotl.", hugging_face_url='https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/phi-2-3b', display_name='Phi-2 3B', create_time=datetime.datetime(2024, 2, 27, 16, 39, 21, 666046, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.', hugging_face_url='https://huggingface.co/microsoft/phi-2', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'CODE_OF_CONDUCT.md', 'LICENSE', 'NOTICE.md', 'README.md', 'SECURITY.md', 'added_tokens.json', 'config.json', 'configuration_phi.py', 'fireworks.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'modeling_phi.py', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=3000000000), context_length=2048, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 48, 132702, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phi-3-mini-128k-instruct', display_name='Phi-3 Mini 128k Instruct', create_time=datetime.datetime(2024, 5, 24, 1, 2, 10, 118846, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets. This dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.', hugging_face_url='https://huggingface.co/microsoft/Phi-3-mini-128k-instruct', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=3800000000), conversation_config=ConversationConfig(style='jinja'), context_length=131072, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/moamenmoustafa04-bd8fbe/deployedModels/phi-3-mini-128k-instruct-e4oyv1t5', deployment='accounts/moamenmoustafa04-bd8fbe/deployments/tu8a1lvs', state=DeployedModelState.DEPLOYED)], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 48, 684429, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/phi-3p5-vision-instruct', display_name='FP16 Phi3.5 for internal testing', create_time=datetime.datetime(2024, 8, 20, 22, 21, 8, 529254, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=4200000000, model_type='phi3_v'), conversation_config=ConversationConfig(style='jinja'), supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phi-3-vision-128k-instruct', display_name='Phi-3.5 Vision Instruct', create_time=datetime.datetime(2024, 5, 29, 10, 1, 19, 715317, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.', hugging_face_url='https://huggingface.co/microsoft/Phi-3.5-vision-instruct', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=4200000000), conversation_config=ConversationConfig(style='jinja'), context_length=32064, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], update_time=datetime.datetime(2025, 5, 8, 0, 49, 49, 224275, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phi4-eagle', display_name='Phi-4 Eagle', create_time=datetime.datetime(2025, 3, 26, 22, 49, 59, 874233, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(), created_by='m@fireworks.ai', update_time=datetime.datetime(2025, 5, 8, 0, 49, 49, 745937, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phi4-eagle-1l', create_time=datetime.datetime(2025, 3, 26, 22, 49, 59, 874233, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(), created_by='m@fireworks.ai', default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phind-code-llama-34b-python-v1', display_name='Phind CodeLlama 34B Python v1', create_time=datetime.datetime(2024, 3, 2, 0, 45, 0, 452989, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Phind CodeLlama 34B Python V1 is a fine-tuned version of the CodeLlama 34B Python LLM using an internal Phind dataset that achieved 69.5% pass@1 on HumanEval, beating GPT-4.', hugging_face_url='https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00007.bin', 'pytorch_model-00002-of-00007.bin', 'pytorch_model-00003-of-00007.bin', 'pytorch_model-00004-of-00007.bin', 'pytorch_model-00005-of-00007.bin', 'pytorch_model-00006-of-00007.bin', 'pytorch_model-00007-of-00007.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000), context_length=16384, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phind-code-llama-34b-v1', display_name='Phind CodeLlama 34B v1', create_time=datetime.datetime(2024, 3, 2, 4, 7, 31, 456947, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Phind CodeLlama 34B V1 is a fine-tuned version of the Code-Llama 34B LLM using an internal Phind dataset that achieved 67.6% pass@1 on HumanEval, beating GPT-4.', hugging_face_url='https://huggingface.co/Phind/Phind-CodeLlama-34B-v1', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00007.bin', 'pytorch_model-00002-of-00007.bin', 'pytorch_model-00003-of-00007.bin', 'pytorch_model-00004-of-00007.bin', 'pytorch_model-00005-of-00007.bin', 'pytorch_model-00006-of-00007.bin', 'pytorch_model-00007-of-00007.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000), context_length=16384, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phind-code-llama-34b-v2', display_name='Phind CodeLlama 34B v2', create_time=datetime.datetime(2024, 3, 2, 3, 6, 41, 278123, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='This model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves 73.8% pass@1 on HumanEval. Phind-CodeLlama-34B-v2 is multi-lingual and is proficient in Python, C/C++, TypeScript, Java, and more.', hugging_face_url='https://huggingface.co/Phind/Phind-CodeLlama-34B-v2', base_model_details=BaseModelDetails(world_size=2, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00007.bin', 'pytorch_model-00002-of-00007.bin', 'pytorch_model-00003-of-00007.bin', 'pytorch_model-00004-of-00007.bin', 'pytorch_model-00005-of-00007.bin', 'pytorch_model-00006-of-00007.bin', 'pytorch_model-00007-of-00007.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000), context_length=16384, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/phi-ocr-demo-v1', create_time=datetime.datetime(2024, 10, 22, 21, 59, 40, 29597, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json', 'configuration_phi3_v.py', 'generation_config.json', 'modeling_phi3_v.py', 'preprocessor_config.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=4448611328), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{{'<|' + message['role'] + '|>' + '\n' + message['content'] + '<|end|>\n' }}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{- '<|assistant|>\n' -}}{% endif %}"), context_length=8192, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/pythia-12b', display_name='Pythia 12B', create_time=datetime.datetime(2024, 3, 2, 4, 51, 7, 868859, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Pythia model suite was deliberately designed to promote scientific research on large language models, especially interpretability research. Despite not centering downstream performance as a design goal, the models match or exceed the performance of similar and same-sized models, such as those in the OPT and GPT-Neo suites.', hugging_face_url='https://huggingface.co/EleutherAI/pythia-12b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=12000000000), context_length=2048, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qvq-72b-preview', create_time=datetime.datetime(2025, 1, 21, 19, 16, 47, 168692, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), hugging_face_url='https://huggingface.co/Qwen/QVQ-72B-Preview', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00038.safetensors', 'model-00002-of-00038.safetensors', 'model-00003-of-00038.safetensors', 'model-00004-of-00038.safetensors', 'model-00005-of-00038.safetensors', 'model-00006-of-00038.safetensors', 'model-00007-of-00038.safetensors', 'model-00008-of-00038.safetensors', 'model-00009-of-00038.safetensors', 'model-00010-of-00038.safetensors', 'model-00011-of-00038.safetensors', 'model-00012-of-00038.safetensors', 'model-00013-of-00038.safetensors', 'model-00014-of-00038.safetensors', 'model-00015-of-00038.safetensors', 'model-00016-of-00038.safetensors', 'model-00017-of-00038.safetensors', 'model-00018-of-00038.safetensors', 'model-00019-of-00038.safetensors', 'model-00020-of-00038.safetensors', 'model-00021-of-00038.safetensors', 'model-00022-of-00038.safetensors', 'model-00023-of-00038.safetensors', 'model-00024-of-00038.safetensors', 'model-00025-of-00038.safetensors', 'model-00026-of-00038.safetensors', 'model-00027-of-00038.safetensors', 'model-00028-of-00038.safetensors', 'model-00029-of-00038.safetensors', 'model-00030-of-00038.safetensors', 'model-00031-of-00038.safetensors', 'model-00032-of-00038.safetensors', 'model-00033-of-00038.safetensors', 'model-00034-of-00038.safetensors', 'model-00035-of-00038.safetensors', 'model-00036-of-00038.safetensors', 'model-00037-of-00038.safetensors', 'model-00038-of-00038.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=73405560320, model_type='qwen2_vl'), conversation_config=ConversationConfig(style='jinja', template="{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"), context_length=128000, created_by='tianyi@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 1.0, 'top_p': 0.0010000000474974513, 'temperature': 0.009999999776482582}),
Model(name='accounts/fireworks/models/qwen1p5-72b-chat', display_name='Qwen1.5 72B Chat', create_time=datetime.datetime(2024, 2, 27, 1, 56, 51, 410884, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. Compared to previous versions, Qwen1.5 has significant performance improvements, multilingual support, and stable 32k context support.', github_url='https://github.com/QwenLM/Qwen', hugging_face_url='https://huggingface.co/Qwen/Qwen1.5-72B-Chat', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=72000000000, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja'), context_length=32768, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2-72b-instruct', display_name='Qwen2 72B Instruct', create_time=datetime.datetime(2024, 6, 6, 0, 30, 51, 796575, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2 72B Instruct is a 72 billion parameter model developed by Alibaba for instruction-tuned tasks. It excels in natural language understanding and generation tasks, including summarization, dialogue, and complex reasoning. Qwen2 is optimized for instruction-following, making it ideal for applications that require detailed and structured responses across a wide range of domains.', hugging_face_url='https://huggingface.co/Qwen/Qwen2-72B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'config.json.backup', 'fireworks.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00037.safetensors', 'model-00002-of-00037.safetensors', 'model-00003-of-00037.safetensors', 'model-00004-of-00037.safetensors', 'model-00005-of-00037.safetensors', 'model-00006-of-00037.safetensors', 'model-00007-of-00037.safetensors', 'model-00008-of-00037.safetensors', 'model-00009-of-00037.safetensors', 'model-00010-of-00037.safetensors', 'model-00011-of-00037.safetensors', 'model-00012-of-00037.safetensors', 'model-00013-of-00037.safetensors', 'model-00014-of-00037.safetensors', 'model-00015-of-00037.safetensors', 'model-00016-of-00037.safetensors', 'model-00017-of-00037.safetensors', 'model-00018-of-00037.safetensors', 'model-00019-of-00037.safetensors', 'model-00020-of-00037.safetensors', 'model-00021-of-00037.safetensors', 'model-00022-of-00037.safetensors', 'model-00023-of-00037.safetensors', 'model-00024-of-00037.safetensors', 'model-00025-of-00037.safetensors', 'model-00026-of-00037.safetensors', 'model-00027-of-00037.safetensors', 'model-00028-of-00037.safetensors', 'model-00029-of-00037.safetensors', 'model-00030-of-00037.safetensors', 'model-00031-of-00037.safetensors', 'model-00032-of-00037.safetensors', 'model-00033-of-00037.safetensors', 'model-00034-of-00037.safetensors', 'model-00035-of-00037.safetensors', 'model-00036-of-00037.safetensors', 'model-00037-of-00037.safetensors', 'model.safetensors.index.json', 'test.sh', 'test_dev.sh', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=72706203648, tunable=True, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"), context_length=32768, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2-7b-instruct', display_name='Qwen2 7B Instruct', create_time=datetime.datetime(2024, 6, 6, 15, 29, 6, 121272, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2 7B Instruct is a 7-billion-parameter instruction-tuned language model developed by the Qwen team. Optimized for following instructions, it excels at tasks like question answering, dialogue generation, and summarization. The model is designed to provide accurate and contextually appropriate responses, making it suitable for a wide range of natural language processing applications.', hugging_face_url='https://huggingface.co/Qwen/Qwen2-7B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template="{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"), context_length=32768, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-0p5b-instruct', display_name='Qwen2.5 0.5B Instruct', create_time=datetime.datetime(2024, 12, 18, 23, 6, 24, 730862, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=630167424, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='divchenko@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-0p5b-instruct-on1jw3kq', deployment='accounts/pyroworks/deployments/gyseoyri', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-0p5b-instruct-nsn3h01x', deployment='accounts/pyroworks/deployments/qtig4zxi', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-0p5b-instruct-tqswufsn', deployment='accounts/pyroworks/deployments/z1v5vjsr', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-14b', display_name='Qwen2.5 14B', create_time=datetime.datetime(2024, 10, 2, 0, 15, 58, 89865, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-14B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'model-00003-of-00008.safetensors', 'model-00004-of-00008.safetensors', 'model-00005-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00008-of-00008.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14770033664, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-14b-instruct', display_name='Qwen2.5 14B Instruct', create_time=datetime.datetime(2024, 10, 2, 0, 15, 23, 266955, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-14B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'model-00003-of-00008.safetensors', 'model-00004-of-00008.safetensors', 'model-00005-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00008-of-00008.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14770033664, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-1p5b-instruct', display_name='Qwen2.5 1.5B Instruct', create_time=datetime.datetime(2025, 5, 12, 20, 7, 22, 781560, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=1777088000, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='dhuang@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-1p5b-instruct-wg0xa3n3', deployment='accounts/pyroworks/deployments/fd9k036r', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-1p5b-instruct-jf818r9u', deployment='accounts/pyroworks/deployments/xq4rs4yb', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-1p5b-instruct-uay1hjn8', deployment='accounts/pyroworks/deployments/tbsqpdd7', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 12, 20, 31, 24, 551252, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen2p5-32b', display_name='Qwen2.5 32B', create_time=datetime.datetime(2024, 10, 2, 0, 14, 25, 202229, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-32B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00017.safetensors', 'model-00002-of-00017.safetensors', 'model-00003-of-00017.safetensors', 'model-00004-of-00017.safetensors', 'model-00005-of-00017.safetensors', 'model-00006-of-00017.safetensors', 'model-00007-of-00017.safetensors', 'model-00008-of-00017.safetensors', 'model-00009-of-00017.safetensors', 'model-00010-of-00017.safetensors', 'model-00011-of-00017.safetensors', 'model-00012-of-00017.safetensors', 'model-00013-of-00017.safetensors', 'model-00014-of-00017.safetensors', 'model-00015-of-00017.safetensors', 'model-00016-of-00017.safetensors', 'model-00017-of-00017.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-32b-instruct', display_name='Qwen2.5 32B Instruct', create_time=datetime.datetime(2024, 10, 2, 0, 13, 26, 946121, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-32B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00017.safetensors', 'model-00002-of-00017.safetensors', 'model-00003-of-00017.safetensors', 'model-00004-of-00017.safetensors', 'model-00005-of-00017.safetensors', 'model-00006-of-00017.safetensors', 'model-00007-of-00017.safetensors', 'model-00008-of-00017.safetensors', 'model-00009-of-00017.safetensors', 'model-00010-of-00017.safetensors', 'model-00011-of-00017.safetensors', 'model-00012-of-00017.safetensors', 'model-00013-of-00017.safetensors', 'model-00014-of-00017.safetensors', 'model-00015-of-00017.safetensors', 'model-00016-of-00017.safetensors', 'model-00017-of-00017.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-32b-instruct-gg604jds', deployment='accounts/pyroworks/deployments/m69z46f7', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-32b-instruct-quf5rlwa', deployment='accounts/pyroworks/deployments/uhv2apz6', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-32b-instruct-qzgih6zc', deployment='accounts/pyroworks/deployments/mdvc9nvb', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8], tunable=True, supports_lora=True, default_sampling_params={'top_k': 20.0, 'top_p': 0.800000011920929, 'temperature': 0.699999988079071}),
Model(name='accounts/fireworks/models/qwen2p5-3b-instruct', display_name='Qwen2.5 3B Instruct', create_time=datetime.datetime(2025, 5, 12, 21, 26, 55, 697193, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-3B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=3085938688, tunable=True), created_by='dhuang@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-3b-instruct-m3i76xft', deployment='accounts/pyroworks/deployments/ejqtfybo', state=DeployedModelState.DEPLOYED)], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen2'], update_time=datetime.datetime(2025, 5, 12, 23, 21, 37, 207198, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen2p5-72b', display_name='Qwen2.5 72B', create_time=datetime.datetime(2024, 10, 2, 0, 11, 25, 283033, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-72B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00037.safetensors', 'model-00002-of-00037.safetensors', 'model-00003-of-00037.safetensors', 'model-00004-of-00037.safetensors', 'model-00005-of-00037.safetensors', 'model-00006-of-00037.safetensors', 'model-00007-of-00037.safetensors', 'model-00008-of-00037.safetensors', 'model-00009-of-00037.safetensors', 'model-00010-of-00037.safetensors', 'model-00011-of-00037.safetensors', 'model-00012-of-00037.safetensors', 'model-00013-of-00037.safetensors', 'model-00014-of-00037.safetensors', 'model-00015-of-00037.safetensors', 'model-00016-of-00037.safetensors', 'model-00017-of-00037.safetensors', 'model-00018-of-00037.safetensors', 'model-00019-of-00037.safetensors', 'model-00020-of-00037.safetensors', 'model-00021-of-00037.safetensors', 'model-00022-of-00037.safetensors', 'model-00023-of-00037.safetensors', 'model-00024-of-00037.safetensors', 'model-00025-of-00037.safetensors', 'model-00026-of-00037.safetensors', 'model-00027-of-00037.safetensors', 'model-00028-of-00037.safetensors', 'model-00029-of-00037.safetensors', 'model-00030-of-00037.safetensors', 'model-00031-of-00037.safetensors', 'model-00032-of-00037.safetensors', 'model-00033-of-00037.safetensors', 'model-00034-of-00037.safetensors', 'model-00035-of-00037.safetensors', 'model-00036-of-00037.safetensors', 'model-00037-of-00037.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=72706203648, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 4, 11, 19, 16, 12, 435139, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/qwen2p5-72b-instruct', display_name='Qwen2.5 72B Instruct', create_time=datetime.datetime(2024, 10, 2, 0, 1, 24, 167522, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-72B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00037.safetensors', 'model-00002-of-00037.safetensors', 'model-00003-of-00037.safetensors', 'model-00004-of-00037.safetensors', 'model-00005-of-00037.safetensors', 'model-00006-of-00037.safetensors', 'model-00007-of-00037.safetensors', 'model-00008-of-00037.safetensors', 'model-00009-of-00037.safetensors', 'model-00010-of-00037.safetensors', 'model-00011-of-00037.safetensors', 'model-00012-of-00037.safetensors', 'model-00013-of-00037.safetensors', 'model-00014-of-00037.safetensors', 'model-00015-of-00037.safetensors', 'model-00016-of-00037.safetensors', 'model-00017-of-00037.safetensors', 'model-00018-of-00037.safetensors', 'model-00019-of-00037.safetensors', 'model-00020-of-00037.safetensors', 'model-00021-of-00037.safetensors', 'model-00022-of-00037.safetensors', 'model-00023-of-00037.safetensors', 'model-00024-of-00037.safetensors', 'model-00025-of-00037.safetensors', 'model-00026-of-00037.safetensors', 'model-00027-of-00037.safetensors', 'model-00028-of-00037.safetensors', 'model-00029-of-00037.safetensors', 'model-00030-of-00037.safetensors', 'model-00031-of-00037.safetensors', 'model-00032-of-00037.safetensors', 'model-00033-of-00037.safetensors', 'model-00034-of-00037.safetensors', 'model-00035-of-00037.safetensors', 'model-00036-of-00037.safetensors', 'model-00037-of-00037.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=72706203648, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, supports_tools=True, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwen2p5-72b-instruct-8d4ed93d', deployment='accounts/fireworks/deployments/a5d5a693', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/mercuryinsurance/deployedModels/qwen2p5-72b-instruct-5049527f', deployment='accounts/mercuryinsurance/deployments/f0b51658', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-72b-instruct-rref25xs', deployment='accounts/pyroworks/deployments/zu3jnx9l', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8, DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 4, 24, 21, 52, 1, 929621, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-7b', display_name='Qwen2.5 7B', create_time=datetime.datetime(2024, 10, 2, 0, 20, 54, 543986, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-7b-instruct', display_name='Qwen2.5 7B Instruct', create_time=datetime.datetime(2024, 10, 2, 0, 21, 14, 159272, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-7b-instruct-ni6tix2i', deployment='accounts/pyroworks/deployments/uxvwfys6', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/doordash/deployedModels/qwen2p5-7b-instruct-goqpgiwu', deployment='accounts/doordash/deployments/vny8y1tp', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/vineeth-vajipey-7e1414/deployedModels/qwen2p5-7b-instruct-npdjelc8', deployment='accounts/vineeth-vajipey-7e1414/deployments/jmpt0swv', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-7b-instruct-yng1jzuk', deployment='accounts/pyroworks/deployments/x8a8odhh', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-7b-instruct-ptjhobv8', deployment='accounts/pyroworks/deployments/y9qrye2b', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen2p5-7b-instruct-w7x83ajq', deployment='accounts/pyroworks/deployments/nyijrehe', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/joppe-c3bf90/deployedModels/qwen2p5-7b-instruct-le60erqp', deployment='accounts/joppe-c3bf90/deployments/olwlpxiq', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/notion/deployedModels/qwen2p5-7b-instruct-p0gfbvhf', deployment='accounts/notion/deployments/hujcs8r4', state=DeployedModelState.DEPLOYED)], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 4, 30, 19, 0, 48, 807587, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-0p5b', display_name='Qwen2.5-Coder 0.5B', create_time=datetime.datetime(2024, 11, 12, 1, 7, 51, 573009, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=630167424, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 50, 283563, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-0p5b-instruct', display_name='Qwen2.5-Coder 0.5B Instruct', create_time=datetime.datetime(2024, 11, 12, 0, 55, 29, 81703, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=630167424, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 50, 816200, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-0p5-instruct', create_time=datetime.datetime(2024, 11, 14, 23, 29, 56, 911991, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(), conversation_config=ConversationConfig(template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-14b', display_name='Qwen2.5-Coder 14B', create_time=datetime.datetime(2024, 11, 12, 0, 57, 49, 735049, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-14B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14770033664, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 51, 341045, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-14b-instruct', display_name='Qwen2.5-Coder 14B Instruct', create_time=datetime.datetime(2024, 11, 12, 0, 55, 59, 580567, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14770033664, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 51, 861208, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-1p5b', display_name='Qwen2.5-Coder 1.5B', create_time=datetime.datetime(2024, 9, 26, 23, 41, 19, 843191, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 billion parameters.', github_url='https://github.com/QwenLM/Qwen2.5-Coder', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=1777088000, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 52, 392677, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-1p5b-instruct', display_name='Qwen2.5-Coder 1.5B Instruct', create_time=datetime.datetime(2024, 9, 26, 23, 41, 28, 833866, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 billion parameters.', github_url='https://github.com/QwenLM/Qwen2.5-Coder', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=1777088000, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 52, 952458, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-32b', display_name='Qwen2.5-Coder 32B', create_time=datetime.datetime(2024, 11, 12, 1, 10, 55, 640513, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-32B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 53, 494982, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-32b-instruct', display_name='Qwen2.5-Coder 32B Instruct', create_time=datetime.datetime(2024, 11, 12, 1, 9, 48, 505008, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Note: This model is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, featured_priority=4, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 54, 49429, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k', display_name='Qwen2.5-Coder 32B Instruct 128K', create_time=datetime.datetime(2024, 11, 12, 1, 9, 48, 505008, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Note: This model is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, featured_priority=4, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 54, 583733, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 20.0, 'top_p': 0.800000011920929, 'temperature': 0.699999988079071}),
Model(name='accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope', display_name='Qwen2.5-Coder 32B Instruct 32K RoPE', create_time=datetime.datetime(2024, 11, 12, 1, 9, 48, 505008, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Note: This model is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, featured_priority=4, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], precisions=[DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 55, 140875, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k', display_name='Qwen2.5-Coder 32B Instruct 64k', create_time=datetime.datetime(2024, 11, 12, 1, 9, 48, 505008, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Note: This model is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['config.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, featured_priority=4, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/frank-bf9616/deployedModels/qwen2p5-coder-32b-instruct-64k-fa51d133', deployment='accounts/frank-bf9616/deployments/3e7bac4f', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8_MM_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 55, 672154, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 20.0, 'top_p': 0.800000011920929, 'temperature': 0.699999988079071}),
Model(name='accounts/fireworks/models/qwen2p5-coder-3b', display_name='Qwen2.5-Coder 3B', create_time=datetime.datetime(2024, 11, 12, 1, 8, 16, 330498, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-3B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=3397103616, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 56, 213371, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-3b-instruct', display_name='Qwen2.5-Coder 3B Instruct', create_time=datetime.datetime(2024, 11, 12, 0, 55, 54, 708004, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=3397103616, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 56, 744693, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 0.800000011920929, 'temperature': 0.699999988079071, 'top_k': 20.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-7b', display_name='Qwen2.5-Coder 7B', create_time=datetime.datetime(2024, 9, 26, 23, 36, 8, 483398, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 billion parameters.', github_url='https://github.com/QwenLM/Qwen2.5-Coder', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/gitlab/deployedModels/qwen2p5-coder-7b-w2fd80gq', deployment='accounts/gitlab/deployments/ycdqpztz', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/gitlab/deployedModels/qwen2p5-coder-7b-ea3bce72', deployment='accounts/gitlab/deployments/84f6d9f4', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/gitlab/deployedModels/qwen2p5-coder-7b-2f5d7c8d', deployment='accounts/gitlab/deployments/ac589058', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/abdelatif-abualya-c7b863/deployedModels/qwen2p5-coder-7b-zhcy0k6f', deployment='accounts/abdelatif-abualya-c7b863/deployments/vu3bymlq', state=DeployedModelState.DEPLOYED)], precisions=[DeploymentPrecision.FP8, DeploymentPrecision.FP8_MM_V2, DeploymentPrecision.FP8_V2], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 57, 271038, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-coder-7b-instruct', display_name='Qwen2.5-Coder 7B Instruct', create_time=datetime.datetime(2024, 9, 26, 23, 20, 17, 258448, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 billion parameters.', github_url='https://github.com/QwenLM/Qwen2.5-Coder', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='\n{%- set _mode = mode | default(\'generate\', true) -%}\n{%- if tools %}\n    {{- \'<|imstart|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- \'\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\' }}\n    {%- for tool in tools %}\n        {{- \'\\n\' }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \'\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\' }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if _mode == \'train\' and message.role == \'assistant\' %}\n        {%- set mask_marker = unk_token %}\n    {%- else %}\n        {%- set mask_marker = \'\' %}\n    {%- endif %}\n    {%- if (message.role == \'user\') or (message.role == \'system\' and not loop.first) or (message.role == \'assistant\' and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + mask_marker + \'\\n\' + message.content + \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'assistant\' %}\n        {{- \'<|im_start|>\' + message.role + mask_marker }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{\\"name\\": \\"\' }}\n            {{- tool_call.name }}\n            {{- \'\\", \\"arguments\\": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\' + mask_marker + \'\\n\' }}\n    {%- elif message.role == \'tool\' %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \'tool\') %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \'tool\') %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], calibrated=True, tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 57, 771840, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen2p5-math-72b-instruct', display_name='Qwen2.5-Math 72B Instruct', create_time=datetime.datetime(2024, 10, 7, 19, 57, 51, 148854, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-Math-72B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00037.safetensors', 'model-00002-of-00037.safetensors', 'model-00003-of-00037.safetensors', 'model-00004-of-00037.safetensors', 'model-00005-of-00037.safetensors', 'model-00006-of-00037.safetensors', 'model-00007-of-00037.safetensors', 'model-00008-of-00037.safetensors', 'model-00009-of-00037.safetensors', 'model-00010-of-00037.safetensors', 'model-00011-of-00037.safetensors', 'model-00012-of-00037.safetensors', 'model-00013-of-00037.safetensors', 'model-00014-of-00037.safetensors', 'model-00015-of-00037.safetensors', 'model-00016-of-00037.safetensors', 'model-00017-of-00037.safetensors', 'model-00018-of-00037.safetensors', 'model-00019-of-00037.safetensors', 'model-00020-of-00037.safetensors', 'model-00021-of-00037.safetensors', 'model-00022-of-00037.safetensors', 'model-00023-of-00037.safetensors', 'model-00024-of-00037.safetensors', 'model-00025-of-00037.safetensors', 'model-00026-of-00037.safetensors', 'model-00027-of-00037.safetensors', 'model-00028-of-00037.safetensors', 'model-00029-of-00037.safetensors', 'model-00030-of-00037.safetensors', 'model-00031-of-00037.safetensors', 'model-00032-of-00037.safetensors', 'model-00033-of-00037.safetensors', 'model-00034-of-00037.safetensors', 'model-00035-of-00037.safetensors', 'model-00036-of-00037.safetensors', 'model-00037-of-00037.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=72706203648, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'Please reason step by step, and put your final answer within \\\\boxed{}.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=4096, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 58, 308626, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-vl-32b-instruct', display_name='Qwen2.5-VL 32B Instruct', create_time=datetime.datetime(2025, 3, 31, 3, 45, 39, 123033, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Latest Qwen's VLM model", base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'chat_template.json', 'config.json', 'configuration.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00018.safetensors', 'model-00002-of-00018.safetensors', 'model-00003-of-00018.safetensors', 'model-00004-of-00018.safetensors', 'model-00005-of-00018.safetensors', 'model-00006-of-00018.safetensors', 'model-00007-of-00018.safetensors', 'model-00008-of-00018.safetensors', 'model-00009-of-00018.safetensors', 'model-00010-of-00018.safetensors', 'model-00011-of-00018.safetensors', 'model-00012-of-00018.safetensors', 'model-00013-of-00018.safetensors', 'model-00014-of-00018.safetensors', 'model-00015-of-00018.safetensors', 'model-00016-of-00018.safetensors', 'model-00017-of-00018.safetensors', 'model-00018-of-00018.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=33452718336, model_type='qwen2_5_vl'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=128000, supports_image_input=True, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwen2p5-vl-32b-instruct-f3643f19', deployment='accounts/fireworks/deployments/356bcf7c', state=DeployedModelState.DEPLOYED, default=True, public=True)], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 58, 832177, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 9.999999974752427e-07}),
Model(name='accounts/fireworks/models/qwen2p5-vl-3b-instruct', display_name='Qwen2.5-VL 3B Instruct', create_time=datetime.datetime(2025, 3, 31, 3, 40, 42, 562429, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Latest Qwen's VLM model", base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=4065787904, model_type='qwen2_5_vl'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=128000, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 59, 373702, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 9.999999974752427e-07, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2p5-vl-72b-instruct', display_name='Qwen2.5-VL 72B Instruct', create_time=datetime.datetime(2025, 3, 31, 4, 15, 25, 390810, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Latest Qwen's VLM model", base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00038.safetensors', 'model-00002-of-00038.safetensors', 'model-00003-of-00038.safetensors', 'model-00004-of-00038.safetensors', 'model-00005-of-00038.safetensors', 'model-00006-of-00038.safetensors', 'model-00007-of-00038.safetensors', 'model-00008-of-00038.safetensors', 'model-00009-of-00038.safetensors', 'model-00010-of-00038.safetensors', 'model-00011-of-00038.safetensors', 'model-00012-of-00038.safetensors', 'model-00013-of-00038.safetensors', 'model-00014-of-00038.safetensors', 'model-00015-of-00038.safetensors', 'model-00016-of-00038.safetensors', 'model-00017-of-00038.safetensors', 'model-00018-of-00038.safetensors', 'model-00019-of-00038.safetensors', 'model-00020-of-00038.safetensors', 'model-00021-of-00038.safetensors', 'model-00022-of-00038.safetensors', 'model-00023-of-00038.safetensors', 'model-00024-of-00038.safetensors', 'model-00025-of-00038.safetensors', 'model-00026-of-00038.safetensors', 'model-00027-of-00038.safetensors', 'model-00028-of-00038.safetensors', 'model-00029-of-00038.safetensors', 'model-00030-of-00038.safetensors', 'model-00031-of-00038.safetensors', 'model-00032-of-00038.safetensors', 'model-00033-of-00038.safetensors', 'model-00034-of-00038.safetensors', 'model-00035-of-00038.safetensors', 'model-00036-of-00038.safetensors', 'model-00037-of-00038.safetensors', 'model-00038-of-00038.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=73410777344, model_type='qwen2_5_vl'), conversation_config=ConversationConfig(style='jinja', template="{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"), context_length=128000, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 49, 59, 910145, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 9.999999974752427e-07}),
Model(name='accounts/fireworks/models/qwen2p5-vl-7b-instruct', display_name='Qwen2.5-VL 7B Instruct', create_time=datetime.datetime(2025, 3, 31, 3, 43, 8, 870373, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Latest Qwen's VLM model", base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=8292166656, model_type='qwen2_5_vl'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=128000, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], calibrated=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 15, 17, 20, 5, 601347, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 9.999999974752427e-07, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwen2-vl-2b-instruct', display_name='Qwen2-VL 2B Instruct', create_time=datetime.datetime(2024, 12, 2, 19, 53, 2, 263588, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The 2B variant of the latest iteration of Qwen-VL model from Alibaba, representing nearly a year of innovation.', hugging_face_url='https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=2442359296, model_type='qwen2_vl'), conversation_config=ConversationConfig(style='jinja', template="{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"), context_length=32768, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 1, 108868, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.009999999776482582, 'top_k': 1.0, 'top_p': 0.0010000000474974513}),
Model(name='accounts/fireworks/models/qwen2-vl-72b-instruct', display_name='Qwen2-VL 72B Instruct', create_time=datetime.datetime(2024, 12, 2, 19, 55, 45, 379244, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The 72B variant of the latest iteration of Qwen-VL model from Alibaba, representing nearly a year of innovation.', hugging_face_url='https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00038.safetensors', 'model-00002-of-00038.safetensors', 'model-00003-of-00038.safetensors', 'model-00004-of-00038.safetensors', 'model-00005-of-00038.safetensors', 'model-00006-of-00038.safetensors', 'model-00007-of-00038.safetensors', 'model-00008-of-00038.safetensors', 'model-00009-of-00038.safetensors', 'model-00010-of-00038.safetensors', 'model-00011-of-00038.safetensors', 'model-00012-of-00038.safetensors', 'model-00013-of-00038.safetensors', 'model-00014-of-00038.safetensors', 'model-00015-of-00038.safetensors', 'model-00016-of-00038.safetensors', 'model-00017-of-00038.safetensors', 'model-00018-of-00038.safetensors', 'model-00019-of-00038.safetensors', 'model-00020-of-00038.safetensors', 'model-00021-of-00038.safetensors', 'model-00022-of-00038.safetensors', 'model-00023-of-00038.safetensors', 'model-00024-of-00038.safetensors', 'model-00025-of-00038.safetensors', 'model-00026-of-00038.safetensors', 'model-00027-of-00038.safetensors', 'model-00028-of-00038.safetensors', 'model-00029-of-00038.safetensors', 'model-00030-of-00038.safetensors', 'model-00031-of-00038.safetensors', 'model-00032-of-00038.safetensors', 'model-00033-of-00038.safetensors', 'model-00034-of-00038.safetensors', 'model-00035-of-00038.safetensors', 'model-00036-of-00038.safetensors', 'model-00037-of-00038.safetensors', 'model-00038-of-00038.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=73405560320, model_type='qwen2_vl'), conversation_config=ConversationConfig(style='jinja', template="{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"), context_length=32768, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwen2-vl-72b-instruct-8cf3def7', deployment='accounts/fireworks/deployments/56258344', state=DeployedModelState.DEPLOYED, default=True, public=True)], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 1, 825203, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 1.0, 'top_p': 0.0010000000474974513}),
Model(name='accounts/fireworks/models/qwen2-vl-7b-instruct', display_name='Qwen2-VL 7B Instruct', create_time=datetime.datetime(2024, 12, 2, 19, 54, 48, 830150, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The 7B variant of the latest iteration of Qwen-VL model from Alibaba, representing nearly a year of innovation.', hugging_face_url='https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'chat_template.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'preprocessor_config.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=8291375616, model_type='qwen2_vl'), conversation_config=ConversationConfig(style='jinja', template="{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"), context_length=32768, supports_image_input=True, created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 2, 527827, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 1.0, 'top_p': 0.0010000000474974513, 'temperature': 0.009999999776482582}),
Model(name='accounts/fireworks/models/qwen3-0p6b', display_name='Qwen3 0.6B', create_time=datetime.datetime(2025, 4, 28, 23, 44, 56, 795571, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen3 0.6B model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=751632384, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/tuhins-251000/deployedModels/qwen3-0p6b-p9bvjmhc', deployment='accounts/tuhins-251000/deployments/ikg1p301', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/tuhins-251000/deployedModels/qwen3-0p6b-qbr8yel5', deployment='accounts/tuhins-251000/deployments/l94eza4d', state=DeployedModelState.DEPLOYED)], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 14, 19, 0, 39, 74610, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-14b', display_name='Qwen3 14B', create_time=datetime.datetime(2025, 4, 28, 23, 41, 5, 797558, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen3 14B model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'model-00003-of-00008.safetensors', 'model-00004-of-00008.safetensors', 'model-00005-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00008-of-00008.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14768307200, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--moe-sharding=ep', '--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 8, 0, 50, 3, 637591, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-1p7b', display_name='Qwen3 1.7B', create_time=datetime.datetime(2025, 4, 29, 4, 53, 14, 109687, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen 1.7B Model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=2031825920, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 14, 19, 0, 49, 16400, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-235b-a22b', display_name='Qwen3 235B-A22B', create_time=datetime.datetime(2025, 4, 29, 0, 7, 29, 649090, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen3 235B with 22B active parameter  model', hugging_face_url='https://huggingface.co/Qwen/Qwen3-235B-A22B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00048.safetensors', 'model-00002-of-00048.safetensors', 'model-00003-of-00048.safetensors', 'model-00004-of-00048.safetensors', 'model-00005-of-00048.safetensors', 'model-00006-of-00048.safetensors', 'model-00007-of-00048.safetensors', 'model-00008-of-00048.safetensors', 'model-00009-of-00048.safetensors', 'model-00010-of-00048.safetensors', 'model-00011-of-00048.safetensors', 'model-00012-of-00048.safetensors', 'model-00013-of-00048.safetensors', 'model-00014-of-00048.safetensors', 'model-00015-of-00048.safetensors', 'model-00016-of-00048.safetensors', 'model-00017-of-00048.safetensors', 'model-00018-of-00048.safetensors', 'model-00019-of-00048.safetensors', 'model-00020-of-00048.safetensors', 'model-00021-of-00048.safetensors', 'model-00022-of-00048.safetensors', 'model-00023-of-00048.safetensors', 'model-00024-of-00048.safetensors', 'model-00025-of-00048.safetensors', 'model-00026-of-00048.safetensors', 'model-00027-of-00048.safetensors', 'model-00028-of-00048.safetensors', 'model-00029-of-00048.safetensors', 'model-00030-of-00048.safetensors', 'model-00031-of-00048.safetensors', 'model-00032-of-00048.safetensors', 'model-00033-of-00048.safetensors', 'model-00034-of-00048.safetensors', 'model-00035-of-00048.safetensors', 'model-00036-of-00048.safetensors', 'model-00037-of-00048.safetensors', 'model-00038-of-00048.safetensors', 'model-00039-of-00048.safetensors', 'model-00040-of-00048.safetensors', 'model-00041-of-00048.safetensors', 'model-00042-of-00048.safetensors', 'model-00043-of-00048.safetensors', 'model-00044-of-00048.safetensors', 'model-00045-of-00048.safetensors', 'model-00046-of-00048.safetensors', 'model-00047-of-00048.safetensors', 'model-00048-of-00048.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']), conversation_config=ConversationConfig(style='Jinja'), context_length=128000, supports_tools=True, featured_priority=13, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=220000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=880000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwen3-235b-a22b-ncxuiyay', deployment='accounts/fireworks/deployments/rnvfgg7o', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen3-235b-a22b-or4sezmg', deployment='accounts/pyroworks/deployments/ykqlijix', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/zjuwangjue-7f535b/deployedModels/qwen3-235b-a22b-ifm5ft5g', deployment='accounts/zjuwangjue-7f535b/deployments/fe4vng3o', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen3-235b-a22b-zpqydbps', deployment='accounts/pyroworks/deployments/aeivrqop', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen3-235b-a22b-cuu54mrj', deployment='accounts/pyroworks/deployments/tu112fao', state=DeployedModelState.DEPLOYED)], use_hf_apply_chat_template=True, extra_deployment_args=['--end-thinking-token="</think>"', '--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 18, 23, 37, 15, 34029, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-30b-a3b', display_name='Qwen3 30B-A3B', create_time=datetime.datetime(2025, 4, 28, 22, 8, 53, 637383, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Latest Qwen3 state of the art model, 30B-A3B version', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'USE_POLICY.md', 'config.json', 'generation_config.json', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json'], parameter_count=30000000000, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n'), context_length=40000, featured_priority=13, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=150000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=600000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwen3-30b-a3b-ik885qi4', deployment='accounts/fireworks/deployments/eensi0ts', state=DeployedModelState.DEPLOYED, default=True, public=True), DeployedModelRef(name='accounts/yucheng-lai-2d7b27/deployedModels/qwen3-30b-a3b-r9mvc3zs', deployment='accounts/yucheng-lai-2d7b27/deployments/rhvphy5a', state=DeployedModelState.DEPLOYED)], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 8, 0, 50, 5, 249447, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-32b', display_name='Qwen3 32B', create_time=datetime.datetime(2025, 4, 28, 23, 50, 1, 555797, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen3 32B model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00017.safetensors', 'model-00002-of-00017.safetensors', 'model-00003-of-00017.safetensors', 'model-00004-of-00017.safetensors', 'model-00005-of-00017.safetensors', 'model-00006-of-00017.safetensors', 'model-00007-of-00017.safetensors', 'model-00008-of-00017.safetensors', 'model-00009-of-00017.safetensors', 'model-00010-of-00017.safetensors', 'model-00011-of-00017.safetensors', 'model-00012-of-00017.safetensors', 'model-00013-of-00017.safetensors', 'model-00014-of-00017.safetensors', 'model-00015-of-00017.safetensors', 'model-00016-of-00017.safetensors', 'model-00017-of-00017.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32762123264, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/armislabs/deployedModels/qwen3-32b-zddkvzlo', deployment='accounts/armislabs/deployments/jqcog6ye', state=DeployedModelState.DEPLOYED)], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 14, 19, 1, 3, 119854, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-4b', display_name='Qwen3 4B', create_time=datetime.datetime(2025, 4, 28, 23, 50, 29, 787828, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen3 4B model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=4411424256, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/wrtn/deployedModels/qwen3-4b-rtptbfck', deployment='accounts/wrtn/deployments/oahk41o3', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/andrew-37afb5/deployedModels/qwen3-4b-gprbj08f', deployment='accounts/andrew-37afb5/deployments/x1rz15ep', state=DeployedModelState.DEPLOYED)], supports_lora=True, use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 14, 18, 59, 35, 952485, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen3-8b', display_name='Qwen3 8B', create_time=datetime.datetime(2025, 4, 29, 6, 7, 42, 965494, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen 8B Model', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=8191159296, model_type='qwen3'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0].role == \'system\' %}\n        {{- messages[0].content + \'\\n\\n\' }}\n    {%- endif %}\n    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0].role == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0].content + \'<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith(\'<tool_response>\') and message.content.endswith(\'</tool_response>\')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = \'\' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if \'</think>\' in message.content %}\n                {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n                {%- set reasoning_content = message.content.split(\'</think>\')[0].rstrip(\'\\n\').split(\'<think>\')[-1].lstrip(\'\\n\') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- \'<|im_start|>\' + message.role + \'\\n<think>\\n\' + reasoning_content.strip(\'\\n\') + \'\\n</think>\\n\\n\' + content.lstrip(\'\\n\') }}\n            {%- else %}\n                {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- \'<|im_start|>\' + message.role + \'\\n\' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- \'\\n\' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- \'<tool_call>\\n{"name": "\' }}\n                {{- tool_call.name }}\n                {{- \'", "arguments": \' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- \'}\\n</tool_call>\' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- \'<think>\\n\\n</think>\\n\\n\' }}\n    {%- endif %}\n{%- endif %}'), context_length=40960, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], use_hf_apply_chat_template=True, extra_deployment_args=['--conversation-style=qwen3'], update_time=datetime.datetime(2025, 5, 14, 19, 0, 25, 134690, tzinfo=datetime.timezone.utc)),
Model(name='accounts/fireworks/models/qwen-qwq-32b-preview', display_name='Qwen QWQ 32B Preview', create_time=datetime.datetime(2024, 11, 27, 20, 23, 10, 171680, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description="Qwen QwQ model focuses on advancing AI reasoning, and showcases the power of open models to match closed frontier model performance.QwQ-32B-Preview is an experimental release, comparable to o1 and surpassing GPT-4o and Claude 3.5 Sonnet on analytical and reasoning abilities across GPQA, AIME, MATH-500 and LiveCodeBench benchmarks. Note: This model is served experimentally as a serverless model. If you're deploying in production, be aware that Fireworks may undeploy the model with short notice.", hugging_face_url='https://huggingface.co/Qwen/QWQ-32B-Preview', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00017.safetensors', 'model-00002-of-00017.safetensors', 'model-00003-of-00017.safetensors', 'model-00004-of-00017.safetensors', 'model-00005-of-00017.safetensors', 'model-00006-of-00017.safetensors', 'model-00007-of-00017.safetensors', 'model-00008-of-00017.safetensors', 'model-00009-of-00017.safetensors', 'model-00010-of-00017.safetensors', 'model-00011-of-00017.safetensors', 'model-00012-of-00017.safetensors', 'model-00013-of-00017.safetensors', 'model-00014-of-00017.safetensors', 'model-00015-of-00017.safetensors', 'model-00016-of-00017.safetensors', 'model-00017-of-00017.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32763876352), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='bchen@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 7, 391073, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen-v2p5-0p5b-instruct', create_time=datetime.datetime(2024, 10, 8, 6, 0, 41, 160390, tzinfo=datetime.timezone.utc), kind=ModelKind.DRAFT_ADDON, state=ModelState.READY, status=Status(), base_model_details=BaseModelDetails(), created_by='yaozhang@fireworks.ai', default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/qwen-v2p5-14b-instruct', display_name='Qwen2.5 14B Instruct', create_time=datetime.datetime(2024, 9, 19, 23, 56, 28, 945326, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-14B-Instruct', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'model-00003-of-00008.safetensors', 'model-00004-of-00008.safetensors', 'model-00005-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00008-of-00008.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=14770033664, tunable=True, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=32768, created_by='yingwg@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen-v2p5-14b-instruct-d2ufkvgt', deployment='accounts/pyroworks/deployments/ap5md1l2', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen-v2p5-14b-instruct-gqqwxlgl', deployment='accounts/pyroworks/deployments/xxchw9c5', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/pyroworks/deployedModels/qwen-v2p5-14b-instruct-yyabtcbc', deployment='accounts/pyroworks/deployments/t1fq95c5', state=DeployedModelState.DEPLOYED), DeployedModelRef(name='accounts/vineeth-vajipey-7e1414/deployedModels/qwen-v2p5-14b-instruct-m86rbdlh', deployment='accounts/vineeth-vajipey-7e1414/deployments/urkq0vwl', state=DeployedModelState.DEPLOYED)], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.699999988079071, 'top_k': 20.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/qwen-v2p5-7b', display_name='Qwen2.5 7B', create_time=datetime.datetime(2024, 9, 19, 22, 47, 4, 709996, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Qwen2.5 are a series of decoder-only language models developed by Qwen team, Alibaba Cloud, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.', github_url='https://github.com/QwenLM/Qwen2.5', hugging_face_url='https://huggingface.co/Qwen/Qwen2.5-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=7615616512, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'You are a helpful assistant.\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n    {%- else %}\n        {{- \'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='yingwg@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/qwq-32b', display_name='QWQ 32B', create_time=datetime.datetime(2025, 3, 5, 19, 50, 51, 682955, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Medium-sized reasoning model from Qwen.', hugging_face_url='https://huggingface.co/Qwen/QwQ-32B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'added_tokens.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00014.safetensors', 'model-00002-of-00014.safetensors', 'model-00003-of-00014.safetensors', 'model-00004-of-00014.safetensors', 'model-00005-of-00014.safetensors', 'model-00006-of-00014.safetensors', 'model-00007-of-00014.safetensors', 'model-00008-of-00014.safetensors', 'model-00009-of-00014.safetensors', 'model-00010-of-00014.safetensors', 'model-00011-of-00014.safetensors', 'model-00012-of-00014.safetensors', 'model-00013-of-00014.safetensors', 'model-00014-of-00014.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json'], parameter_count=32763876352, model_type='qwen2'), conversation_config=ConversationConfig(style='jinja', template='{%- if tools %}\n    {{- \'<|im_start|>system\\n\' }}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- messages[0][\'content\'] }}\n    {%- else %}\n        {{- \'\' }}\n    {%- endif %}\n    {{- "\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}\n    {%- for tool in tools %}\n        {{- "\\n" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}\n{%- else %}\n    {%- if messages[0][\'role\'] == \'system\' %}\n        {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" and not message.tool_calls %}\n        {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n        {{- \'<|im_start|>\' + message.role + \'\\n\' + content + \'<|im_end|>\' + \'\\n\' }}\n    {%- elif message.role == "assistant" %}\n        {%- set content = message.content.split(\'</think>\')[-1].lstrip(\'\\n\') %}\n        {{- \'<|im_start|>\' + message.role }}\n        {%- if message.content %}\n            {{- \'\\n\' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- \'\\n<tool_call>\\n{"name": "\' }}\n            {{- tool_call.name }}\n            {{- \'", "arguments": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \'}\\n</tool_call>\' }}\n        {%- endfor %}\n        {{- \'<|im_end|>\\n\' }}\n    {%- elif message.role == "tool" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}\n            {{- \'<|im_start|>user\' }}\n        {%- endif %}\n        {{- \'\\n<tool_response>\\n\' }}\n        {{- message.content }}\n        {{- \'\\n</tool_response>\' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}\n            {{- \'<|im_end|>\\n\' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|im_start|>assistant\\n<think>\\n\' }}\n{%- endif %}\n'), context_length=131072, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/fireworks/deployedModels/qwq-32b-c1235b87', deployment='accounts/fireworks/deployments/0ee4c9fc', state=DeployedModelState.DEPLOYED, default=True, public=True)], tunable=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 8, 19015, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 40.0, 'top_p': 0.949999988079071}),
Model(name='accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo', display_name='Snorkel Mistral PairRM DPO', create_time=datetime.datetime(2024, 3, 2, 4, 42, 30, 51233, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='A fine-tuned version of the Mistral-7B model developed by Snorkel using PairRM for response ranking and Direct Preference Optimization (DPO) for model adaptation and refinement.', hugging_face_url='https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000, model_type='mistral'), conversation_config=ConversationConfig(style='mistral-chat'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 8, 584368, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/stablecode-3b', display_name='Stable Code 3B', create_time=datetime.datetime(2024, 8, 2, 20, 30, 3, 181175, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Stable Code 3B is a 2.7-billion-parameter decoder-only language model developed by Stability AI. Pre-trained on 1.3 trillion tokens of diverse text and code datasets, it covers 18 programming languages selected based on the 2023 StackOverflow Developer Survey. The model demonstrates state-of-the-art performance among similar-sized models on MultiPL-E benchmarks across multiple programming languages. Key features include fill-in-the-middle capability and support for long context sequences up to 16,384 tokens, making it excel in code generation tasks.', hugging_face_url='https://huggingface.co/stabilityai/stable-code-3b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=2700000000, tunable=True, model_type='stablelm'), created_by='beiyijie@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/stable-diffusion-3p5-large', create_time=datetime.datetime(2024, 10, 24, 21, 50, 57, 221393, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), hugging_face_url='https://huggingface.co/stabilityai/stable-diffusion-3.5-large', base_model_details=BaseModelDetails(world_size=1), created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 4, 21, 8, 24, 30, 926982, tzinfo=datetime.timezone.utc), default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/stable-diffusion-3p5-large-turbo', create_time=datetime.datetime(2024, 10, 24, 23, 45, 13, 644415, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), hugging_face_url='https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo', base_model_details=BaseModelDetails(world_size=1), created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 4, 21, 8, 19, 36, 143638, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/stable-diffusion-3p5-medium', create_time=datetime.datetime(2024, 10, 28, 23, 4, 10, 711150, tzinfo=datetime.timezone.utc), kind=ModelKind.FLUMINA_BASE_MODEL, state=ModelState.READY, status=Status(), hugging_face_url='https://huggingface.co/stabilityai/stable-diffusion-3.5-medium', base_model_details=BaseModelDetails(world_size=1), created_by='james@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], update_time=datetime.datetime(2025, 4, 22, 0, 5, 23, 747608, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/starcoder-16b', display_name='StarCoder 16B', create_time=datetime.datetime(2024, 2, 17, 0, 55, 12, 592542, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='StarCoder-15.5B is a 15.5B parameter model trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8,192 tokens, and was trained using the Fill-in-the-Middle objective on 1 trillion tokens.', github_url='https://github.com/bigcode-project/starcoder', hugging_face_url='https://huggingface.co/bigcode/starcoder', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=16000000000), context_length=8192, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 9, 109698, tzinfo=datetime.timezone.utc), default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/starcoder2-15b', display_name='StarCoder2 15B', create_time=datetime.datetime(2024, 3, 12, 21, 10, 8, 897313, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='StarCoder2-15B is a 15B parameter model trained on 600+ programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 4+ trillion tokens.', github_url='https://github.com/bigcode-project/starcoder2', hugging_face_url='https://huggingface.co/bigcode/starcoder2-15b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=15000000000, tunable=True, model_type='starcoder2'), context_length=16384, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/starcoder2-3b', display_name='StarCoder2 3B', create_time=datetime.datetime(2024, 3, 12, 21, 28, 32, 862575, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='StarCoder2-3B is a 3B parameter model trained on 17 programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3+ trillion tokens.', github_url='https://github.com/bigcode-project/starcoder2', hugging_face_url='https://huggingface.co/bigcode/starcoder2-3b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=3000000000, tunable=True, model_type='starcoder2'), context_length=16384, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], deployed_model_refs=[DeployedModelRef(name='accounts/pyroworks/deployedModels/starcoder2-3b-n037wf9m', deployment='accounts/pyroworks/deployments/feggt7ds', state=DeployedModelState.DEPLOYED)], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/starcoder2-7b', display_name='StarCoder2 7B', create_time=datetime.datetime(2024, 3, 12, 21, 25, 24, 463210, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(code=Code.INTERNAL, message='Prepare failed: FP8'), public=True, description='StarCoder2-7B is a 7B parameter model trained on 17 programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3.5+ trillion tokens.', github_url='https://github.com/bigcode-project/starcoder2', hugging_face_url='https://huggingface.co/bigcode/starcoder2-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, parameter_count=7000000000, tunable=True, model_type='starcoder2'), context_length=16384, created_by='yingliu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/starcoder-7b', display_name='StarCoder 7B', create_time=datetime.datetime(2024, 2, 17, 0, 42, 0, 328311, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='StarCoderBase-7B is a 7B parameter model trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8,192 tokens, and was trained using the Fill-in-the-Middle objective on 1 trillion tokens.', github_url='https://github.com/bigcode-project/starcoder', hugging_face_url='https://huggingface.co/bigcode/starcoderbase-7b', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=7000000000), context_length=8192, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'top_k': 50.0, 'top_p': 1.0, 'temperature': 1.0}),
Model(name='accounts/fireworks/models/toppy-m-7b', display_name='Toppy M 7B', create_time=datetime.datetime(2024, 2, 29, 16, 54, 17, 46673, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='A wild 7B parameter model that merges several models using the new task_arithmetic merge method from mergekit.', hugging_face_url='https://huggingface.co/Undi95/Toppy-M-7B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=7000000000), conversation_config=ConversationConfig(style='alpaca'), context_length=32768, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/yi-34b', display_name='Yi 34B', create_time=datetime.datetime(2024, 3, 3, 2, 26, 32, 275589, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Yi series models are the next generation of open-source large language models trained from scratch by 01.AI. Yi-34B model ranked first among all existing open-source models (such as Falcon-180B, Llama-70B, Claude) in both English and Chinese on various benchmarks, including Hugging Face Open LLM Leaderboard (pre-trained) and C-Eval (based on data available up to November 2023).', hugging_face_url='https://huggingface.co/01-ai/Yi-34B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'md5', 'model-00001-of-00007.safetensors', 'model-00002-of-00007.safetensors', 'model-00003-of-00007.safetensors', 'model-00004-of-00007.safetensors', 'model-00005-of-00007.safetensors', 'model-00006-of-00007.safetensors', 'model-00007-of-00007.safetensors', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000, model_type='llama'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/yi-34b-200k-capybara', display_name='Nouse Capybara 34B V1.9', create_time=datetime.datetime(2023, 11, 27, 19, 31, 31, 266384, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Nous Research fine-tuned version of the Yi-34B-200k LLM trained on in-house synthesized datasets, able to generate complex summaries of advanced topics and studies.', hugging_face_url='https://huggingface.co/NousResearch/Nous-Capybara-34B', base_model_details=BaseModelDetails(world_size=4, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'pytorch_model-00001-of-00007.bin', 'pytorch_model-00002-of-00007.bin', 'pytorch_model-00003-of-00007.bin', 'pytorch_model-00004-of-00007.bin', 'pytorch_model-00005-of-00007.bin', 'pytorch_model-00006-of-00007.bin', 'pytorch_model-00007-of-00007.bin', 'pytorch_model.bin.index.json', 'special_tokens_map.json', 'tokenization_yi.py', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=1), conversation_config=ConversationConfig(style='llava-chat'), context_length=200000, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=100000000), unit='1M tokens')], calibrated=True, supports_lora=True, update_time=datetime.datetime(2025, 5, 8, 0, 50, 9, 648526, tzinfo=datetime.timezone.utc), default_sampling_params={'top_p': 1.0, 'temperature': 1.0, 'top_k': 50.0}),
Model(name='accounts/fireworks/models/yi-34b-chat', display_name='Yi 34B Chat', create_time=datetime.datetime(2024, 2, 29, 17, 47, 16, 893807, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Yi series models are the next generation of open-source large language models trained from scratch by 01.AI. Yi-34B-Chat landed in second place (following GPT-4 Turbo), outperforming other LLMs (such as GPT-4, Mixtral, Claude) on the AlpacaEval Leaderboard (based on data available up to January 2024).', hugging_face_url='https://huggingface.co/01-ai/Yi-34B-Chat', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00015.safetensors', 'model-00002-of-00015.safetensors', 'model-00003-of-00015.safetensors', 'model-00004-of-00015.safetensors', 'model-00005-of-00015.safetensors', 'model-00006-of-00015.safetensors', 'model-00007-of-00015.safetensors', 'model-00008-of-00015.safetensors', 'model-00009-of-00015.safetensors', 'model-00010-of-00015.safetensors', 'model-00011-of-00015.safetensors', 'model-00012-of-00015.safetensors', 'model-00013-of-00015.safetensors', 'model-00014-of-00015.safetensors', 'model-00015-of-00015.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer_config.json'], parameter_count=34000000000, model_type='llama'), conversation_config=ConversationConfig(style='chatml'), context_length=4096, created_by='zchenyu@fireworks.ai', sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=900000000), unit='1M tokens')], tunable=True, supports_lora=True, default_sampling_params={'temperature': 0.6000000238418579, 'top_k': 50.0, 'top_p': 0.800000011920929}),
Model(name='accounts/fireworks/models/yi-6b', display_name='Yi 6B', create_time=datetime.datetime(2023, 11, 21, 1, 40, 13, 573799, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='The Yi series models are the next generation of open-source large language models trained from scratch by 01.AI. Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.', github_url='https://github.com/01-ai/Yi', hugging_face_url='https://huggingface.co/01-ai/Yi-6B', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.NATIVE, parameter_count=6000000000), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
Model(name='accounts/fireworks/models/zephyr-7b-beta', display_name='Zephyr 7B Beta', create_time=datetime.datetime(2023, 11, 1, 23, 55, 13, 752655, tzinfo=datetime.timezone.utc), kind=ModelKind.HF_BASE_MODEL, state=ModelState.READY, status=Status(), public=True, description='Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B- is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO).', github_url='https://github.com/huggingface/alignment-handbook', hugging_face_url='https://huggingface.co/HuggingFaceH4/zephyr-7b-beta', base_model_details=BaseModelDetails(world_size=1, checkpoint_format=BaseModelDetailsCheckpointFormat.HUGGINGFACE, huggingface_files=['.gitattributes', 'README.md', 'added_tokens.json', 'all_results.json', 'config.json', 'eval_results.json', 'fireworks.json', 'generation_config.json', 'model-00001-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'model-00003-of-00008.safetensors', 'model-00004-of-00008.safetensors', 'model-00005-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00008-of-00008.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json', 'train_results.json', 'trainer_state.json', 'training_args.bin'], parameter_count=7000000000), context_length=4096, sku_infos=[SkuInfo(sku='LLM input tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens'), SkuInfo(sku='LLM output tokens', amount=Money(currency_code='USD', nanos=200000000), unit='1M tokens')], supports_lora=True, default_sampling_params={'temperature': 1.0, 'top_k': 50.0, 'top_p': 1.0}),
]
