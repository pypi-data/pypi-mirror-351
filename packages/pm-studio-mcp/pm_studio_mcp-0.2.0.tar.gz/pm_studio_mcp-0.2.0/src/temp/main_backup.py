from mcp.server.fastmcp import FastMCP
import os
from typing import List, Dict

# Import tools
from utils.file_utils import FileUtils
from Joe_backup.calendar import CalendarUtils
from utils.search_utils import SearchUtils
from utils.data_visualization_utils import DataVisualizationUtils
from utils.auth import AuthUtils
from utils.chat import ChatUtils
from skills.feedback_analysis import FeedbackAnalysisUtils
from skills.greeting import GreetingUtils
from skills.profile_analysis import ProfileAnalyzer

from pm_studio_mcp.constant import *
 
# Create MCP server instance
mcp = FastMCP("pm-studio-mcp")  # this is my mcp server name

# Configure WORKING_PATH from environment variables if provided
if 'WORKING_PATH' in os.environ:
    WORKING_PATH = os.environ['WORKING_PATH']
    print(f"Using configured working path: {WORKING_PATH}")
else:
    # Set a default working path for testing
    WORKING_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../temp')
    print(f"Using default working path for testing: {WORKING_PATH}")

# Create the working directory if it doesn't exist
os.makedirs(WORKING_PATH, exist_ok=True)

@mcp.tool()
async def greeting_with_pm_studio(name: str):  # this is the one of the tool of my MCP server
    """
    Respond to a greeting message with a formatted template.
    """
    return GreetingUtils.greeting_with_pm_studio(name)

# @mcp.tool()
# async def upload_files_to_working_dir_tool(file_paths: List[str]): #handle the file uploaded from the client
#     return upload_files_to_working_dir_tool(file_paths, WORKING_PATH)  # handle the file uploaded from the client


@mcp.tool()
async def ocv_feedback_data_clean_tool(input_files: List[str]):  # Receives file paths generated by the previous tool as parameters
    """
    Clean the OCV user feedback data files provided as input, and output the specified column to new intermediate files.
    Args:
        input_files (List[str]): List of input CSV file paths generated by the previous tool.
    Returns:
        List[str]: List of generated intermediate file paths.
    """
    return FeedbackAnalysisUtils.ocv_feedback_data_clean_tool(input_files, WORKING_PATH)  


@mcp.tool()
async def unwrap_feedback_data_clean_tool(input_files: List[str]):  # Receives file paths generated by the previous tool as parameters
    """
    Clean the unwrap user feedback data files provided as input, and output the specified column to new intermediate files.
    Args:
        input_files (List[str]): List of input CSV file paths generated by the previous tool.
    Returns:
        List[str]: List of generated intermediate file paths.
    """
    return FeedbackAnalysisUtils.unwrap_feedback_data_clean_tool(input_files, WORKING_PATH)


@mcp.tool()
async def merge_feedback_data_clean_tool(input_files: List[str]):  # Receives file paths generated by the previous tool as paramters
    """
    Merge the intermediate data files generated by unwrap_feedback_data_clean_tool and ocv_feedback_data_clean_tool.
    Args:
        input_files (List[str]): List of input CSV file paths generated by the previous tools.
    Returns:
        str: Path to the merged output file.
    """
    return FeedbackAnalysisUtils.merge_feedback_data_clean_tool(input_files, WORKING_PATH)

            
@mcp.tool()
async def write_to_csv_tool(content: str):
    """
    Write the given content to a CSV file.
    Args:
        content (str): The content to write to the CSV file. Content format per line: category,count,original comment 1|original comment 2|...
        output_file (str): The path to the output CSV file.
    """
    output_file = os.path.join(WORKING_PATH, FINAL_RESULT_FILE)
    return FileUtils.write_to_csv_tool(content, output_file)


@mcp.tool()
async def working_dir_cleanup_tool_intermediate():
    """
    Clean up the working directory by deleting all files with _cleaned.csv suffix.
    """
    return FeedbackAnalysisUtils.working_dir_cleanup_tool_intermediate(WORKING_PATH)


@mcp.tool()
async def google_web_tool(keywords: List[str], num_results: int = 10):
    """
    Perform a Google web search with the given query and return top 10 result URLs.
    
    Args:
        query: Search query
        num_results: Number of search results to return (default: 10)
        
    Returns:
        List of 10 search result URLs
    """
    return SearchUtils.search_google(keywords, num_results)

@mcp.tool()
async def generate_markdown_tool(content: str, filename: str):
    """
    Write the given content to a Markdown file in the working directory with a customizable filename.
            
    Args:
        content (str): The content to write to the Markdown file.
        filename (str, optional): The name of the output file.
            
    Returns:
        str: Path to the saved Markdown file.
    """
    return FileUtils.generate_markdown_tool(content, filename, WORKING_PATH)

@mcp.tool()
async def generate_pie_chart_tool(data: Dict[str, int]):
    """
    Generate a pie chart based on the provided data and save it as 'userfeedback_piechart.jpg'.

    Args:
        data (Dict[str, int]): A dictionary where keys are categories (str) and values are counts (int).
    
    Returns:
        str: Path to the saved pie chart image.
    """
    return DataVisualizationUtils.generate_pie_chart_tool(data, WORKING_PATH)

@mcp.tool()
async def analyze_user_profiles_tool(file_path: str, max_clusters: int = 10, id_column: str = None):
    """
    Perform cluster analysis on user data to generate user profiles
    
    Args:
        file_path (str): Path to user data file (Excel or CSV)
        max_clusters (int): Maximum number of clusters, default is 10
        id_column (str, optional): User ID column name, if any
        
    Returns:
        dict: Dictionary containing paths to all generated files, including cluster data files, cluster summary files, and visualization charts
    """
    result = ProfileAnalyzer.analyze_user_profiles(file_path, WORKING_PATH, max_clusters, id_column)
    if result["status"] == "success":
        # Return all generated file paths
        return {
            "status": "success",
            "message": result["message"],
            "cluster_count": result["cluster_count"],
            "files": result["result_files"]
        }
    else:
        return {
            "status": "error",
            "message": result["message"]
        }

@mcp.tool()
async def analyze_complete_cluster_profiles_tool(input_file: str, cluster_file: str = None, show_all_features: bool = False):
    """
    Analyze complete cluster user profiles, showing feature distribution for each cluster
    
    Args:
        input_file (str): Path to original data file (with cluster column)
        cluster_file (str, optional): Path to cluster results CSV file
        show_all_features (bool, optional): Whether to display all feature details in terminal
        
    Returns:
        str: Path to Markdown report file containing complete cluster analysis results
    """
    result = ProfileAnalyzer.analyze_complete_cluster_profiles(input_file, WORKING_PATH, cluster_file, show_all_features)
    if result["status"] == "success":
        return result["result_files"]["markdown_report"]
    else:
        return result["message"]

@mcp.tool()
async def convert_to_markdown_tool(file_path: str):
    """
    Convert a document (doc/excel/ppt/pdf/images/csv/json/xml) to markdown format using MarkItDown.

    Args:
        file_path (str): Path to the input document file

    Returns:
        str: Path to the generated markdown file or error message
    """
    return FileUtils.convert_to_markdown_tool(file_path, WORKING_PATH)

@mcp.tool()
async def scrape_reddit_tool(
    subreddit_name: str,
    keywords: List[str],
    client_id: str,
    client_secret: str,
    post_limit: int = 100,
    time_filter: str = "month"
):
    """
    Scrape posts from a Reddit subreddit and filter by keywords.
    
    Args:
        subreddit_name: Name of the subreddit to scrape
        keywords: List of keywords to filter posts by
        client_id: Reddit API client ID
        client_secret: Reddit API client secret
        post_limit: Maximum number of posts to retrieve (default: 100)
        time_filter: Time filter for posts (choices: "all", "year", "month", "week", "day", "new") (default: "month")
        
    Returns:
        Dictionary with status and results including path to CSV file with scraped data
    """
    return SearchUtils.scrape_reddit(
        subreddit_name, 
        keywords,
        client_id,
        client_secret,
        post_limit,
        time_filter,
        WORKING_PATH
    )

@mcp.tool()
async def login():
    """
    start authentication process against MSAL.

    Returns:
        bool: True if authentication is not needed, False otherwise
    """
    return AuthUtils.login()

@mcp.tool()
async def get_my_calendar_events(parameter: dict[str,str]):
    """
    Retrieve the user specified calendar events after authentication.

     Args:
        parameter: parameters dictionary for Microsoft Graph API request.
        it contains the following
        - $top: number of events to retrieve
        - $orderby: order of events
        - $filter: filter for events
        - $select: fields to select

    Returns:
        list: List of calendar events
    """
    events = CalendarUtils.get_my_calendar_events(parameter)
    return CalendarUtils.format_events(events)

@mcp.tool()
async def get_my_unread_messages():
    """
    Retrieve all unread messages from Microsoft Teams using MS Graph API.

    Returns:
        list: List of unread messages
    """
    result = ChatUtils.get_unread_teams_messages()
    if result["status"] == "success":
        return result["messages"]
    else:
        return result["message"]
    

@mcp.tool()
async def send_notes_to_myself(notes: str):
    """
    Send a note to myself in Microsoft Teams using MS Graph API.
    """
    result = ChatUtils.send_to_myself(notes)
    return result["message"]


def main():
    mcp.run(transport='stdio')

if __name__ == "__main__":
    main()



