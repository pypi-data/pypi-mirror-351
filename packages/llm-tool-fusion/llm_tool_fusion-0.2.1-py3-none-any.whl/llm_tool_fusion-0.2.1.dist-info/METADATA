Metadata-Version: 2.4
Name: llm-tool-fusion
Version: 0.2.1
Summary: Biblioteca Python que simplifica e unifica a definiÃ§Ã£o e chamada de ferramentas para grandes modelos de linguagem (LLMs). CompatÃ­vel com Ollama, LangChain, OpenAI e outros frameworks.
Home-page: https://github.com/caua1503/llm-tool-fusion
Author: Caua ramos
Author-email: cauamedinax@gmail.com
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: author
Dynamic: author-email
Dynamic: home-page
Dynamic: license-file

# llm-tool-fusion

<div align="center">
  <img src="logo.png" alt="LLM Tool Fusion Logo" width="300">
</div>

<div align="center">

[![Python](https://img.shields.io/badge/python->=3.12-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.2.0-orange.svg)](pyproject.toml)

</div>

## ğŸ‡§ğŸ‡· PortuguÃªs

### ğŸ“– DescriÃ§Ã£o

**llm-tool-fusion** Ã© uma biblioteca Python que simplifica e unifica a definiÃ§Ã£o e chamada de ferramentas para grandes modelos de linguagem (LLMs). CompatÃ­vel com frameworks populares que suportam tool calling, como Ollama, LangChain e OpenAI, ela permite integrar facilmente novas funÃ§Ãµes e mÃ³dulos, tornando o desenvolvimento de aplicativos avanÃ§ados de IA mais Ã¡gil e modular atraves de decoradores de funÃ§Ãµes.

### âœ¨ Principais Recursos

- ğŸ”§ **UnificaÃ§Ã£o de APIs**: Interface Ãºnica para diferentes frameworks de LLM
- ğŸš€ **IntegraÃ§Ã£o Simplificada**: Adicione novas ferramentas com facilidade
- ğŸ”— **Compatibilidade Ampla**: Suporte para Ollama, LangChain, OpenAI e outros
- ğŸ“¦ **Modularidade**: Arquitetura modular para desenvolvimento escalÃ¡vel
- âš¡ **Performance**: Otimizado para aplicaÃ§Ãµes em produÃ§Ã£o
- ğŸ“ **Menos Verbosidade**: Sintaxe simplificada para declaraÃ§Ã£o de funÃ§Ãµes
- ğŸ”„ **Processamento AutomÃ¡tico**: ExecuÃ§Ã£o automÃ¡tica de chamadas de ferramentas (opcional)

### ğŸš€ InstalaÃ§Ã£o

```bash
pip install llm-tool-fusion
```

### ğŸ“‹ Uso BÃ¡sico (Exemplo com OpenAI)

```python
from openai import OpenAI
from llm_tool_fusion import ToolCaller, process_tool_calls

# Inicializa o cliente OpenAI e o gerenciador de ferramentas
client = OpenAI()
manager = ToolCaller()

# Define uma ferramenta usando o decorador
@manager.tool
def calculate_price(price: float, discount: float) -> float:
    """
    Calcula o preÃ§o final com desconto
    
    Args:
        price (float): PreÃ§o base
        discount (float): Percentual de desconto
        
    Returns:
        float: PreÃ§o final com desconto
    """
    return price * (1 - discount / 100)

# Prepara a mensagem e faz a chamada ao LLM
messages = [
    {"role": "user", "content": "Calcule o preÃ§o final de um produto de R$100 com 20% de desconto"}
]

# Primeira chamada ao LLM
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=manager.get_tools()
)

available_tools = manager.get_map_tools()
async_available_tools = manager.get_name_async_tools()
    
# Processamento manual das chamadas de ferramentas
if response.choices[0].message.tool_calls:
    tool_results = []
    for tool_call in response.choices[0].message.tool_calls:
        if tool_call.function.name in available_tools:
            # ExecuÃ§Ã£o manual da ferramenta
            import json
            args = json.loads(tool_call.function.arguments)

            #verificaÃ§Ã£o se a ferramenta e assincrona
            result = available_tools[tool_call.function.name](**args) if tool_call.function.name not in async_available_tools else asyncio.run(available_tools[tool_call.function.name](**args)) 
                
            # Coloca os resultados em uma lista
            tool_results.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": tool_call.function.name,
                "content": str(result)
            })
        
    # Adiciona todas as respostas de uma vez
    messages.append(response.choices[0].message)
    messages.extend(tool_results)
        
    # Nova chamada para processar o resultado
    final_response = client.chat.completions.create(
        model=default_model,
        messages=messages
    )
            
    return final_response.choices[0].message.content

print(final_response)
```

### ğŸ”„ Processamento AutomÃ¡tico de Chamadas

O llm-tool-fusion oferece um sistema robusto e simples para processar chamadas de ferramentas (instruÃ§Ãµes de uso em examples):

```python
# FunÃ§Ã£o para chamadas ao LLM
llm_call_fn = lambda model, messages, tools: client.chat.completions.create(
    model=model, 
    messages=messages, 
    tools=tools
)

# Processamento automÃ¡tico de chamadas
final_response = process_tool_calls(
    response=response,           # Resposta inicial do LLM
    messages=messages,           # HistÃ³rico de mensagens
    tool_caller=manager,         # InstÃ¢ncia do ToolCaller
    model="gpt-4",              # Modelo a ser usado
    llm_call_fn=llm_call_fn,    # FunÃ§Ã£o de chamada ao LLM
    verbose=True,               # (opcional) Logs detalhados
    verbose_time=True,          # (opcional) MÃ©tricas de tempo
    clean_messages=True,        # (opcional) Retorna apenas o conteÃºdo da mensagem
    use_async_poll=False,       # (opcional) Executa ferramentas assÃ­ncronas em paralelo
    max_chained_calls=5         # (opcional) MÃ¡ximo de chamadas encadeadas
)
```

#### ğŸ¯ ParÃ¢metros Principais

- **`response`** (obrigatÃ³rio): Resposta inicial do modelo
- **`messages`** (obrigatÃ³rio): Lista de mensagens do chat
- **`tool_caller`** (obrigatÃ³rio): InstÃ¢ncia da classe ToolCaller
- **`model`** (obrigatÃ³rio): Nome do modelo
- **`llm_call_fn`** (obrigatÃ³rio): FunÃ§Ã£o que faz a chamada ao modelo

#### âš™ï¸ ParÃ¢metros Opcionais

- **`verbose`**: Exibe logs detalhados da execuÃ§Ã£o
- **`verbose_time`**: Mostra mÃ©tricas de tempo de execuÃ§Ã£o
- **`clean_messages`**: Retorna apenas o conteÃºdo da mensagem final
- **`use_async_poll`**: Executa ferramentas assÃ­ncronas em paralelo para melhor performance
- **`max_chained_calls`**: Limite de chamadas encadeadas (padrÃ£o: 5)

#### âš¡ Performance com `use_async_poll`

Quando vocÃª tem mÃºltiplas ferramentas assÃ­ncronas sendo chamadas simultaneamente, o parÃ¢metro `use_async_poll=True` oferece melhor performance:

```python
# Sem async_poll: ferramentas executam sequencialmente
final_response = process_tool_calls(
    # ... outros parÃ¢metros ...
    use_async_poll=False  # PadrÃ£o: execuÃ§Ã£o sequencial
)

# Com async_poll: ferramentas assÃ­ncronas executam em paralelo
final_response = process_tool_calls(
    # ... outros parÃ¢metros ...
    use_async_poll=True   # ExecuÃ§Ã£o paralela para melhor performance
)
```

#### âœ¨ CaracterÃ­sticas Principais

- ğŸ” **Ciclo AutomÃ¡tico**: Processa todas as chamadas de ferramentas atÃ© a conclusÃ£o
- âš¡ **Suporte AssÃ­ncrono**: Executa ferramentas sÃ­ncronas e assÃ­ncronas automaticamente
- ğŸ“ **Logs Inteligentes**: Acompanhe a execuÃ§Ã£o com logs detalhados e mÃ©tricas de tempo
- ğŸ›¡ï¸ **Tratamento de Erros**: Gerenciamento robusto de erros durante a execuÃ§Ã£o
- ğŸ’¬ **GestÃ£o de Contexto**: MantÃ©m o histÃ³rico de conversas organizado
- ğŸ”§ **ConfigurÃ¡vel**: Personalize o comportamento conforme sua necessidade

#### ğŸš€ VersÃ£o AssÃ­ncrona

Para aplicaÃ§Ãµes que precisam de processamento assÃ­ncrono:

```python
# Processamento assÃ­ncrono de chamadas
final_response = await process_tool_calls_async(
    response=response,
    messages=messages,
    tool_caller=manager,
    model="gpt-4",
    llm_call_fn=async_llm_call_fn,
    verbose=True,
    use_async_poll=True  # Recomendado para melhor performance
)
```

#### ğŸ”§ Suporte a Frameworks

O sistema funciona com diferentes frameworks atravÃ©s do parÃ¢metro `framework` no `ToolCaller`:

```python
# Para OpenAI (padrÃ£o)
manager = ToolCaller(framework="openai")

# Para Ollama
manager = ToolCaller(framework="ollama")
llm_call_fn = lambda model, messages, tools: ollama.Client().chat(
    model=model,
    messages=messages,
    tools=tools
)
```

### ğŸ”§ Frameworks Suportados

- **OpenAI** - API oficial e modelos GPT
- **LangChain** - Framework completo para aplicaÃ§Ãµes LLM
- **Ollama** - ExecuÃ§Ã£o local de modelos
- **Anthropic Claude** - API da Anthropic
- **E muito mais...**

### ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ‡ºğŸ‡¸ English

### ğŸ“– Description

**llm-tool-fusion** is a Python library that simplifies and unifies the definition and calling of tools for large language models (LLMs). Compatible with popular frameworks that support tool calling, such as Ollama, LangChain, and OpenAI, it allows you to easily integrate new functions and modules, making the development of advanced AI applications more agile and modular through function decorators.

### âœ¨ Key Features

- ğŸ”§ **API Unification**: Single interface for different LLM frameworks
- ğŸš€ **Simplified Integration**: Add new tools with ease
- ğŸ”— **Wide Compatibility**: Support for Ollama, LangChain, OpenAI, and others
- ğŸ“¦ **Modularity**: Modular architecture for scalable development
- âš¡ **Performance**: Optimized for production applications
- ğŸ“ **Less Verbosity**: Simplified syntax for function declarations
- ğŸ”„ **Automatic Processing**: Automatic execution of tool calls (optional)

### ğŸš€ Installation

```bash
pip install llm-tool-fusion
```

### ğŸ“‹ Basic Usage (Example with OpenAI)

```python
from openai import OpenAI
from llm_tool_fusion import ToolCaller, process_tool_calls

# Initialize OpenAI client and tool manager
client = OpenAI()
manager = ToolCaller()

# Define a tool using the decorator
@manager.tool
def calculate_price(price: float, discount: float) -> float:
    """
    Calculate final price with discount
    
    Args:
        price (float): Base price
        discount (float): Discount percentage
        
    Returns:
        float: Final price with discount
    """
    return price * (1 - discount / 100)

# Prepare message and make LLM call
messages = [
    {"role": "user", "content": "Calculate the final price of a $100 product with 20% discount"}
]

# First LLM call
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=manager.get_tools()
)

available_tools = manager.get_map_tools()
async_available_tools = manager.get_name_async_tools()
    
# Manual processing of tool calls
if response.choices[0].message.tool_calls:
    tool_results = []
    for tool_call in response.choices[0].message.tool_calls:
        if tool_call.function.name in available_tools:
            # Manual tool execution
            import json
            args = json.loads(tool_call.function.arguments)

print(final_response)
```

### ğŸ”„ Automatic Call Processing

llm-tool-fusion provides a robust and simple system for processing tool calls automatically:

```python
# Function for LLM calls
llm_call_fn = lambda model, messages, tools: client.chat.completions.create(
    model=model, 
    messages=messages, 
    tools=tools
)

# Automatic tool call processing
final_response = process_tool_calls(
    response=response,           # Initial response from the LLM
    messages=messages,           # Message history
    tool_caller=manager,         # ToolCaller instance
    model="gpt-4",              # Model to be used
    llm_call_fn=llm_call_fn,    # Function to call the LLM
    verbose=True,               # (optional) Detailed logs
    verbose_time=True,          # (optional) Time metrics
    clean_messages=True,        # (optional) Returns only message content
    use_async_poll=False,       # (optional) Execute async tools in parallel
    max_chained_calls=5         # (optional) Maximum chained calls
)
```

#### ğŸ¯ Main Parameters

- **`response`** (required): Initial response from the model
- **`messages`** (required): List of chat messages
- **`tool_caller`** (required): ToolCaller class instance
- **`model`** (required): Model name
- **`llm_call_fn`** (required): Function that calls the model

#### âš™ï¸ Optional Parameters

- **`verbose`**: Shows detailed execution logs
- **`verbose_time`**: Shows execution time metrics
- **`clean_messages`**: Returns only the final message content
- **`use_async_poll`**: Executes async tools in parallel for better performance
- **`max_chained_calls`**: Limit of chained calls (default: 5)

#### âš¡ Performance with `use_async_poll`

When you have multiple asynchronous tools being called simultaneously, the `use_async_poll=True` parameter offers better performance:

```python
# Without async_poll: tools execute sequentially
final_response = process_tool_calls(
    # ... other parameters ...
    use_async_poll=False  # Default: sequential execution
)

# With async_poll: async tools execute in parallel
final_response = process_tool_calls(
    # ... other parameters ...
    use_async_poll=True   # Parallel execution for better performance
)
```

#### âœ¨ Main Features

- ğŸ” **Automatic Loop**: Processes all tool calls to completion
- âš¡ **Asynchronous Support**: Runs synchronous and asynchronous tools automatically
- ğŸ“ **Smart Logs**: Track execution with detailed logs and time metrics
- ğŸ›¡ï¸ **Error Handling**: Robust error management during execution
- ğŸ’¬ **Context Management**: Keeps conversation history organized
- ğŸ”§ **Configurable**: Customize behavior to your needs

#### ğŸš€ Asynchronous Version

For applications that need asynchronous processing:

```python
# Asynchronous processing of tool calls
final_response = await process_tool_calls_async(
    response=response,
    messages=messages,
    tool_caller=manager,
    model="gpt-4",
    llm_call_fn=async_llm_call_fn,
    verbose=True,
    use_async_poll=True  # Recommended for better performance
)
```

#### ğŸ”§ Framework Support

The system works with different frameworks through the `framework` parameter in `ToolCaller`:

```python
# For OpenAI (default)
manager = ToolCaller(framework="openai")

# For Ollama
manager = ToolCaller(framework="ollama")
llm_call_fn = lambda model, messages, tools: ollama.Client().chat(
    model=model,
    messages=messages,
    tools=tools
)
```

### ğŸ”§ Supported Frameworks

- **OpenAI** - Official API and GPT models
- **LangChain** - Complete framework for LLM applications
- **Ollama** - Local model execution
- **Anthropic Claude** - Anthropic's API
- **And many more...**

### ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ› ï¸ Development

### Prerequisites

- Python >= 3.12
- pip or poetry for dependency management

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/caua1503/llm-tool-fusion.git
cd llm-tool-fusion

# Install dependencies
pip install -e .

# Run tests
python -m pytest
```

### Project Structure

```
llm-tool-fusion/
â”œâ”€â”€ llm_tool_fusion/
â”‚   â””â”€â”€ __init__.py
|   â””â”€â”€ _core.py
|   â””â”€â”€ _utils.py
â”‚      
â”œâ”€â”€ tests/
â”œâ”€â”€ examples/
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

**â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela no GitHub!**

**â­ If this project was helpful to you, consider starring it on GitHub!**
