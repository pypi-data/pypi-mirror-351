# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['vllm_block',
 'vllm_block.adapter_commons',
 'vllm_block.assets',
 'vllm_block.attention',
 'vllm_block.attention.backends',
 'vllm_block.attention.backends.mla',
 'vllm_block.attention.ops',
 'vllm_block.attention.ops.blocksparse_attention',
 'vllm_block.benchmarks',
 'vllm_block.compilation',
 'vllm_block.core',
 'vllm_block.core.block',
 'vllm_block.device_allocator',
 'vllm_block.distributed',
 'vllm_block.distributed.device_communicators',
 'vllm_block.distributed.kv_transfer',
 'vllm_block.distributed.kv_transfer.kv_connector',
 'vllm_block.distributed.kv_transfer.kv_lookup_buffer',
 'vllm_block.distributed.kv_transfer.kv_pipe',
 'vllm_block.engine',
 'vllm_block.engine.multiprocessing',
 'vllm_block.engine.output_processor',
 'vllm_block.entrypoints',
 'vllm_block.entrypoints.cli',
 'vllm_block.entrypoints.cli.benchmark',
 'vllm_block.entrypoints.openai',
 'vllm_block.entrypoints.openai.reasoning_parsers',
 'vllm_block.entrypoints.openai.tool_parsers',
 'vllm_block.executor',
 'vllm_block.inputs',
 'vllm_block.logging_utils',
 'vllm_block.lora',
 'vllm_block.lora.ops',
 'vllm_block.lora.ops.torch_ops',
 'vllm_block.lora.ops.triton_ops',
 'vllm_block.lora.punica_wrapper',
 'vllm_block.model_executor',
 'vllm_block.model_executor.guided_decoding',
 'vllm_block.model_executor.guided_decoding.reasoner',
 'vllm_block.model_executor.layers',
 'vllm_block.model_executor.layers.fused_moe',
 'vllm_block.model_executor.layers.mamba',
 'vllm_block.model_executor.layers.mamba.ops',
 'vllm_block.model_executor.layers.quantization',
 'vllm_block.model_executor.layers.quantization.compressed_tensors',
 'vllm_block.model_executor.layers.quantization.compressed_tensors.schemes',
 'vllm_block.model_executor.layers.quantization.kernels',
 'vllm_block.model_executor.layers.quantization.kernels.mixed_precision',
 'vllm_block.model_executor.layers.quantization.kernels.scaled_mm',
 'vllm_block.model_executor.layers.quantization.quark',
 'vllm_block.model_executor.layers.quantization.quark.schemes',
 'vllm_block.model_executor.layers.quantization.utils',
 'vllm_block.model_executor.model_loader',
 'vllm_block.model_executor.models',
 'vllm_block.multimodal',
 'vllm_block.platforms',
 'vllm_block.plugins',
 'vllm_block.profiler',
 'vllm_block.prompt_adapter',
 'vllm_block.spec_decode',
 'vllm_block.third_party',
 'vllm_block.transformers_utils',
 'vllm_block.transformers_utils.configs',
 'vllm_block.transformers_utils.processors',
 'vllm_block.transformers_utils.tokenizer_group',
 'vllm_block.transformers_utils.tokenizers',
 'vllm_block.triton_utils',
 'vllm_block.usage',
 'vllm_block.v1',
 'vllm_block.v1.attention',
 'vllm_block.v1.attention.backends',
 'vllm_block.v1.attention.backends.mla',
 'vllm_block.v1.core',
 'vllm_block.v1.core.sched',
 'vllm_block.v1.engine',
 'vllm_block.v1.executor',
 'vllm_block.v1.metrics',
 'vllm_block.v1.sample',
 'vllm_block.v1.sample.ops',
 'vllm_block.v1.sample.tpu',
 'vllm_block.v1.spec_decode',
 'vllm_block.v1.stats',
 'vllm_block.v1.structured_output',
 'vllm_block.v1.worker',
 'vllm_block.worker']

package_data = \
{'': ['*'],
 'vllm_block': ['vllm_flash_attn/*'],
 'vllm_block.model_executor.layers.fused_moe': ['configs/*'],
 'vllm_block.model_executor.layers.quantization.utils': ['configs/*']}

setup_kwargs = {
    'name': 'vllm-block',
    'version': '0.1.0',
    'description': 'Block implementation of vLLM',
    'long_description': None,
    'author': 'Wei Da',
    'author_email': 'wd312@cam.ac.uk',
    'maintainer': None,
    'maintainer_email': None,
    'url': None,
    'packages': packages,
    'package_data': package_data,
}


setup(**setup_kwargs)
