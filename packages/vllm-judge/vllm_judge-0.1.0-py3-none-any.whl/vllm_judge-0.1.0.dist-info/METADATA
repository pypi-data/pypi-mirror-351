Metadata-Version: 2.4
Name: vllm_judge
Version: 0.1.0
Summary: LLM-as-a-Judge evaluations for vLLM hosted models
Author: TrustyAI team
Author-email: Sai Chandra Pandraju <saichandrapandraju@gmail.com>
Project-URL: Homepage, https://github.com/saichandrapandraju/vllm_judge
Project-URL: Repository, https://github.com/saichandrapandraju/vllm_judge
Project-URL: Issues, https://github.com/saichandrapandraju/vllm_judge/issues
Keywords: llm,evaluation,vllm,judge,ai,machine-learning,nlp,llm-evaluation,llm-as-judge
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: httpx>=0.24.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: tenacity>=8.0.0
Requires-Dist: click>=8.0.0
Provides-Extra: api
Requires-Dist: fastapi>=0.100.0; extra == "api"
Requires-Dist: uvicorn[standard]>=0.22.0; extra == "api"
Requires-Dist: websockets>=11.0; extra == "api"
Provides-Extra: jinja2
Requires-Dist: jinja2>=3.0.0; extra == "jinja2"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "test"
Requires-Dist: pytest-cov>=4.0.0; extra == "test"
Requires-Dist: pytest-mock>=3.10.0; extra == "test"
Provides-Extra: docs
Requires-Dist: mkdocs>=1.5.0; extra == "docs"
Requires-Dist: mkdocs-material>=9.0.0; extra == "docs"
Requires-Dist: mkdocstrings[python]>=0.24.0; extra == "docs"

# vLLM Judge

A lightweight library for LLM-as-a-Judge evaluations using vLLM hosted models.

## Features

- üöÄ **Simple Interface**: Single `evaluate()` method that adapts to any use case
- üéØ **Pre-built Metrics**: 20+ ready-to-use evaluation metrics
- üîß **Template Support**: Dynamic evaluations with template variables
- ‚ö° **High Performance**: Optimized for vLLM with automatic batching
- üåê **API Mode**: Run as a REST API service
- üîÑ **Async Native**: Built for high-throughput evaluations

## Installation

```bash
# Basic installation
pip install vllm_judge

# With API support
pip install vllm_judge[api]

# With Jinja2 template support
pip install vllm_judge[jinja2]

# Everything
pip install vllm_judge[api,jinja2]
```

## Quick Start

```python
from vllm_judge import Judge

# Initialize with vLLM url
judge = await Judge.from_url("http://localhost:8000")

# Simple evaluation
result = await judge.evaluate(
    response="The Earth orbits around the Sun.",
    criteria="scientific accuracy"
)
print(f"Decision: {result.decision}")
print(f"Reasoning: {result.reasoning}")

# Using pre-built metrics
from vllm_judge import CODE_QUALITY

result = await judge.evaluate(
    response="def add(a, b): return a + b",
    metric=CODE_QUALITY
)

# With template variables
result = await judge.evaluate(
    response="Essay content here...",
    criteria="Evaluate this {doc_type} for {audience}",
    template_vars={
        "doc_type": "essay",
        "audience": "high school students"
    }
)
```

## API Server

Run Judge as a REST API:

```bash
vllm-judge serve --base-url http://localhost:8000 --port 9090 --host localhost
```

Then use the HTTP API:

```python
from vllm_judge.api import JudgeClient

client = JudgeClient("http://localhost:9090")
result = await client.evaluate(
    response="Python is great!",
    criteria="technical accuracy"
)
```

