{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DerivaML Dataset\n",
    "\n",
    "DerivaML is a class library built on the Deriva Scientific Asset management system that is designed to help simplify a number of the basic operations associated with building and testing ML libraries based on common toolkits such as TensorFlow.  This notebook reviews the basic features of the DerivaML library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Set up DerivaML  for test case"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:12.754240Z",
     "start_time": "2025-05-15T16:17:12.716077Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:14.159468Z",
     "start_time": "2025-05-15T16:17:12.900332Z"
    }
   },
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "from deriva_ml.demo_catalog import create_demo_catalog, DemoML\n",
    "from deriva_ml import MLVocab, ExecutionConfiguration, Workflow, DerivaSystemColumns, VersionPart, DatasetSpec\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML, JSON"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Set the details for the catalog we want and authenticate to the server if needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "1b84d2da0ec8fa7d",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:14.364023Z",
     "start_time": "2025-05-15T16:17:14.350787Z"
    }
   },
   "source": [
    "hostname = 'localhost'\n",
    "domain_schema = 'demo-schema'"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:15.446580Z",
     "start_time": "2025-05-15T16:17:14.381123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gnl = GlobusNativeLogin(host=hostname)\n",
    "if gnl.is_logged_in([hostname]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([hostname], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")\n"
   ],
   "id": "5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Create a test catalog and get an instance of the DemoML class."
   ]
  },
  {
   "cell_type": "code",
   "id": "7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:27.419521Z",
     "start_time": "2025-05-15T16:17:15.474102Z"
    }
   },
   "source": [
    "test_catalog = create_demo_catalog(hostname, domain_schema)\n",
    "ml_instance = DemoML(hostname, test_catalog.catalog_id, use_minid=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:17:22,612 - deriva_ml.WARNING - File /Users/carl/Repos/Projects/deriva-ml/docs/Notebooks/DerivaML Dataset.ipynb has been modified since last commit. Consider commiting before executing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Execution RID: https://localhost/id/2/3NG@33A-BR05-5SMY"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Configure DerivaML Datasets\n",
    "\n",
    "In Deriva-ML a dataset is used to aggregate instances of entities.  However, before we can create any datasets, we must configure \n",
    "Deriva-ML for the specifics of the datasets.  The first stp is we need to tell Deriva-ML what types of use defined objects can be associated with a dataset.  \n",
    "\n",
    "Note that out of the box, Deriva-ML is configured to allow datasets to contained dataset (i.e. nested datasets), so we don't need to do anything for that specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:27.888525Z",
     "start_time": "2025-05-15T16:17:27.440534Z"
    }
   },
   "source": [
    "print(f\"Current dataset_table element types: {[a.name for a in ml_instance.list_dataset_element_types()]}\")\n",
    "ml_instance.add_dataset_element_type(\"Subject\")\n",
    "ml_instance.add_dataset_element_type(\"Image\")\n",
    "print(f\"New dataset_table element types {[a.name for a in ml_instance.list_dataset_element_types()]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset_table element types: ['Dataset']\n",
      "New dataset_table element types ['Dataset', 'Subject', 'Image']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now that we have configured our datasets, we need to identify the dataset types so we can distinguish between them."
   ]
  },
  {
   "cell_type": "code",
   "id": "11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:28.164153Z",
     "start_time": "2025-05-15T16:17:27.909882Z"
    }
   },
   "source": [
    "# Create a new dataset_table\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"DemoSet\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, 'Partitioned', description=\"A partitioned dataset_table for ML training.\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Subject\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Image\", description=\"A test dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Training\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Testing\", description=\"Training dataset_table\")\n",
    "ml_instance.add_term(MLVocab.dataset_type, \"Validation\", description=\"Validation dataset_table\")\n",
    "\n",
    "ml_instance.list_vocabulary_terms(MLVocab.dataset_type)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VocabularyTerm(name='DemoSet', synonyms=[], id='ml-test:3V8', uri='/id/3V8', description='A test dataset_table', rid='3V8'),\n",
       " VocabularyTerm(name='Partitioned', synonyms=[], id='ml-test:3VA', uri='/id/3VA', description='A partitioned dataset_table for ML training.', rid='3VA'),\n",
       " VocabularyTerm(name='Subject', synonyms=[], id='ml-test:3VC', uri='/id/3VC', description='A test dataset_table', rid='3VC'),\n",
       " VocabularyTerm(name='Image', synonyms=[], id='ml-test:3VE', uri='/id/3VE', description='A test dataset_table', rid='3VE'),\n",
       " VocabularyTerm(name='Training', synonyms=[], id='ml-test:3VG', uri='/id/3VG', description='Training dataset_table', rid='3VG'),\n",
       " VocabularyTerm(name='Testing', synonyms=[], id='ml-test:3VJ', uri='/id/3VJ', description='Training dataset_table', rid='3VJ'),\n",
       " VocabularyTerm(name='Validation', synonyms=[], id='ml-test:3VM', uri='/id/3VM', description='Validation dataset_table', rid='3VM')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now create datasets and populate with elements from the test catalogs."
   ]
  },
  {
   "cell_type": "code",
   "id": "13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:28.550241Z",
     "start_time": "2025-05-15T16:17:28.184565Z"
    }
   },
   "source": [
    "ml_instance.add_term(MLVocab.workflow_type, \"Create Dataset Notebook\", description=\"A Workflow that creates a new dataset_table\")\n",
    "\n",
    "# Now lets create model configuration for our program.\n",
    "api_workflow = Workflow(\n",
    "    name=\"API Workflow\",\n",
    "    url=\"https://github.com/informatics-isi-edu/deriva-ml/blob/main/docs/Notebooks/DerivaML%20Dataset.ipynb\",\n",
    "    workflow_type=\"Create Dataset Notebook\"\n",
    ")\n",
    "\n",
    "dataset_execution = ml_instance.create_execution(\n",
    "    ExecutionConfiguration(\n",
    "        workflow=api_workflow,\n",
    "        description=\"Our Sample Workflow instance\")\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:17:28,327 - deriva_ml.INFO - Downloading assets ...\n",
      "2025-05-15 09:17:28,520 - deriva_ml.INFO - Initialize status finished.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:28.882583Z",
     "start_time": "2025-05-15T16:17:28.570410Z"
    }
   },
   "source": [
    "subject_dataset = dataset_execution.create_dataset(['DemoSet', 'Subject'], description=\"A subject dataset_table\")\n",
    "image_dataset = dataset_execution.create_dataset(['DemoSet', 'Image'], description=\"A image training dataset_table\")\n",
    "datasets = pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)\n",
    "display(\n",
    "    Markdown('## Datasets'),\n",
    "    datasets)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Datasets"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                      Description  Deleted Version MLVocab.dataset_type\n",
       "0         A subject dataset_table    False     3W4   [DemoSet, Subject]\n",
       "1  A image training dataset_table    False     3WE     [DemoSet, Image]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Deleted</th>\n",
       "      <th>Version</th>\n",
       "      <th>MLVocab.dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A subject dataset_table</td>\n",
       "      <td>False</td>\n",
       "      <td>3W4</td>\n",
       "      <td>[DemoSet, Subject]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A image training dataset_table</td>\n",
       "      <td>False</td>\n",
       "      <td>3WE</td>\n",
       "      <td>[DemoSet, Image]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "And now that we have defined some datasets, we can add elements of the appropriate type to them.  We can see what is in our new datasets by listing the dataset members."
   ]
  },
  {
   "cell_type": "code",
   "id": "16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:29.793194Z",
     "start_time": "2025-05-15T16:17:28.934451Z"
    }
   },
   "source": [
    "# Get list of subjects and images from the catalog using the DataPath API.\n",
    "dp = ml_instance.domain_path  # Each call returns a new path instance, so only call once...\n",
    "subject_rids = [i['RID'] for i in dp.tables['Subject'].entities().fetch()]\n",
    "image_rids = [i['RID'] for i in dp.tables['Image'].entities().fetch()]\n",
    "\n",
    "ml_instance.add_dataset_members(dataset_rid=subject_dataset, members=subject_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=image_dataset, members=image_rids)\n",
    "\n",
    "# List the contents of our datasets, and let's not include columns like modify time.\n",
    "display(\n",
    "    Markdown('## Subject Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(subject_dataset)['Subject']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Image Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(image_dataset)['Image']).drop(columns=DerivaSystemColumns))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Subject Dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     Name\n",
       "0  Thing1\n",
       "1  Thing2\n",
       "2  Thing3\n",
       "3  Thing4"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thing1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thing3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thing4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Image Dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                                                 URL      Filename  \\\n",
       "0  /hatrac/Image/76ad71f5f58c1d43bc952d94c90503cc...  test_3N4.txt   \n",
       "1  /hatrac/Image/0c6d7a6fa248293258bca0babe6dfc4d...  test_3N8.txt   \n",
       "2  /hatrac/Image/7ec56c23ed13e79a897ed33ace86db42...  test_3N6.txt   \n",
       "3  /hatrac/Image/a0ed5af616f045d98bedcfc0ff429820...  test_3NA.txt   \n",
       "\n",
       "  Description  Length                               MD5 Subject  \n",
       "0        None      31  76ad71f5f58c1d43bc952d94c90503cc     3N4  \n",
       "1        None      31  0c6d7a6fa248293258bca0babe6dfc4d     3N8  \n",
       "2        None      32  7ec56c23ed13e79a897ed33ace86db42     3N6  \n",
       "3        None      31  a0ed5af616f045d98bedcfc0ff429820     3NA  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Description</th>\n",
       "      <th>Length</th>\n",
       "      <th>MD5</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/hatrac/Image/76ad71f5f58c1d43bc952d94c90503cc...</td>\n",
       "      <td>test_3N4.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>76ad71f5f58c1d43bc952d94c90503cc</td>\n",
       "      <td>3N4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/hatrac/Image/0c6d7a6fa248293258bca0babe6dfc4d...</td>\n",
       "      <td>test_3N8.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>0c6d7a6fa248293258bca0babe6dfc4d</td>\n",
       "      <td>3N8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/hatrac/Image/7ec56c23ed13e79a897ed33ace86db42...</td>\n",
       "      <td>test_3N6.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>7ec56c23ed13e79a897ed33ace86db42</td>\n",
       "      <td>3N6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/hatrac/Image/a0ed5af616f045d98bedcfc0ff429820...</td>\n",
       "      <td>test_3NA.txt</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>a0ed5af616f045d98bedcfc0ff429820</td>\n",
       "      <td>3NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Create partitioned dataset\n",
    "\n",
    "Now let's create some subsets of the original dataset based on subject level metadata. We are going to create the subsets based on the metadata values of the subjects. We will download the subject dataset and look at its metadata to figure out how to partition the original data. Since we are not going to look at the images, we use the materialize=False option to save some time."
   ]
  },
  {
   "cell_type": "code",
   "id": "18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T16:17:31.976222Z",
     "start_time": "2025-05-15T16:17:29.851634Z"
    }
   },
   "source": [
    "dataset_bag = ml_instance.download_dataset_bag(DatasetSpec(rid=subject_dataset, version=ml_instance.dataset_version(subject_dataset), materialize=False))\n",
    "print(f\"Bag materialized\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:17:30,396 - deriva_ml.INFO - Downloading dataset minid for catalog: 3VW@0.2.0\n",
      "2025-05-15 09:17:30,449 - deriva.transfer.download.deriva_export.INFO - Processing export config file: /var/folders/0k/27qzm97x3t7g3j1m6ksf_9f40000gn/T/tmpcx062rvu/download_spec.json\n",
      "2025-05-15 09:17:30,450 - deriva.transfer.download.deriva_export.INFO - Requesting bdbag export at: https://localhost/deriva/export/bdbag\n"
     ]
    },
    {
     "ename": "DerivaMLException",
     "evalue": "[DerivaDownloadAuthenticationError] The requested service requires authentication and a valid login session could not be found for the specified host. Server responded: [HTTPError] 401 Client Error: Unauthorized for url: https://localhost/deriva/export/bdbag - Server responded: None Detail: [DerivaDownloadAuthenticationError] [HTTPError] 401 Client Error: UNAUTHORIZED for url: [https://localhost/ermrest/catalog/2@33A-BR0H-F5GP/entity/M:=deriva-ml:Dataset/RID=3VW] Details: b'Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID\\n' - Server responded: Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-test/lib/python3.10/site-packages/deriva/transfer/download/deriva_export.py:170\u001B[0m, in \u001B[0;36mDerivaExport.export\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    169\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession\u001B[38;5;241m.\u001B[39mpost(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mservice_url, json\u001B[38;5;241m=\u001B[39mconfig)\n\u001B[0;32m--> 170\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m result_urls \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-test/lib/python3.10/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://localhost/deriva/export/bdbag",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mDerivaDownloadAuthenticationError\u001B[0m         Traceback (most recent call last)",
      "File \u001B[0;32m~/Repos/Projects/deriva-ml/src/deriva_ml/dataset.py:1027\u001B[0m, in \u001B[0;36mDataset._create_dataset_minid\u001B[0;34m(self, dataset, snapshot_catalog)\u001B[0m\n\u001B[1;32m   1019\u001B[0m     exporter \u001B[38;5;241m=\u001B[39m DerivaExport(\n\u001B[1;32m   1020\u001B[0m         host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mcatalog\u001B[38;5;241m.\u001B[39mderiva_server\u001B[38;5;241m.\u001B[39mserver,\n\u001B[1;32m   1021\u001B[0m         config_file\u001B[38;5;241m=\u001B[39mspec_file,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1025\u001B[0m         envars\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRID\u001B[39m\u001B[38;5;124m\"\u001B[39m: dataset\u001B[38;5;241m.\u001B[39mrid},\n\u001B[1;32m   1026\u001B[0m     )\n\u001B[0;32m-> 1027\u001B[0m     minid_page_url \u001B[38;5;241m=\u001B[39m \u001B[43mexporter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# Get the MINID launch page\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m   1030\u001B[0m     DerivaDownloadError,\n\u001B[1;32m   1031\u001B[0m     DerivaDownloadConfigurationError,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m     DerivaDownloadTimeoutError,\n\u001B[1;32m   1035\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-test/lib/python3.10/site-packages/deriva/transfer/download/deriva_export.py:194\u001B[0m, in \u001B[0;36mDerivaExport.export\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m requests\u001B[38;5;241m.\u001B[39mcodes\u001B[38;5;241m.\u001B[39munauthorized:\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DerivaDownloadAuthenticationError(\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe requested service requires authentication and a valid login session could \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be found for the specified host. Server responded: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m format_exception(e))\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m requests\u001B[38;5;241m.\u001B[39mcodes\u001B[38;5;241m.\u001B[39mforbidden:\n",
      "\u001B[0;31mDerivaDownloadAuthenticationError\u001B[0m: The requested service requires authentication and a valid login session could not be found for the specified host. Server responded: [HTTPError] 401 Client Error: Unauthorized for url: https://localhost/deriva/export/bdbag - Server responded: None Detail: [DerivaDownloadAuthenticationError] [HTTPError] 401 Client Error: UNAUTHORIZED for url: [https://localhost/ermrest/catalog/2@33A-BR0H-F5GP/entity/M:=deriva-ml:Dataset/RID=3VW] Details: b'Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID\\n' - Server responded: Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mDerivaMLException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataset_bag \u001B[38;5;241m=\u001B[39m \u001B[43mml_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_dataset_bag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDatasetSpec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubject_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mml_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubject_dataset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaterialize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBag materialized\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-test/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:39\u001B[0m, in \u001B[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(wrapped)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapper_function\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deriva-test/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:136\u001B[0m, in \u001B[0;36mValidateCallWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__pydantic_complete__:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_validators()\n\u001B[0;32m--> 136\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpydantic_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mArgsKwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__return_pydantic_validator__:\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__return_pydantic_validator__(res)\n",
      "File \u001B[0;32m~/Repos/Projects/deriva-ml/src/deriva_ml/deriva_ml_base.py:780\u001B[0m, in \u001B[0;36mDerivaML.download_dataset_bag\u001B[0;34m(self, dataset, execution_rid)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[38;5;129m@validate_call\u001B[39m(config\u001B[38;5;241m=\u001B[39mConfigDict(arbitrary_types_allowed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    765\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdownload_dataset_bag\u001B[39m(\n\u001B[1;32m    766\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    767\u001B[0m     dataset: DatasetSpec,\n\u001B[1;32m    768\u001B[0m     execution_rid: Optional[RID] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    769\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DatasetBag:\n\u001B[1;32m    770\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Download a dataset onto the local file system.  Create a MINID for the dataset if one doesn't already exist.\u001B[39;00m\n\u001B[1;32m    771\u001B[0m \n\u001B[1;32m    772\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;124;03m        for the dataset.\u001B[39;00m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 780\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_dataset_bag\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    781\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexecution_rid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexecution_rid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    783\u001B[0m \u001B[43m        \u001B[49m\u001B[43msnapshot_catalog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDerivaML\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhost_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_version_snapshot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Projects/deriva-ml/src/deriva_ml/dataset.py:985\u001B[0m, in \u001B[0;36mDataset._download_dataset_bag\u001B[0;34m(self, dataset, execution_rid, snapshot_catalog)\u001B[0m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    980\u001B[0m     execution_rid\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m execution_rid \u001B[38;5;241m!=\u001B[39m DRY_RUN_RID\n\u001B[1;32m    982\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mcatalog\u001B[38;5;241m.\u001B[39mresolve_rid(execution_rid)\u001B[38;5;241m.\u001B[39mtable\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecution\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    983\u001B[0m ):\n\u001B[1;32m    984\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DerivaMLException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexecution_rid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not an execution\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 985\u001B[0m minid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_dataset_minid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_catalog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msnapshot_catalog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    987\u001B[0m bag_path \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    988\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_materialize_dataset_bag(minid, execution_rid\u001B[38;5;241m=\u001B[39mexecution_rid)\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mmaterialize\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_dataset_minid(minid)\n\u001B[1;32m    991\u001B[0m )\n\u001B[1;32m    992\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatabaseModel(minid, bag_path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_working_dir)\u001B[38;5;241m.\u001B[39mget_dataset()\n",
      "File \u001B[0;32m~/Repos/Projects/deriva-ml/src/deriva_ml/dataset.py:1094\u001B[0m, in \u001B[0;36mDataset._get_dataset_minid\u001B[0;34m(self, dataset, snapshot_catalog, create)\u001B[0m\n\u001B[1;32m   1092\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_minid:\n\u001B[1;32m   1093\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating new MINID for dataset \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, dataset\u001B[38;5;241m.\u001B[39mrid)\n\u001B[0;32m-> 1094\u001B[0m     minid_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataset_minid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_catalog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;66;03m# If provided a MINID, use the MINID metadata to get the checksum and download the bag.\u001B[39;00m\n\u001B[1;32m   1096\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_minid:\n",
      "File \u001B[0;32m~/Repos/Projects/deriva-ml/src/deriva_ml/dataset.py:1036\u001B[0m, in \u001B[0;36mDataset._create_dataset_minid\u001B[0;34m(self, dataset, snapshot_catalog)\u001B[0m\n\u001B[1;32m   1027\u001B[0m     minid_page_url \u001B[38;5;241m=\u001B[39m exporter\u001B[38;5;241m.\u001B[39mexport()[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# Get the MINID launch page\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m   1030\u001B[0m     DerivaDownloadError,\n\u001B[1;32m   1031\u001B[0m     DerivaDownloadConfigurationError,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m     DerivaDownloadTimeoutError,\n\u001B[1;32m   1035\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DerivaMLException(format_exception(e))\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m# Update version table with MINID.\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_minid:\n",
      "\u001B[0;31mDerivaMLException\u001B[0m: [DerivaDownloadAuthenticationError] The requested service requires authentication and a valid login session could not be found for the specified host. Server responded: [HTTPError] 401 Client Error: Unauthorized for url: https://localhost/deriva/export/bdbag - Server responded: None Detail: [DerivaDownloadAuthenticationError] [HTTPError] 401 Client Error: UNAUTHORIZED for url: [https://localhost/ermrest/catalog/2@33A-BR0H-F5GP/entity/M:=deriva-ml:Dataset/RID=3VW] Details: b'Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID\\n' - Server responded: Access requires authentication. Detail: select access on :deriva-ml:Dataset:RID"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The domain model has two objects: Subject and Images where an Image is associated with a subject, but a subject can have multiple images associated with it.  Let's look at the subjects and partition into test and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "id": "20",
   "metadata": {},
   "source": [
    "# Get information about the subjects.....\n",
    "subject_df = dataset_bag.get_table_as_dataframe('Subject')[['RID', 'Name']]\n",
    "image_df = dataset_bag.get_table_as_dataframe('Image')[['RID', 'Subject', 'URL']]\n",
    "metadata_df = subject_df.join(image_df, lsuffix=\"_subject\", rsuffix=\"_image\")\n",
    "display(metadata_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "For ths example, lets partition the data based on the name of the subject.  Of course in real examples, we would do a more complex analysis in deciding\n",
    "what subset goes into each data set."
   ]
  },
  {
   "cell_type": "code",
   "id": "22",
   "metadata": {},
   "source": [
    "def thing_number(name: pd.Series) -> pd.Series:\n",
    "    return name.map(lambda n: int(n.replace('Thing','')))\n",
    "\n",
    "training_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 0]['RID_image'].tolist()\n",
    "testing_rids =  metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 1]['RID_image'].tolist()\n",
    "validation_rids = metadata_df.loc[lambda df: thing_number(df['Name']) % 3 == 2]['RID_image'].tolist()\n",
    "\n",
    "print(f'Training images: {training_rids}')\n",
    "print(f'Testing images: {testing_rids}')\n",
    "print(f'Validation images: {validation_rids}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now that we know what we want in each dataset, lets create datasets for each of our partitioned elements along with a nested dataset to track the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "id": "24",
   "metadata": {},
   "source": [
    "nested_dataset = dataset_execution.create_dataset(['Partitioned', 'Image'], description='A nested dataset_table for machine learning')\n",
    "training_dataset = dataset_execution.create_dataset('Training', description='An image dataset_table for training')\n",
    "testing_dataset = dataset_execution.create_dataset('Testing', description='A image dataset_table for testing')\n",
    "validation_dataset = dataset_execution.create_dataset('Validation', description='A image dataset_table for validation')\n",
    "pd.DataFrame(ml_instance.find_datasets()).drop(columns=DerivaSystemColumns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "And then fill the datasets with the appropriate members."
   ]
  },
  {
   "cell_type": "code",
   "id": "26",
   "metadata": {},
   "source": [
    "ml_instance.add_dataset_members(dataset_rid=nested_dataset, members=[training_dataset, testing_dataset, validation_dataset])\n",
    "ml_instance.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=testing_dataset, members=testing_rids)\n",
    "ml_instance.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Ok, lets see what we have now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "As our very last step, lets get a PID that will allow us to share and cite the dataset that we just created"
   ]
  },
  {
   "cell_type": "code",
   "id": "29",
   "metadata": {},
   "source": [
    "display(\n",
    "    Markdown('## Nested Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(nested_dataset)['Dataset']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Training Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(training_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Testing Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(testing_dataset)['Image']).drop(columns=DerivaSystemColumns),\n",
    "    Markdown('## Validation Dataset'),\n",
    "    pd.DataFrame(ml_instance.list_dataset_members(validation_dataset)['Image']).drop(columns=DerivaSystemColumns),)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30",
   "metadata": {},
   "source": [
    "print(f'Dataset parents: {ml_instance.list_dataset_parents(training_dataset)}')\n",
    "print(f'Dataset children: {ml_instance.list_dataset_children(nested_dataset)}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31",
   "metadata": {},
   "source": [
    "dataset_citation = ml_instance.cite(nested_dataset)\n",
    "display(\n",
    "    HTML(f'Nested dataset_table citation: <a href={dataset_citation}>{dataset_citation}</a>')\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32",
   "metadata": {},
   "source": [
    "display(\n",
    "     Markdown('## Nested Dataset -- Recursive Listing'),\n",
    "    JSON(ml_instance.list_dataset_members(nested_dataset, recurse=True))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Dataset Versions\n",
    "Datasets have a version number which can be retrieved or incremented.  We follow the equivalent of semantic versioning, but for data rather than code.  Note that datasets are also versioned by virtue of the fact that the dataset RID can include a catalog snapshot ID as well."
   ]
  },
  {
   "cell_type": "code",
   "id": "34",
   "metadata": {},
   "source": [
    "print(f'Current dataset_table version for training_dataset: {ml_instance.dataset_version(training_dataset)}')\n",
    "next_version = ml_instance.increment_dataset_version(training_dataset, VersionPart.minor)\n",
    "print(f'Next dataset_table version for training_dataset: {next_version}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35",
   "metadata": {},
   "source": [
    "display(HTML(f'<a href={ml_instance.chaise_url(\"Dataset\")}>Browse Datasets</a>'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36",
   "metadata": {},
   "source": [
    "test_catalog.delete_ermrest_catalog(really=True)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
